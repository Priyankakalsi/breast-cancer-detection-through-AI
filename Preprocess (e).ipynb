{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdPY2d7LTJ9Q",
        "outputId": "98783918-6e1c-4c0a-a02f-6dfd6b5aba19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# # Mount Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.6.1-cp313-cp313-win_amd64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\victus\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (2.1.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\victus\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (1.15.2)\n",
            "Collecting joblib>=1.2.0 (from scikit-learn)\n",
            "  Downloading joblib-1.5.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
            "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading scikit_learn-1.6.1-cp313-cp313-win_amd64.whl (11.1 MB)\n",
            "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
            "   ----------- ---------------------------- 3.1/11.1 MB 17.9 MB/s eta 0:00:01\n",
            "   ------------------------- -------------- 7.1/11.1 MB 18.9 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 10.2/11.1 MB 17.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 11.1/11.1 MB 16.8 MB/s eta 0:00:00\n",
            "Downloading joblib-1.5.0-py3-none-any.whl (307 kB)\n",
            "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
            "Successfully installed joblib-1.5.0 scikit-learn-1.6.1 threadpoolctl-3.6.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzbIAEHkS6DH"
      },
      "outputs": [],
      "source": [
        "import zipfile # Used for working with ZIP files (e.g., extracting dataset archives)\n",
        "import os # Provides functions for interacting with the operating system (e.g., file paths, directories)\n",
        "from glob import glob # Lets you search for file patterns using wildcards (e.g., *.png)\n",
        "from tqdm import tqdm # Adds a progress bar to loops for visual feedback during long operations\n",
        "import shutil # Allows copying and moving files and directories\n",
        "from PIL import Image # Python Imaging Library for opening, manipulating, and saving images\n",
        "from sklearn.model_selection import train_test_split  # Used to split datasets into training and testing sets\n",
        "import torch  # Core PyTorch library for building and training neural networks\n",
        "import torch.nn as nn # Provides classes for building neural network layers\n",
        "import torch.nn.functional as F # Contains functions like activation functions (ReLU, softmax, etc.)\n",
        "from torchvision import datasets, transforms, models # torchvision tools for image data, preprocessing, and pre-trained models\n",
        "from torch.utils.data import DataLoader  # Utility for batching and loading datasets during training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmnrZe9ZTQJ9"
      },
      "outputs": [],
      "source": [
        "# Paths to ZIP files\n",
        "# breakhis_zip = \"./BreakHis - Breast Cancer Histopathological Database.zip\"\n",
        "# idc_zip = \"./archive (1).zip\"\n",
        "\n",
        "# # Extract ZIPs\n",
        "# with zipfile.ZipFile(breakhis_zip, 'r') as zip_ref:\n",
        "#     zip_ref.extractall(\"/content/BreaKHis\")\n",
        "# with zipfile.ZipFile(idc_zip, 'r') as zip_ref:\n",
        "#     zip_ref.extractall(\"/content/IDC\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJuQkQq3TYDW"
      },
      "outputs": [],
      "source": [
        "# Check if nested ZIP exists in BreaKHis\n",
        "# nested_zip_path = \"/content/BreaKHis/BreakHis - Breast Cancer Histopathological Database/dataset_cancer_v1.zip\"\n",
        "# extract_path = \"/content/BreaKHis/Extracted\"\n",
        "# if not os.path.exists(extract_path):\n",
        "#     os.makedirs(extract_path)\n",
        "# with zipfile.ZipFile(nested_zip_path, 'r') as zip_ref:\n",
        "#     zip_ref.extractall(extract_path)\n",
        "\n",
        "# # Merge IDC and BreaKHis into Unified_Dataset (benign/malignant)\n",
        "# output_root = \"/content/Unified_Dataset\"\n",
        "# for cls in [\"benign\", \"malignant\"]:\n",
        "#     os.makedirs(f\"{output_root}/{cls}\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6M8DlQlGqAB",
        "outputId": "788a412e-392b-4077-c087-ed622af4bf71"
      },
      "outputs": [],
      "source": [
        "# # Copy IDC images\n",
        "# print(\"Copying IDC images...\")\n",
        "# for label in ['0', '1']:\n",
        "#     class_name = \"benign\" if label == '0' else \"malignant\"\n",
        "#     image_paths = glob(f\"/content/IDC/**/{label}/*.png\", recursive=True)\n",
        "#     for img_path in tqdm(image_paths, desc=f\"IDC → {class_name}\"):\n",
        "#         try:\n",
        "#             filename = os.path.basename(img_path)\n",
        "#             shutil.copy(img_path, f\"{output_root}/{class_name}/{label}_IDC_{filename}\")\n",
        "#         except:\n",
        "#             pass\n",
        "\n",
        "# # Copy BreaKHis images\n",
        "# print(\"Copying BreaKHis images...\")\n",
        "# for class_name in [\"benign\", \"malignant\"]:\n",
        "#     image_paths = glob(f\"{extract_path}/{class_name}/**/*.png\", recursive=True)\n",
        "#     for img_path in tqdm(image_paths, desc=f\"BreaKHis → {class_name}\"):\n",
        "#         try:\n",
        "#             filename = os.path.basename(img_path)\n",
        "#             shutil.copy(img_path, f\"{output_root}/{class_name}/BreaKHis_{filename}\")\n",
        "#         except:\n",
        "#             pass\n",
        "\n",
        "# # Resize, normalize, and split dataset into train/val/test\n",
        "# output_dir = \"/content/Preprocessed\"\n",
        "# img_size = (224, 224)\n",
        "# for split in ['train', 'val', 'test']:\n",
        "#     for cls in ['benign', 'malignant']:\n",
        "#         os.makedirs(f\"{output_dir}/{split}/{cls}\", exist_ok=True)\n",
        "\n",
        "# def preprocess_and_split(class_name):\n",
        "#     image_paths = glob(f\"{output_root}/{class_name}/*.png\")\n",
        "#     train, test = train_test_split(image_paths, test_size=0.2, random_state=42)\n",
        "#     train, val = train_test_split(train, test_size=0.1, random_state=42)\n",
        "#     for split, paths in zip(['train', 'val', 'test'], [train, val, test]):\n",
        "#         for img_path in tqdm(paths, desc=f\"{class_name} → {split}\"):\n",
        "#             try:\n",
        "#                 img = Image.open(img_path).convert('RGB')\n",
        "#                 img = img.resize(img_size)\n",
        "#                 img.save(f\"{output_dir}/{split}/{class_name}/{os.path.basename(img_path)}\")\n",
        "#             except:\n",
        "#                 continue\n",
        "\n",
        "# preprocess_and_split(\"benign\")\n",
        "# preprocess_and_split(\"malignant\")\n",
        "# print(\"Preprocessing complete.\")\n",
        "\n",
        "# Prepare DataLoaders\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),  # Convert image to PyTorch tensor (shape: [C, H, W], values: [0, 1])\n",
        "    transforms.Normalize([0.5], [0.5])      # Normalize with mean=0.5 and std=0.5 (scales input to [-1, 1])\n",
        "])\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(datasets.ImageFolder(f\"{output_dir}/train\", transform=transform), batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(datasets.ImageFolder(f\"{output_dir}/val\", transform=transform), batch_size=batch_size)\n",
        "test_loader  = DataLoader(datasets.ImageFolder(f\"{output_dir}/test\", transform=transform), batch_size=batch_size)\n",
        "\n",
        "# Define BreastNet model with channel attention\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, in_planes, ratio=8):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1) # Reduce spatial dimensions to 1x1\n",
        "        self.fc1 = nn.Conv2d(in_planes, in_planes // ratio, kernel_size=1)  # Shrink channels\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Conv2d(in_planes // ratio, in_planes, kernel_size=1)  # Expand channels back\n",
        "        self.sigmoid = nn.Sigmoid()     # Output weights between 0 and 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.pool(x)\n",
        "        out = self.relu(self.fc1(out))\n",
        "        out = self.sigmoid(self.fc2(out))\n",
        "        return x * out          # Reweight original features\n",
        "\n",
        "class BreastNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BreastNet, self).__init__()\n",
        "        self.conv = nn.Conv2d(3, 32, kernel_size=3, padding=1)  # Conv layer: 3 input channels → 32 output\n",
        "        self.bn = nn.BatchNorm2d(32)                # Normalize feature maps\n",
        "        self.attn = ChannelAttention(32)            # Learn which channels to emphasize\n",
        "        self.pool = nn.AdaptiveAvgPool2d((8, 8))    # Reduce to 8x8 spatial size\n",
        "        self.fc = nn.Linear(32 * 8 * 8, 2)          # Final layer → 2 output classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn(self.conv(x)))   # Conv → BN → ReLU\n",
        "        x = self.attn(x)        # Apply channel attention\n",
        "        x = self.pool(x)        # Downsample\n",
        "        x = x.view(x.size(0), -1)   # Flatten for linear layer\n",
        "        return self.fc(x)       # Final prediction\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Copying IDC images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IDC → benign: 100%|██████████| 479/479 [00:03<00:00, 128.85it/s]\n",
            "IDC → malignant: 100%|██████████| 70/70 [00:00<00:00, 144.87it/s]\n",
            "IDC → benign: 100%|██████████| 772/772 [00:05<00:00, 141.95it/s]\n",
            "IDC → malignant: 100%|██████████| 76/76 [00:00<00:00, 139.78it/s]\n",
            "IDC → benign: 100%|██████████| 181/181 [00:01<00:00, 140.82it/s]\n",
            "IDC → malignant: 100%|██████████| 91/91 [00:00<00:00, 137.33it/s]\n",
            "IDC → benign: 100%|██████████| 351/351 [00:02<00:00, 130.30it/s]\n",
            "IDC → malignant: 100%|██████████| 117/117 [00:00<00:00, 131.67it/s]\n",
            "IDC → benign: 100%|██████████| 427/427 [00:03<00:00, 140.41it/s]\n",
            "IDC → malignant: 100%|██████████| 208/208 [00:01<00:00, 139.85it/s]\n",
            "IDC → benign: 100%|██████████| 422/422 [00:03<00:00, 133.56it/s]\n",
            "IDC → malignant: 100%|██████████| 108/108 [00:00<00:00, 132.47it/s]\n",
            "IDC → benign: 100%|██████████| 1434/1434 [00:10<00:00, 138.84it/s]\n",
            "IDC → malignant: 100%|██████████| 31/31 [00:00<00:00, 140.24it/s]\n",
            "IDC → benign: 100%|██████████| 928/928 [00:06<00:00, 142.02it/s]\n",
            "IDC → malignant: 100%|██████████| 361/361 [00:02<00:00, 142.05it/s]\n",
            "IDC → benign: 100%|██████████| 590/590 [00:04<00:00, 142.90it/s]\n",
            "IDC → malignant: 100%|██████████| 56/56 [00:00<00:00, 140.38it/s]\n",
            "IDC → benign: 100%|██████████| 1053/1053 [00:07<00:00, 143.31it/s]\n",
            "IDC → malignant: 100%|██████████| 754/754 [00:05<00:00, 142.00it/s]\n",
            "IDC → benign: 100%|██████████| 617/617 [00:04<00:00, 142.86it/s]\n",
            "IDC → malignant: 100%|██████████| 587/587 [00:04<00:00, 143.88it/s]\n",
            "IDC → benign: 100%|██████████| 2086/2086 [00:14<00:00, 146.61it/s]\n",
            "IDC → malignant: 100%|██████████| 23/23 [00:00<00:00, 128.79it/s]\n",
            "IDC → benign: 100%|██████████| 904/904 [00:06<00:00, 143.34it/s]\n",
            "IDC → malignant: 100%|██████████| 250/250 [00:01<00:00, 140.04it/s]\n",
            "IDC → benign: 100%|██████████| 2150/2150 [00:14<00:00, 144.07it/s]\n",
            "IDC → malignant: 100%|██████████| 25/25 [00:00<00:00, 136.16it/s]\n",
            "IDC → benign: 100%|██████████| 811/811 [00:05<00:00, 141.43it/s]\n",
            "IDC → malignant: 100%|██████████| 1211/1211 [00:08<00:00, 139.41it/s]\n",
            "IDC → benign: 100%|██████████| 659/659 [00:04<00:00, 141.40it/s]\n",
            "IDC → malignant: 100%|██████████| 219/219 [00:01<00:00, 141.41it/s]\n",
            "IDC → benign: 100%|██████████| 297/297 [00:02<00:00, 143.26it/s]\n",
            "IDC → malignant: 100%|██████████| 760/760 [00:05<00:00, 141.87it/s]\n",
            "IDC → benign: 100%|██████████| 591/591 [00:04<00:00, 142.89it/s]\n",
            "IDC → malignant: 100%|██████████| 348/348 [00:02<00:00, 148.01it/s]\n",
            "IDC → benign: 100%|██████████| 785/785 [00:05<00:00, 153.83it/s]\n",
            "IDC → malignant: 100%|██████████| 170/170 [00:01<00:00, 156.46it/s]\n",
            "IDC → benign: 100%|██████████| 1068/1068 [00:06<00:00, 154.03it/s]\n",
            "IDC → malignant: 100%|██████████| 91/91 [00:00<00:00, 146.81it/s]\n",
            "IDC → benign: 100%|██████████| 1267/1267 [00:08<00:00, 141.75it/s]\n",
            "IDC → malignant: 100%|██████████| 427/427 [00:02<00:00, 149.22it/s]\n",
            "IDC → benign: 100%|██████████| 1835/1835 [00:11<00:00, 154.02it/s]\n",
            "IDC → malignant: 100%|██████████| 198/198 [00:01<00:00, 151.39it/s]\n",
            "IDC → benign: 100%|██████████| 1011/1011 [00:07<00:00, 143.42it/s]\n",
            "IDC → malignant: 100%|██████████| 222/222 [00:01<00:00, 140.82it/s]\n",
            "IDC → benign: 100%|██████████| 458/458 [00:03<00:00, 142.20it/s]\n",
            "IDC → malignant: 100%|██████████| 162/162 [00:01<00:00, 122.93it/s]\n",
            "IDC → benign: 100%|██████████| 2231/2231 [00:15<00:00, 145.84it/s]\n",
            "IDC → malignant: 100%|██████████| 47/47 [00:00<00:00, 154.25it/s]\n",
            "IDC → benign: 100%|██████████| 1891/1891 [00:12<00:00, 155.10it/s]\n",
            "IDC → malignant: 100%|██████████| 140/140 [00:00<00:00, 156.32it/s]\n",
            "IDC → benign: 100%|██████████| 999/999 [00:06<00:00, 143.39it/s]\n",
            "IDC → malignant: 100%|██████████| 213/213 [00:01<00:00, 142.21it/s]\n",
            "IDC → benign: 100%|██████████| 998/998 [00:06<00:00, 150.60it/s]\n",
            "IDC → malignant: 100%|██████████| 487/487 [00:03<00:00, 154.40it/s]\n",
            "IDC → benign: 100%|██████████| 649/649 [00:04<00:00, 152.23it/s]\n",
            "IDC → malignant: 100%|██████████| 221/221 [00:01<00:00, 135.90it/s]\n",
            "IDC → benign: 100%|██████████| 761/761 [00:05<00:00, 143.50it/s]\n",
            "IDC → malignant: 100%|██████████| 134/134 [00:00<00:00, 143.05it/s]\n",
            "IDC → benign: 100%|██████████| 759/759 [00:05<00:00, 141.76it/s]\n",
            "IDC → malignant: 100%|██████████| 1347/1347 [00:10<00:00, 130.04it/s]\n",
            "IDC → benign: 100%|██████████| 1464/1464 [00:11<00:00, 132.13it/s]\n",
            "IDC → malignant: 100%|██████████| 29/29 [00:00<00:00, 141.35it/s]\n",
            "IDC → benign: 100%|██████████| 1074/1074 [00:07<00:00, 143.78it/s]\n",
            "IDC → malignant: 100%|██████████| 342/342 [00:02<00:00, 152.99it/s]\n",
            "IDC → benign: 100%|██████████| 598/598 [00:04<00:00, 148.46it/s]\n",
            "IDC → malignant: 100%|██████████| 1309/1309 [00:08<00:00, 149.42it/s]\n",
            "IDC → benign: 100%|██████████| 579/579 [00:03<00:00, 153.31it/s]\n",
            "IDC → malignant: 100%|██████████| 773/773 [00:05<00:00, 142.73it/s]\n",
            "IDC → benign: 100%|██████████| 779/779 [00:05<00:00, 142.13it/s]\n",
            "IDC → malignant: 100%|██████████| 101/101 [00:00<00:00, 154.34it/s]\n",
            "IDC → benign: 100%|██████████| 1802/1802 [00:12<00:00, 143.36it/s]\n",
            "IDC → malignant: 100%|██████████| 19/19 [00:00<00:00, 155.81it/s]\n",
            "IDC → benign: 100%|██████████| 751/751 [00:05<00:00, 149.82it/s]\n",
            "IDC → malignant: 100%|██████████| 273/273 [00:01<00:00, 148.05it/s]\n",
            "IDC → benign: 100%|██████████| 915/915 [00:06<00:00, 150.25it/s]\n",
            "IDC → malignant: 100%|██████████| 80/80 [00:00<00:00, 157.26it/s]\n",
            "IDC → benign: 100%|██████████| 1383/1383 [00:09<00:00, 140.70it/s]\n",
            "IDC → malignant: 100%|██████████| 895/895 [00:06<00:00, 142.71it/s]\n",
            "IDC → benign: 100%|██████████| 37/37 [00:00<00:00, 138.24it/s]\n",
            "IDC → malignant: 100%|██████████| 115/115 [00:00<00:00, 145.31it/s]\n",
            "IDC → benign: 100%|██████████| 668/668 [00:04<00:00, 141.55it/s]\n",
            "IDC → malignant: 100%|██████████| 429/429 [00:03<00:00, 142.95it/s]\n",
            "IDC → benign: 100%|██████████| 1088/1088 [00:10<00:00, 101.51it/s]\n",
            "IDC → malignant: 100%|██████████| 254/254 [00:03<00:00, 80.05it/s] \n",
            "IDC → benign: 100%|██████████| 168/168 [00:01<00:00, 133.94it/s]\n",
            "IDC → malignant: 100%|██████████| 198/198 [00:01<00:00, 144.71it/s]\n",
            "IDC → benign: 100%|██████████| 1199/1199 [00:08<00:00, 138.98it/s]\n",
            "IDC → malignant: 100%|██████████| 563/563 [00:03<00:00, 146.52it/s]\n",
            "IDC → benign: 100%|██████████| 1413/1413 [00:10<00:00, 140.70it/s]\n",
            "IDC → malignant: 100%|██████████| 21/21 [00:00<00:00, 144.87it/s]\n",
            "IDC → benign: 100%|██████████| 849/849 [00:05<00:00, 148.73it/s]\n",
            "IDC → malignant: 100%|██████████| 967/967 [00:06<00:00, 141.12it/s]\n",
            "IDC → benign: 100%|██████████| 464/464 [00:03<00:00, 140.01it/s]\n",
            "IDC → malignant: 100%|██████████| 635/635 [00:04<00:00, 150.15it/s]\n",
            "IDC → benign: 100%|██████████| 914/914 [00:06<00:00, 147.80it/s]\n",
            "IDC → malignant: 100%|██████████| 252/252 [00:01<00:00, 148.18it/s]\n",
            "IDC → benign: 100%|██████████| 125/125 [00:00<00:00, 151.73it/s]\n",
            "IDC → malignant: 100%|██████████| 126/126 [00:00<00:00, 153.01it/s]\n",
            "IDC → benign: 100%|██████████| 362/362 [00:02<00:00, 149.61it/s]\n",
            "IDC → malignant: 100%|██████████| 572/572 [00:03<00:00, 152.78it/s]\n",
            "IDC → benign: 100%|██████████| 666/666 [00:04<00:00, 149.21it/s]\n",
            "IDC → malignant: 100%|██████████| 945/945 [00:06<00:00, 151.50it/s]\n",
            "IDC → benign: 100%|██████████| 1404/1404 [00:09<00:00, 149.65it/s]\n",
            "IDC → malignant: 100%|██████████| 223/223 [00:01<00:00, 150.35it/s]\n",
            "IDC → benign: 100%|██████████| 753/753 [00:05<00:00, 149.56it/s]\n",
            "IDC → malignant: 100%|██████████| 369/369 [00:02<00:00, 152.40it/s]\n",
            "IDC → benign: 100%|██████████| 1066/1066 [00:07<00:00, 135.57it/s]\n",
            "IDC → malignant: 100%|██████████| 319/319 [00:02<00:00, 136.44it/s]\n",
            "IDC → benign: 100%|██████████| 490/490 [00:03<00:00, 145.69it/s]\n",
            "IDC → malignant: 100%|██████████| 271/271 [00:01<00:00, 152.63it/s]\n",
            "IDC → benign: 100%|██████████| 556/556 [00:03<00:00, 147.25it/s]\n",
            "IDC → malignant: 100%|██████████| 447/447 [00:02<00:00, 151.35it/s]\n",
            "IDC → benign: 100%|██████████| 597/597 [00:04<00:00, 146.78it/s]\n",
            "IDC → malignant: 100%|██████████| 110/110 [00:00<00:00, 152.48it/s]\n",
            "IDC → benign: 100%|██████████| 963/963 [00:06<00:00, 138.34it/s]\n",
            "IDC → malignant: 100%|██████████| 174/174 [00:01<00:00, 140.57it/s]\n",
            "IDC → benign: 100%|██████████| 851/851 [00:06<00:00, 139.03it/s]\n",
            "IDC → malignant: 100%|██████████| 575/575 [00:04<00:00, 125.55it/s]\n",
            "IDC → benign: 100%|██████████| 500/500 [00:03<00:00, 135.73it/s]\n",
            "IDC → malignant: 100%|██████████| 361/361 [00:02<00:00, 140.61it/s]\n",
            "IDC → benign: 100%|██████████| 778/778 [00:06<00:00, 118.92it/s]\n",
            "IDC → malignant: 100%|██████████| 18/18 [00:00<00:00, 138.03it/s]\n",
            "IDC → benign: 100%|██████████| 788/788 [00:05<00:00, 134.37it/s]\n",
            "IDC → malignant: 100%|██████████| 41/41 [00:00<00:00, 130.94it/s]\n",
            "IDC → benign: 100%|██████████| 146/146 [00:01<00:00, 139.60it/s]\n",
            "IDC → malignant: 100%|██████████| 36/36 [00:00<00:00, 138.66it/s]\n",
            "IDC → benign: 100%|██████████| 711/711 [00:04<00:00, 146.52it/s]\n",
            "IDC → malignant: 100%|██████████| 69/69 [00:00<00:00, 147.74it/s]\n",
            "IDC → benign: 100%|██████████| 49/49 [00:00<00:00, 146.30it/s]\n",
            "IDC → malignant: 100%|██████████| 232/232 [00:01<00:00, 135.69it/s]\n",
            "IDC → benign: 100%|██████████| 331/331 [00:02<00:00, 133.81it/s]\n",
            "IDC → malignant: 100%|██████████| 43/43 [00:00<00:00, 135.29it/s]\n",
            "IDC → benign: 100%|██████████| 50/50 [00:00<00:00, 136.82it/s]\n",
            "IDC → malignant: 100%|██████████| 105/105 [00:00<00:00, 136.91it/s]\n",
            "IDC → benign: 100%|██████████| 272/272 [00:02<00:00, 124.69it/s]\n",
            "IDC → malignant: 100%|██████████| 33/33 [00:00<00:00, 128.19it/s]\n",
            "IDC → benign: 100%|██████████| 1289/1289 [00:09<00:00, 140.06it/s]\n",
            "IDC → malignant: 100%|██████████| 185/185 [00:01<00:00, 146.14it/s]\n",
            "IDC → benign: 100%|██████████| 272/272 [00:01<00:00, 144.30it/s]\n",
            "IDC → malignant: 100%|██████████| 144/144 [00:01<00:00, 143.67it/s]\n",
            "IDC → benign: 100%|██████████| 788/788 [00:05<00:00, 137.38it/s]\n",
            "IDC → malignant: 100%|██████████| 1147/1147 [00:08<00:00, 136.30it/s]\n",
            "IDC → benign: 100%|██████████| 115/115 [00:00<00:00, 136.94it/s]\n",
            "IDC → malignant: 100%|██████████| 158/158 [00:01<00:00, 140.55it/s]\n",
            "IDC → benign: 100%|██████████| 238/238 [00:01<00:00, 145.22it/s]\n",
            "IDC → malignant: 100%|██████████| 154/154 [00:01<00:00, 143.20it/s]\n",
            "IDC → benign: 100%|██████████| 276/276 [00:01<00:00, 145.01it/s]\n",
            "IDC → malignant: 100%|██████████| 73/73 [00:00<00:00, 147.11it/s]\n",
            "IDC → benign: 100%|██████████| 533/533 [00:03<00:00, 143.13it/s]\n",
            "IDC → malignant: 100%|██████████| 236/236 [00:01<00:00, 148.48it/s]\n",
            "IDC → benign: 100%|██████████| 240/240 [00:01<00:00, 141.65it/s]\n",
            "IDC → malignant: 100%|██████████| 287/287 [00:01<00:00, 144.95it/s]\n",
            "IDC → benign: 100%|██████████| 1313/1313 [00:09<00:00, 135.00it/s]\n",
            "IDC → malignant: 100%|██████████| 158/158 [00:01<00:00, 136.51it/s]\n",
            "IDC → benign: 100%|██████████| 442/442 [00:03<00:00, 139.37it/s]\n",
            "IDC → malignant: 100%|██████████| 172/172 [00:01<00:00, 145.57it/s]\n",
            "IDC → benign: 100%|██████████| 133/133 [00:00<00:00, 147.94it/s]\n",
            "IDC → malignant: 100%|██████████| 93/93 [00:00<00:00, 154.25it/s]\n",
            "IDC → benign: 100%|██████████| 216/216 [00:01<00:00, 144.88it/s]\n",
            "IDC → malignant: 100%|██████████| 482/482 [00:03<00:00, 147.93it/s]\n",
            "IDC → benign: 100%|██████████| 1066/1066 [00:07<00:00, 137.80it/s]\n",
            "IDC → malignant: 100%|██████████| 650/650 [00:04<00:00, 141.58it/s]\n",
            "IDC → benign: 100%|██████████| 939/939 [00:06<00:00, 137.26it/s]\n",
            "IDC → malignant: 100%|██████████| 741/741 [00:05<00:00, 140.04it/s]\n",
            "IDC → benign: 100%|██████████| 424/424 [00:03<00:00, 137.42it/s]\n",
            "IDC → malignant: 100%|██████████| 83/83 [00:00<00:00, 131.70it/s]\n",
            "IDC → benign: 100%|██████████| 565/565 [00:04<00:00, 137.27it/s]\n",
            "IDC → malignant: 100%|██████████| 296/296 [00:02<00:00, 137.23it/s]\n",
            "IDC → benign: 100%|██████████| 372/372 [00:02<00:00, 132.45it/s]\n",
            "IDC → malignant: 100%|██████████| 208/208 [00:01<00:00, 132.52it/s]\n",
            "IDC → benign: 100%|██████████| 573/573 [00:04<00:00, 137.60it/s]\n",
            "IDC → malignant: 100%|██████████| 450/450 [00:03<00:00, 142.32it/s]\n",
            "IDC → benign: 100%|██████████| 578/578 [00:04<00:00, 143.91it/s]\n",
            "IDC → malignant: 100%|██████████| 230/230 [00:01<00:00, 145.07it/s]\n",
            "IDC → benign: 100%|██████████| 1193/1193 [00:08<00:00, 144.15it/s]\n",
            "IDC → malignant: 100%|██████████| 21/21 [00:00<00:00, 147.43it/s]\n",
            "IDC → benign: 100%|██████████| 816/816 [00:06<00:00, 135.98it/s]\n",
            "IDC → malignant: 100%|██████████| 887/887 [00:06<00:00, 138.63it/s]\n",
            "IDC → benign: 100%|██████████| 546/546 [00:03<00:00, 137.32it/s]\n",
            "IDC → malignant: 100%|██████████| 504/504 [00:03<00:00, 144.15it/s]\n",
            "IDC → benign: 100%|██████████| 778/778 [00:05<00:00, 144.51it/s]\n",
            "IDC → malignant: 100%|██████████| 262/262 [00:01<00:00, 147.00it/s]\n",
            "IDC → benign: 100%|██████████| 283/283 [00:01<00:00, 143.69it/s]\n",
            "IDC → malignant: 100%|██████████| 514/514 [00:03<00:00, 145.02it/s]\n",
            "IDC → benign: 100%|██████████| 1496/1496 [00:10<00:00, 137.14it/s]\n",
            "IDC → malignant: 100%|██████████| 222/222 [00:01<00:00, 148.36it/s]\n",
            "IDC → benign: 100%|██████████| 1041/1041 [00:07<00:00, 136.45it/s]\n",
            "IDC → malignant: 100%|██████████| 201/201 [00:01<00:00, 139.58it/s]\n",
            "IDC → benign: 100%|██████████| 90/90 [00:00<00:00, 130.43it/s]\n",
            "IDC → malignant: 100%|██████████| 70/70 [00:00<00:00, 141.33it/s]\n",
            "IDC → benign: 100%|██████████| 835/835 [00:06<00:00, 134.26it/s]\n",
            "IDC → malignant: 100%|██████████| 165/165 [00:01<00:00, 138.82it/s]\n",
            "IDC → benign: 100%|██████████| 477/477 [00:03<00:00, 138.05it/s]\n",
            "IDC → malignant: 100%|██████████| 130/130 [00:00<00:00, 149.32it/s]\n",
            "IDC → benign: 100%|██████████| 433/433 [00:02<00:00, 146.15it/s]\n",
            "IDC → malignant: 100%|██████████| 304/304 [00:02<00:00, 144.36it/s]\n",
            "IDC → benign: 100%|██████████| 125/125 [00:00<00:00, 146.70it/s]\n",
            "IDC → malignant: 100%|██████████| 30/30 [00:00<00:00, 145.60it/s]\n",
            "IDC → benign: 100%|██████████| 1500/1500 [00:10<00:00, 144.45it/s]\n",
            "IDC → malignant: 100%|██████████| 504/504 [00:03<00:00, 147.40it/s]\n",
            "IDC → benign: 100%|██████████| 611/611 [00:04<00:00, 145.80it/s]\n",
            "IDC → malignant: 100%|██████████| 615/615 [00:04<00:00, 144.66it/s]\n",
            "IDC → benign: 100%|██████████| 378/378 [00:02<00:00, 139.18it/s]\n",
            "IDC → malignant: 100%|██████████| 452/452 [00:03<00:00, 135.83it/s]\n",
            "IDC → benign: 100%|██████████| 80/80 [00:00<00:00, 142.33it/s]\n",
            "IDC → malignant: 100%|██████████| 87/87 [00:00<00:00, 139.10it/s]\n",
            "IDC → benign: 100%|██████████| 344/344 [00:02<00:00, 147.61it/s]\n",
            "IDC → malignant: 100%|██████████| 464/464 [00:03<00:00, 141.69it/s]\n",
            "IDC → benign: 100%|██████████| 808/808 [00:05<00:00, 144.20it/s]\n",
            "IDC → malignant: 100%|██████████| 330/330 [00:02<00:00, 144.77it/s]\n",
            "IDC → benign: 100%|██████████| 1945/1945 [00:13<00:00, 144.24it/s]\n",
            "IDC → malignant: 100%|██████████| 64/64 [00:00<00:00, 140.31it/s]\n",
            "IDC → benign: 100%|██████████| 809/809 [00:06<00:00, 132.36it/s]\n",
            "IDC → malignant: 100%|██████████| 253/253 [00:01<00:00, 138.73it/s]\n",
            "IDC → benign: 100%|██████████| 175/175 [00:01<00:00, 134.36it/s]\n",
            "IDC → malignant: 100%|██████████| 128/128 [00:00<00:00, 138.52it/s]\n",
            "IDC → benign: 100%|██████████| 1069/1069 [00:07<00:00, 136.84it/s]\n",
            "IDC → malignant: 100%|██████████| 441/441 [00:03<00:00, 138.73it/s]\n",
            "IDC → benign: 100%|██████████| 336/336 [00:02<00:00, 145.38it/s]\n",
            "IDC → malignant: 100%|██████████| 50/50 [00:00<00:00, 131.49it/s]\n",
            "IDC → benign: 100%|██████████| 1089/1089 [00:08<00:00, 133.94it/s]\n",
            "IDC → malignant: 100%|██████████| 108/108 [00:00<00:00, 135.76it/s]\n",
            "IDC → benign: 100%|██████████| 1056/1056 [00:07<00:00, 135.20it/s]\n",
            "IDC → malignant: 100%|██████████| 286/286 [00:02<00:00, 137.01it/s]\n",
            "IDC → benign: 100%|██████████| 112/112 [00:00<00:00, 141.99it/s]\n",
            "IDC → malignant: 100%|██████████| 115/115 [00:00<00:00, 145.61it/s]\n",
            "IDC → benign: 100%|██████████| 624/624 [00:04<00:00, 144.13it/s]\n",
            "IDC → malignant: 100%|██████████| 186/186 [00:01<00:00, 136.20it/s]\n",
            "IDC → benign: 100%|██████████| 465/465 [00:03<00:00, 137.11it/s]\n",
            "IDC → malignant: 100%|██████████| 296/296 [00:02<00:00, 135.73it/s]\n",
            "IDC → benign: 100%|██████████| 979/979 [00:07<00:00, 134.92it/s]\n",
            "IDC → malignant: 100%|██████████| 155/155 [00:01<00:00, 134.47it/s]\n",
            "IDC → benign: 100%|██████████| 1299/1299 [00:09<00:00, 135.52it/s]\n",
            "IDC → malignant: 100%|██████████| 64/64 [00:00<00:00, 135.01it/s]\n",
            "IDC → benign: 100%|██████████| 323/323 [00:02<00:00, 145.86it/s]\n",
            "IDC → malignant: 100%|██████████| 244/244 [00:01<00:00, 140.83it/s]\n",
            "IDC → benign: 100%|██████████| 292/292 [00:02<00:00, 144.58it/s]\n",
            "IDC → malignant: 100%|██████████| 519/519 [00:03<00:00, 141.98it/s]\n",
            "IDC → benign: 100%|██████████| 201/201 [00:01<00:00, 140.19it/s]\n",
            "IDC → malignant: 100%|██████████| 111/111 [00:00<00:00, 151.96it/s]\n",
            "IDC → benign: 100%|██████████| 379/379 [00:02<00:00, 137.19it/s]\n",
            "IDC → malignant: 100%|██████████| 178/178 [00:01<00:00, 145.81it/s]\n",
            "IDC → benign: 100%|██████████| 394/394 [00:02<00:00, 145.62it/s]\n",
            "IDC → malignant: 100%|██████████| 49/49 [00:00<00:00, 133.91it/s]\n",
            "IDC → benign: 100%|██████████| 802/802 [00:05<00:00, 142.91it/s]\n",
            "IDC → malignant: 100%|██████████| 224/224 [00:01<00:00, 144.98it/s]\n",
            "IDC → benign: 100%|██████████| 623/623 [00:04<00:00, 141.47it/s]\n",
            "IDC → malignant: 100%|██████████| 46/46 [00:00<00:00, 139.05it/s]\n",
            "IDC → benign: 100%|██████████| 594/594 [00:04<00:00, 130.77it/s]\n",
            "IDC → malignant: 100%|██████████| 70/70 [00:00<00:00, 136.22it/s]\n",
            "IDC → benign: 100%|██████████| 1028/1028 [00:08<00:00, 127.24it/s]\n",
            "IDC → malignant: 100%|██████████| 726/726 [00:05<00:00, 140.77it/s]\n",
            "IDC → benign: 100%|██████████| 907/907 [00:06<00:00, 135.18it/s]\n",
            "IDC → malignant: 100%|██████████| 128/128 [00:00<00:00, 136.21it/s]\n",
            "IDC → benign: 100%|██████████| 827/827 [00:05<00:00, 140.21it/s]\n",
            "IDC → malignant: 100%|██████████| 630/630 [00:04<00:00, 141.57it/s]\n",
            "IDC → benign: 100%|██████████| 656/656 [00:04<00:00, 144.93it/s]\n",
            "IDC → malignant: 100%|██████████| 701/701 [00:05<00:00, 134.02it/s]\n",
            "IDC → benign: 100%|██████████| 299/299 [00:02<00:00, 133.47it/s]\n",
            "IDC → malignant: 100%|██████████| 56/56 [00:00<00:00, 133.41it/s]\n",
            "IDC → benign: 100%|██████████| 377/377 [00:02<00:00, 138.19it/s]\n",
            "IDC → malignant: 100%|██████████| 30/30 [00:00<00:00, 106.11it/s]\n",
            "IDC → benign: 100%|██████████| 303/303 [00:02<00:00, 136.57it/s]\n",
            "IDC → malignant: 100%|██████████| 151/151 [00:01<00:00, 139.11it/s]\n",
            "IDC → benign: 100%|██████████| 215/215 [00:01<00:00, 132.63it/s]\n",
            "IDC → malignant: 100%|██████████| 127/127 [00:00<00:00, 136.81it/s]\n",
            "IDC → benign: 100%|██████████| 510/510 [00:03<00:00, 137.53it/s]\n",
            "IDC → malignant: 100%|██████████| 76/76 [00:00<00:00, 135.07it/s]\n",
            "IDC → benign: 100%|██████████| 927/927 [00:06<00:00, 136.22it/s]\n",
            "IDC → malignant: 100%|██████████| 264/264 [00:01<00:00, 138.24it/s]\n",
            "IDC → benign: 100%|██████████| 272/272 [00:01<00:00, 138.44it/s]\n",
            "IDC → malignant: 100%|██████████| 335/335 [00:02<00:00, 137.82it/s]\n",
            "IDC → benign: 100%|██████████| 1935/1935 [00:14<00:00, 138.21it/s]\n",
            "IDC → malignant: 100%|██████████| 460/460 [00:03<00:00, 145.93it/s]\n",
            "IDC → benign: 100%|██████████| 360/360 [00:02<00:00, 141.35it/s]\n",
            "IDC → malignant: 100%|██████████| 870/870 [00:06<00:00, 135.67it/s]\n",
            "IDC → benign: 100%|██████████| 1268/1268 [00:09<00:00, 132.81it/s]\n",
            "IDC → malignant: 100%|██████████| 365/365 [00:02<00:00, 128.44it/s]\n",
            "IDC → benign: 100%|██████████| 100/100 [00:00<00:00, 141.29it/s]\n",
            "IDC → malignant: 100%|██████████| 121/121 [00:00<00:00, 130.52it/s]\n",
            "IDC → benign: 100%|██████████| 435/435 [00:03<00:00, 134.81it/s]\n",
            "IDC → malignant: 100%|██████████| 455/455 [00:03<00:00, 136.67it/s]\n",
            "IDC → benign: 100%|██████████| 117/117 [00:00<00:00, 138.56it/s]\n",
            "IDC → malignant: 100%|██████████| 209/209 [00:01<00:00, 140.91it/s]\n",
            "IDC → benign: 100%|██████████| 281/281 [00:02<00:00, 140.08it/s]\n",
            "IDC → malignant: 100%|██████████| 197/197 [00:01<00:00, 144.32it/s]\n",
            "IDC → benign: 100%|██████████| 579/579 [00:04<00:00, 143.84it/s]\n",
            "IDC → malignant: 100%|██████████| 210/210 [00:01<00:00, 141.96it/s]\n",
            "IDC → benign: 100%|██████████| 691/691 [00:04<00:00, 143.21it/s]\n",
            "IDC → malignant: 100%|██████████| 829/829 [00:05<00:00, 142.93it/s]\n",
            "IDC → benign: 100%|██████████| 671/671 [00:04<00:00, 142.47it/s]\n",
            "IDC → malignant: 100%|██████████| 1206/1206 [00:08<00:00, 135.80it/s]\n",
            "IDC → benign: 100%|██████████| 1201/1201 [00:08<00:00, 136.73it/s]\n",
            "IDC → malignant: 100%|██████████| 197/197 [00:01<00:00, 141.93it/s]\n",
            "IDC → benign: 100%|██████████| 990/990 [00:07<00:00, 135.50it/s]\n",
            "IDC → malignant: 100%|██████████| 488/488 [00:03<00:00, 136.63it/s]\n",
            "IDC → benign: 100%|██████████| 586/586 [00:04<00:00, 127.84it/s]\n",
            "IDC → malignant: 100%|██████████| 123/123 [00:00<00:00, 137.30it/s]\n",
            "IDC → benign: 100%|██████████| 581/581 [00:04<00:00, 136.25it/s]\n",
            "IDC → malignant: 100%|██████████| 421/421 [00:03<00:00, 132.33it/s]\n",
            "IDC → benign: 100%|██████████| 458/458 [00:04<00:00, 113.49it/s]\n",
            "IDC → malignant: 100%|██████████| 491/491 [00:03<00:00, 125.28it/s]\n",
            "IDC → benign: 100%|██████████| 723/723 [00:06<00:00, 112.34it/s]\n",
            "IDC → malignant: 100%|██████████| 617/617 [00:04<00:00, 123.79it/s]\n",
            "IDC → benign: 100%|██████████| 837/837 [00:06<00:00, 130.19it/s]\n",
            "IDC → malignant: 100%|██████████| 195/195 [00:01<00:00, 139.85it/s]\n",
            "IDC → benign: 100%|██████████| 33/33 [00:00<00:00, 128.54it/s]\n",
            "IDC → malignant: 100%|██████████| 309/309 [00:02<00:00, 135.51it/s]\n",
            "IDC → benign: 100%|██████████| 469/469 [00:03<00:00, 135.91it/s]\n",
            "IDC → malignant: 100%|██████████| 104/104 [00:00<00:00, 138.39it/s]\n",
            "IDC → benign: 100%|██████████| 1287/1287 [00:09<00:00, 138.12it/s]\n",
            "IDC → malignant: 100%|██████████| 809/809 [00:05<00:00, 144.45it/s]\n",
            "IDC → benign: 100%|██████████| 167/167 [00:01<00:00, 153.01it/s]\n",
            "IDC → malignant: 100%|██████████| 44/44 [00:00<00:00, 143.95it/s]\n",
            "IDC → benign: 100%|██████████| 169/169 [00:01<00:00, 149.01it/s]\n",
            "IDC → malignant: 100%|██████████| 253/253 [00:01<00:00, 155.31it/s]\n",
            "IDC → benign: 100%|██████████| 410/410 [00:02<00:00, 151.46it/s]\n",
            "IDC → malignant: 100%|██████████| 432/432 [00:02<00:00, 151.63it/s]\n",
            "IDC → benign: 100%|██████████| 714/714 [00:04<00:00, 150.16it/s]\n",
            "IDC → malignant: 100%|██████████| 272/272 [00:01<00:00, 151.73it/s]\n",
            "IDC → benign: 100%|██████████| 264/264 [00:01<00:00, 150.02it/s]\n",
            "IDC → malignant: 100%|██████████| 167/167 [00:01<00:00, 143.04it/s]\n",
            "IDC → benign: 100%|██████████| 426/426 [00:02<00:00, 152.57it/s]\n",
            "IDC → malignant: 100%|██████████| 195/195 [00:01<00:00, 146.92it/s]\n",
            "IDC → benign: 100%|██████████| 448/448 [00:02<00:00, 154.38it/s]\n",
            "IDC → malignant: 100%|██████████| 86/86 [00:00<00:00, 156.41it/s]\n",
            "IDC → benign: 100%|██████████| 1490/1490 [00:10<00:00, 143.46it/s]\n",
            "IDC → malignant: 100%|██████████| 214/214 [00:01<00:00, 148.27it/s]\n",
            "IDC → benign: 100%|██████████| 553/553 [00:03<00:00, 151.11it/s]\n",
            "IDC → malignant: 100%|██████████| 885/885 [00:06<00:00, 142.87it/s]\n",
            "IDC → benign: 100%|██████████| 705/705 [00:05<00:00, 136.89it/s]\n",
            "IDC → malignant: 100%|██████████| 356/356 [00:02<00:00, 141.83it/s]\n",
            "IDC → benign: 100%|██████████| 79/79 [00:00<00:00, 144.34it/s]\n",
            "IDC → malignant: 100%|██████████| 143/143 [00:00<00:00, 143.13it/s]\n",
            "IDC → benign: 100%|██████████| 815/815 [00:05<00:00, 142.99it/s]\n",
            "IDC → malignant: 100%|██████████| 54/54 [00:00<00:00, 140.99it/s]\n",
            "IDC → benign: 100%|██████████| 197/197 [00:01<00:00, 139.68it/s]\n",
            "IDC → malignant: 100%|██████████| 441/441 [00:03<00:00, 146.08it/s]\n",
            "IDC → benign: 100%|██████████| 1051/1051 [00:06<00:00, 150.60it/s]\n",
            "IDC → malignant: 100%|██████████| 111/111 [00:00<00:00, 147.04it/s]\n",
            "IDC → benign: 100%|██████████| 1016/1016 [00:06<00:00, 152.79it/s]\n",
            "IDC → malignant: 100%|██████████| 275/275 [00:01<00:00, 151.72it/s]\n",
            "IDC → benign: 100%|██████████| 373/373 [00:02<00:00, 148.98it/s]\n",
            "IDC → malignant: 100%|██████████| 120/120 [00:00<00:00, 150.78it/s]\n",
            "IDC → benign: 100%|██████████| 114/114 [00:00<00:00, 150.02it/s]\n",
            "IDC → malignant: 100%|██████████| 337/337 [00:02<00:00, 152.38it/s]\n",
            "IDC → benign: 100%|██████████| 439/439 [00:02<00:00, 148.46it/s]\n",
            "IDC → malignant: 100%|██████████| 370/370 [00:02<00:00, 153.26it/s]\n",
            "IDC → benign: 100%|██████████| 105/105 [00:00<00:00, 143.05it/s]\n",
            "IDC → malignant: 100%|██████████| 134/134 [00:00<00:00, 155.04it/s]\n",
            "IDC → benign: 100%|██████████| 862/862 [00:06<00:00, 142.47it/s]\n",
            "IDC → malignant: 100%|██████████| 244/244 [00:01<00:00, 143.98it/s]\n",
            "IDC → benign: 100%|██████████| 706/706 [00:04<00:00, 142.29it/s]\n",
            "IDC → malignant: 100%|██████████| 461/461 [00:03<00:00, 147.41it/s]\n",
            "IDC → benign: 100%|██████████| 418/418 [00:02<00:00, 150.28it/s]\n",
            "IDC → malignant: 100%|██████████| 621/621 [00:04<00:00, 137.55it/s]\n",
            "IDC → benign: 100%|██████████| 497/497 [00:03<00:00, 140.12it/s]\n",
            "IDC → malignant: 100%|██████████| 209/209 [00:01<00:00, 143.52it/s]\n",
            "IDC → benign: 100%|██████████| 1913/1913 [00:12<00:00, 150.45it/s]\n",
            "IDC → malignant: 100%|██████████| 24/24 [00:00<00:00, 130.91it/s]\n",
            "IDC → benign: 100%|██████████| 937/937 [00:06<00:00, 150.71it/s]\n",
            "IDC → malignant: 100%|██████████| 1174/1174 [00:08<00:00, 142.71it/s]\n",
            "IDC → benign: 100%|██████████| 615/615 [00:04<00:00, 145.67it/s]\n",
            "IDC → malignant: 100%|██████████| 675/675 [00:04<00:00, 151.63it/s]\n",
            "IDC → benign: 100%|██████████| 96/96 [00:00<00:00, 152.55it/s]\n",
            "IDC → malignant: 100%|██████████| 96/96 [00:00<00:00, 144.41it/s]\n",
            "IDC → benign: 100%|██████████| 191/191 [00:01<00:00, 151.40it/s]\n",
            "IDC → malignant: 100%|██████████| 58/58 [00:00<00:00, 148.02it/s]\n",
            "IDC → benign: 100%|██████████| 339/339 [00:02<00:00, 145.00it/s]\n",
            "IDC → malignant: 100%|██████████| 128/128 [00:00<00:00, 150.34it/s]\n",
            "IDC → benign: 100%|██████████| 240/240 [00:01<00:00, 149.51it/s]\n",
            "IDC → malignant: 100%|██████████| 127/127 [00:00<00:00, 146.54it/s]\n",
            "IDC → benign: 100%|██████████| 21/21 [00:00<00:00, 150.44it/s]\n",
            "IDC → malignant: 100%|██████████| 42/42 [00:00<00:00, 159.63it/s]\n",
            "IDC → benign: 100%|██████████| 2115/2115 [00:14<00:00, 150.47it/s]\n",
            "IDC → malignant: 100%|██████████| 187/187 [00:01<00:00, 138.63it/s]\n",
            "IDC → benign: 100%|██████████| 1899/1899 [00:12<00:00, 150.73it/s]\n",
            "IDC → malignant: 100%|██████████| 284/284 [00:01<00:00, 152.63it/s]\n",
            "IDC → benign: 100%|██████████| 150/150 [00:00<00:00, 153.73it/s]\n",
            "IDC → malignant: 100%|██████████| 37/37 [00:00<00:00, 152.48it/s]\n",
            "IDC → benign: 100%|██████████| 327/327 [00:02<00:00, 149.40it/s]\n",
            "IDC → malignant: 100%|██████████| 353/353 [00:02<00:00, 139.82it/s]\n",
            "IDC → benign: 100%|██████████| 269/269 [00:01<00:00, 147.26it/s]\n",
            "IDC → malignant: 100%|██████████| 448/448 [00:03<00:00, 142.37it/s]\n",
            "IDC → benign: 100%|██████████| 315/315 [00:02<00:00, 137.61it/s]\n",
            "IDC → malignant: 100%|██████████| 85/85 [00:00<00:00, 141.08it/s]\n",
            "IDC → benign: 100%|██████████| 545/545 [00:03<00:00, 141.44it/s]\n",
            "IDC → malignant: 100%|██████████| 283/283 [00:01<00:00, 150.71it/s]\n",
            "IDC → benign: 100%|██████████| 302/302 [00:02<00:00, 145.05it/s]\n",
            "IDC → malignant: 100%|██████████| 35/35 [00:00<00:00, 155.45it/s]\n",
            "IDC → benign: 100%|██████████| 375/375 [00:02<00:00, 136.99it/s]\n",
            "IDC → malignant: 100%|██████████| 542/542 [00:03<00:00, 151.15it/s]\n",
            "IDC → benign: 100%|██████████| 115/115 [00:00<00:00, 151.98it/s]\n",
            "IDC → malignant: 100%|██████████| 36/36 [00:00<00:00, 148.83it/s]\n",
            "IDC → benign: 100%|██████████| 1017/1017 [00:07<00:00, 137.51it/s]\n",
            "IDC → malignant: 100%|██████████| 110/110 [00:00<00:00, 132.83it/s]\n",
            "IDC → benign: 100%|██████████| 772/772 [00:05<00:00, 131.71it/s]\n",
            "IDC → malignant: 100%|██████████| 207/207 [00:01<00:00, 136.14it/s]\n",
            "IDC → benign: 100%|██████████| 805/805 [00:05<00:00, 143.14it/s]\n",
            "IDC → malignant: 100%|██████████| 328/328 [00:02<00:00, 140.78it/s]\n",
            "IDC → benign: 100%|██████████| 657/657 [00:04<00:00, 140.89it/s]\n",
            "IDC → malignant: 100%|██████████| 55/55 [00:00<00:00, 139.92it/s]\n",
            "IDC → benign: 100%|██████████| 1480/1480 [00:10<00:00, 141.23it/s]\n",
            "IDC → malignant: 100%|██████████| 162/162 [00:01<00:00, 150.80it/s]\n",
            "IDC → benign: 100%|██████████| 873/873 [00:05<00:00, 152.02it/s]\n",
            "IDC → malignant: 100%|██████████| 82/82 [00:00<00:00, 147.84it/s]\n",
            "IDC → benign: 100%|██████████| 978/978 [00:06<00:00, 140.98it/s]\n",
            "IDC → malignant: 100%|██████████| 75/75 [00:00<00:00, 146.65it/s]\n",
            "IDC → benign: 100%|██████████| 60/60 [00:00<00:00, 140.26it/s]\n",
            "IDC → malignant: 100%|██████████| 111/111 [00:00<00:00, 147.44it/s]\n",
            "IDC → benign: 100%|██████████| 578/578 [00:03<00:00, 147.51it/s]\n",
            "IDC → malignant: 100%|██████████| 397/397 [00:02<00:00, 151.26it/s]\n",
            "IDC → benign: 100%|██████████| 1421/1421 [00:10<00:00, 137.68it/s]\n",
            "IDC → malignant: 100%|██████████| 120/120 [00:00<00:00, 142.42it/s]\n",
            "IDC → benign: 100%|██████████| 420/420 [00:02<00:00, 149.55it/s]\n",
            "IDC → malignant: 100%|██████████| 190/190 [00:01<00:00, 152.42it/s]\n",
            "IDC → benign: 100%|██████████| 433/433 [00:02<00:00, 149.47it/s]\n",
            "IDC → malignant: 100%|██████████| 180/180 [00:01<00:00, 150.11it/s]\n",
            "IDC → benign: 100%|██████████| 314/314 [00:02<00:00, 147.42it/s]\n",
            "IDC → malignant: 100%|██████████| 181/181 [00:01<00:00, 148.64it/s]\n",
            "IDC → benign: 100%|██████████| 1485/1485 [00:09<00:00, 150.51it/s]\n",
            "IDC → malignant: 100%|██████████| 340/340 [00:02<00:00, 153.23it/s]\n",
            "IDC → benign: 100%|██████████| 28/28 [00:00<00:00, 141.72it/s]\n",
            "IDC → malignant: 100%|██████████| 83/83 [00:00<00:00, 116.46it/s]\n",
            "IDC → benign: 100%|██████████| 152/152 [00:01<00:00, 129.61it/s]\n",
            "IDC → malignant: 100%|██████████| 204/204 [00:01<00:00, 135.66it/s]\n",
            "IDC → benign: 100%|██████████| 1372/1372 [00:10<00:00, 130.22it/s]\n",
            "IDC → malignant: 100%|██████████| 369/369 [00:02<00:00, 145.16it/s]\n",
            "IDC → benign: 100%|██████████| 1379/1379 [00:09<00:00, 139.63it/s]\n",
            "IDC → malignant: 100%|██████████| 833/833 [00:06<00:00, 137.18it/s]\n",
            "IDC → benign: 100%|██████████| 487/487 [00:03<00:00, 136.64it/s]\n",
            "IDC → malignant: 100%|██████████| 209/209 [00:01<00:00, 139.18it/s]\n",
            "IDC → benign: 100%|██████████| 962/962 [00:07<00:00, 133.19it/s]\n",
            "IDC → malignant: 100%|██████████| 156/156 [00:01<00:00, 136.46it/s]\n",
            "IDC → benign: 100%|██████████| 418/418 [00:02<00:00, 139.52it/s]\n",
            "IDC → malignant: 100%|██████████| 99/99 [00:00<00:00, 131.04it/s]\n",
            "IDC → benign: 100%|██████████| 583/583 [00:04<00:00, 144.78it/s]\n",
            "IDC → malignant: 100%|██████████| 288/288 [00:02<00:00, 143.90it/s]\n",
            "IDC → benign: 100%|██████████| 1497/1497 [00:10<00:00, 149.35it/s]\n",
            "IDC → malignant: 100%|██████████| 137/137 [00:01<00:00, 133.98it/s]\n",
            "IDC → benign: 100%|██████████| 185/185 [00:01<00:00, 147.44it/s]\n",
            "IDC → malignant: 100%|██████████| 51/51 [00:00<00:00, 147.52it/s]\n",
            "IDC → benign: 100%|██████████| 1276/1276 [00:10<00:00, 121.12it/s]\n",
            "IDC → malignant: 100%|██████████| 30/30 [00:00<00:00, 150.63it/s]\n",
            "IDC → benign: 100%|██████████| 924/924 [00:13<00:00, 69.31it/s] \n",
            "IDC → malignant: 100%|██████████| 188/188 [00:01<00:00, 118.14it/s]\n",
            "IDC → benign: 100%|██████████| 857/857 [00:09<00:00, 89.52it/s] \n",
            "IDC → malignant: 100%|██████████| 178/178 [00:01<00:00, 138.42it/s]\n",
            "IDC → benign: 100%|██████████| 276/276 [00:01<00:00, 147.67it/s]\n",
            "IDC → malignant: 100%|██████████| 560/560 [00:03<00:00, 143.61it/s]\n",
            "IDC → benign: 100%|██████████| 112/112 [00:00<00:00, 124.66it/s]\n",
            "IDC → malignant: 100%|██████████| 46/46 [00:00<00:00, 134.02it/s]\n",
            "IDC → benign: 100%|██████████| 771/771 [00:05<00:00, 133.86it/s]\n",
            "IDC → malignant: 100%|██████████| 63/63 [00:00<00:00, 130.04it/s]\n",
            "IDC → benign: 100%|██████████| 1420/1420 [00:09<00:00, 142.44it/s]\n",
            "IDC → malignant: 100%|██████████| 361/361 [00:02<00:00, 143.22it/s]\n",
            "IDC → benign: 100%|██████████| 832/832 [00:05<00:00, 139.50it/s]\n",
            "IDC → malignant: 100%|██████████| 159/159 [00:01<00:00, 143.39it/s]\n",
            "IDC → benign: 100%|██████████| 360/360 [00:02<00:00, 139.84it/s]\n",
            "IDC → malignant: 100%|██████████| 1263/1263 [00:08<00:00, 141.07it/s]\n",
            "IDC → benign: 100%|██████████| 1602/1602 [00:11<00:00, 141.82it/s]\n",
            "IDC → malignant: 100%|██████████| 186/186 [00:01<00:00, 136.76it/s]\n",
            "IDC → benign: 100%|██████████| 681/681 [00:04<00:00, 139.31it/s]\n",
            "IDC → malignant: 100%|██████████| 180/180 [00:01<00:00, 120.46it/s]\n",
            "IDC → benign: 100%|██████████| 373/373 [00:02<00:00, 133.39it/s]\n",
            "IDC → malignant: 100%|██████████| 198/198 [00:01<00:00, 134.15it/s]\n",
            "IDC → benign: 100%|██████████| 1427/1427 [00:09<00:00, 143.26it/s]\n",
            "IDC → malignant: 100%|██████████| 161/161 [00:01<00:00, 150.44it/s]\n",
            "IDC → benign: 100%|██████████| 175/175 [00:01<00:00, 148.95it/s]\n",
            "IDC → malignant: 100%|██████████| 290/290 [00:01<00:00, 149.14it/s]\n",
            "IDC → benign: 100%|██████████| 369/369 [00:02<00:00, 150.28it/s]\n",
            "IDC → malignant: 100%|██████████| 239/239 [00:01<00:00, 143.94it/s]\n",
            "IDC → benign: 100%|██████████| 1147/1147 [00:08<00:00, 135.08it/s]\n",
            "IDC → malignant: 100%|██████████| 447/447 [00:03<00:00, 143.60it/s]\n",
            "IDC → benign: 100%|██████████| 604/604 [00:04<00:00, 139.53it/s]\n",
            "IDC → malignant: 100%|██████████| 111/111 [00:00<00:00, 140.94it/s]\n",
            "IDC → benign: 100%|██████████| 1020/1020 [00:07<00:00, 139.88it/s]\n",
            "IDC → malignant: 100%|██████████| 485/485 [00:03<00:00, 142.00it/s]\n",
            "IDC → benign: 100%|██████████| 190/190 [00:01<00:00, 150.92it/s]\n",
            "IDC → malignant: 100%|██████████| 29/29 [00:00<00:00, 159.49it/s]\n",
            "IDC → benign: 100%|██████████| 108/108 [00:00<00:00, 143.40it/s]\n",
            "IDC → malignant: 100%|██████████| 10/10 [00:00<00:00, 152.45it/s]\n",
            "IDC → benign: 100%|██████████| 636/636 [00:04<00:00, 147.05it/s]\n",
            "IDC → malignant: 100%|██████████| 409/409 [00:02<00:00, 137.83it/s]\n",
            "IDC → benign: 100%|██████████| 842/842 [00:06<00:00, 137.11it/s]\n",
            "IDC → malignant: 100%|██████████| 261/261 [00:01<00:00, 140.93it/s]\n",
            "IDC → benign: 100%|██████████| 1283/1283 [00:08<00:00, 143.45it/s]\n",
            "IDC → malignant: 100%|██████████| 160/160 [00:01<00:00, 124.44it/s]\n",
            "IDC → benign: 100%|██████████| 915/915 [00:06<00:00, 139.67it/s]\n",
            "IDC → malignant: 100%|██████████| 161/161 [00:01<00:00, 143.80it/s]\n",
            "IDC → benign: 100%|██████████| 1454/1454 [00:10<00:00, 139.80it/s]\n",
            "IDC → malignant: 100%|██████████| 89/89 [00:00<00:00, 150.40it/s]\n",
            "IDC → benign: 100%|██████████| 817/817 [00:05<00:00, 139.02it/s]\n",
            "IDC → malignant: 100%|██████████| 421/421 [00:02<00:00, 140.61it/s]\n",
            "IDC → benign: 100%|██████████| 687/687 [00:04<00:00, 145.22it/s]\n",
            "IDC → malignant: 100%|██████████| 127/127 [00:00<00:00, 143.67it/s]\n",
            "IDC → benign: 100%|██████████| 412/412 [00:02<00:00, 146.70it/s]\n",
            "IDC → malignant: 100%|██████████| 71/71 [00:00<00:00, 150.39it/s]\n",
            "IDC → benign: 100%|██████████| 636/636 [00:04<00:00, 144.49it/s]\n",
            "IDC → malignant: 100%|██████████| 515/515 [00:03<00:00, 148.92it/s]\n",
            "IDC → benign: 100%|██████████| 999/999 [00:07<00:00, 138.93it/s]\n",
            "IDC → malignant: 100%|██████████| 173/173 [00:01<00:00, 140.10it/s]\n",
            "IDC → benign: 100%|██████████| 838/838 [00:05<00:00, 142.14it/s]\n",
            "IDC → malignant: 100%|██████████| 388/388 [00:02<00:00, 141.62it/s]\n",
            "IDC → benign: 100%|██████████| 489/489 [00:03<00:00, 137.02it/s]\n",
            "IDC → malignant: 100%|██████████| 442/442 [00:03<00:00, 137.44it/s]\n",
            "IDC → benign: 100%|██████████| 1001/1001 [00:07<00:00, 137.80it/s]\n",
            "IDC → malignant: 100%|██████████| 201/201 [00:01<00:00, 140.19it/s]\n",
            "IDC → benign: 100%|██████████| 264/264 [00:01<00:00, 139.39it/s]\n",
            "IDC → malignant: 100%|██████████| 334/334 [00:02<00:00, 149.22it/s]\n",
            "IDC → benign: 100%|██████████| 1174/1174 [00:08<00:00, 136.35it/s]\n",
            "IDC → malignant: 100%|██████████| 225/225 [00:01<00:00, 137.53it/s]\n",
            "IDC → benign: 100%|██████████| 236/236 [00:01<00:00, 147.08it/s]\n",
            "IDC → malignant: 100%|██████████| 126/126 [00:00<00:00, 151.05it/s]\n",
            "IDC → benign: 100%|██████████| 521/521 [00:03<00:00, 143.96it/s]\n",
            "IDC → malignant: 100%|██████████| 446/446 [00:03<00:00, 146.84it/s]\n",
            "IDC → benign: 100%|██████████| 14/14 [00:00<00:00, 140.56it/s]\n",
            "IDC → malignant: 100%|██████████| 80/80 [00:00<00:00, 140.43it/s]\n",
            "IDC → benign: 100%|██████████| 1636/1636 [00:11<00:00, 137.70it/s]\n",
            "IDC → malignant: 100%|██████████| 41/41 [00:00<00:00, 153.00it/s]\n",
            "IDC → benign: 100%|██████████| 1119/1119 [00:08<00:00, 136.26it/s]\n",
            "IDC → malignant: 100%|██████████| 52/52 [00:00<00:00, 137.00it/s]\n",
            "IDC → benign: 100%|██████████| 337/337 [00:02<00:00, 139.50it/s]\n",
            "IDC → malignant: 100%|██████████| 321/321 [00:02<00:00, 136.86it/s]\n",
            "IDC → benign: 100%|██████████| 1368/1368 [00:09<00:00, 137.42it/s]\n",
            "IDC → malignant: 100%|██████████| 174/174 [00:01<00:00, 142.74it/s]\n",
            "IDC → benign: 100%|██████████| 733/733 [00:05<00:00, 143.29it/s]\n",
            "IDC → malignant: 100%|██████████| 96/96 [00:00<00:00, 140.87it/s]\n",
            "IDC → benign: 100%|██████████| 385/385 [00:02<00:00, 133.42it/s]\n",
            "IDC → malignant: 100%|██████████| 31/31 [00:00<00:00, 147.73it/s]\n",
            "IDC → benign: 100%|██████████| 1453/1453 [00:10<00:00, 140.46it/s]\n",
            "IDC → malignant: 100%|██████████| 451/451 [00:03<00:00, 145.92it/s]\n",
            "IDC → benign: 100%|██████████| 282/282 [00:01<00:00, 141.98it/s]\n",
            "IDC → malignant: 100%|██████████| 30/30 [00:00<00:00, 145.71it/s]\n",
            "IDC → benign: 100%|██████████| 1295/1295 [00:09<00:00, 143.52it/s]\n",
            "IDC → malignant: 100%|██████████| 167/167 [00:01<00:00, 146.99it/s]\n",
            "IDC → benign: 100%|██████████| 1938/1938 [00:13<00:00, 142.74it/s]\n",
            "IDC → malignant: 100%|██████████| 278/278 [00:02<00:00, 136.85it/s]\n",
            "IDC → benign: 100%|██████████| 720/720 [00:05<00:00, 136.03it/s]\n",
            "IDC → malignant: 100%|██████████| 322/322 [00:02<00:00, 136.15it/s]\n",
            "IDC → benign: 100%|██████████| 1060/1060 [00:07<00:00, 135.31it/s]\n",
            "IDC → malignant: 100%|██████████| 68/68 [00:00<00:00, 141.51it/s]\n",
            "IDC → benign: 100%|██████████| 225/225 [00:01<00:00, 136.51it/s]\n",
            "IDC → malignant: 100%|██████████| 310/310 [00:02<00:00, 143.28it/s]\n",
            "IDC → benign: 100%|██████████| 554/554 [00:03<00:00, 144.69it/s]\n",
            "IDC → malignant: 100%|██████████| 631/631 [00:04<00:00, 145.13it/s]\n",
            "IDC → benign: 100%|██████████| 634/634 [00:04<00:00, 143.90it/s]\n",
            "IDC → malignant: 100%|██████████| 727/727 [00:05<00:00, 136.83it/s]\n",
            "IDC → benign: 100%|██████████| 359/359 [00:02<00:00, 140.62it/s]\n",
            "IDC → malignant: 100%|██████████| 51/51 [00:00<00:00, 127.40it/s]\n",
            "IDC → benign: 100%|██████████| 1198/1198 [00:08<00:00, 143.26it/s]\n",
            "IDC → malignant: 100%|██████████| 128/128 [00:00<00:00, 144.76it/s]\n",
            "IDC → benign: 100%|██████████| 1306/1306 [00:09<00:00, 143.36it/s]\n",
            "IDC → malignant: 100%|██████████| 346/346 [00:02<00:00, 126.01it/s]\n",
            "IDC → benign: 100%|██████████| 494/494 [00:03<00:00, 132.51it/s]\n",
            "IDC → malignant: 100%|██████████| 70/70 [00:00<00:00, 135.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Copying BreaKHis images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "BreaKHis 40X → benign: 0it [00:00, ?it/s]\n",
            "BreaKHis 40X → malignant: 0it [00:00, ?it/s]\n",
            "BreaKHis 100X → benign: 0it [00:00, ?it/s]\n",
            "BreaKHis 100X → malignant: 0it [00:00, ?it/s]\n",
            "BreaKHis 200X → benign: 0it [00:00, ?it/s]\n",
            "BreaKHis 200X → malignant: 0it [00:00, ?it/s]\n",
            "BreaKHis 400X → benign: 0it [00:00, ?it/s]\n",
            "BreaKHis 400X → malignant: 0it [00:00, ?it/s]\n",
            "benign → train: 100%|██████████| 143091/143091 [1:05:35<00:00, 36.36it/s]\n",
            "benign → val: 100%|██████████| 15899/15899 [07:19<00:00, 36.21it/s]\n",
            "benign → test: 100%|██████████| 39748/39748 [18:29<00:00, 35.83it/s]\n",
            "malignant → train: 100%|██████████| 56725/56725 [27:25<00:00, 34.47it/s]\n",
            "malignant → val: 100%|██████████| 6303/6303 [03:08<00:00, 33.45it/s]\n",
            "malignant → test: 100%|██████████| 15758/15758 [07:58<00:00, 32.92it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Preprocessing complete.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os # For file and folder operations (e.g., creating directories, joining paths, checking existence).\n",
        "import shutil # For high-level file operations, like copying files.\n",
        "from glob import glob # For finding file paths that match a specified pattern (e.g., all .png files in a directory).\n",
        "from PIL import Image # From the Pillow library, used for opening, manipulating (like resizing), and saving image files.\n",
        "from tqdm import tqdm # For displaying smart progress bars, making loops visually trackable.\n",
        "from sklearn.model_selection import train_test_split # From scikit-learn, used for splitting datasets into random train and test (or train/val/test) subsets.\n",
        "\n",
        "import torch # Imports the PyTorch deep learning framework. (Although not directly used in this script, it's common in a larger project context).\n",
        "from torch.utils.data import DataLoader # Imports DataLoader (not directly used in this script, but typically used with PyTorch datasets).\n",
        "from torchvision import transforms, datasets # Imports transforms and datasets from torchvision (not directly used in this script, but related to data handling).\n",
        "\n",
        "# Adjust these paths to your system\n",
        "idc_root = r\"D:\\extra\\BC\\archive (1)\" # Defines the root directory for the IDC (Invasive Ductal Carcinoma) dataset. The 'r' prefix makes it a raw string, useful for Windows paths.\n",
        "# breakhis_root = r\"D:\\extra\\BC\\dataset_cancer_v1\\classification_binaria\" # This line is commented out, but it would define the root directory for the BreaKHis dataset.\n",
        "output_root = r\"D:\\extra\\BC\\merged_data\" # Defines the root directory where images from both datasets will be copied and merged.\n",
        "\n",
        "# Create output directories\n",
        "os.makedirs(f\"{output_root}/benign\", exist_ok=True) # Creates the 'benign' subfolder within 'output_root'. 'exist_ok=True' prevents an error if the directory already exists.\n",
        "os.makedirs(f\"{output_root}/malignant\", exist_ok=True) # Creates the 'malignant' subfolder within 'output_root'.\n",
        "\n",
        "### --- Copy IDC Images --- ###\n",
        "print(\"Copying IDC images...\") # Prints a message indicating the start of copying IDC images.\n",
        "for folder in glob(os.path.join(idc_root, \"*\")): # Loops over all immediate subfolders within the 'idc_root' directory. These subfolders typically represent patient IDs.\n",
        "    for label in ['0', '1']: # Loops over the two class labels: '0' (benign) and '1' (malignant).\n",
        "        class_name = \"benign\" if label == '0' else \"malignant\" # Assigns a human-readable class name based on the label ('0' -> \"benign\", '1' -> \"malignant\").\n",
        "        label_folder = os.path.join(folder, label) # Constructs the full path to the class-specific subfolder within the current patient's folder (e.g., 'patientID/0' or 'patientID/1').\n",
        "        if os.path.isdir(label_folder): # Checks if the constructed 'label_folder' actually exists and is a directory.\n",
        "            image_paths = glob(os.path.join(label_folder, \"*.png\")) # Finds all PNG image files within the current 'label_folder'.\n",
        "            for img_path in tqdm(image_paths, desc=f\"IDC → {class_name}\"): # Iterates through each image path with a progress bar.\n",
        "                try: # Starts a try-except block to handle potential errors during file operations.\n",
        "                    filename = os.path.basename(img_path) # Extracts just the filename (e.g., 'image.png') from the full path.\n",
        "                    # Copies the image to the 'output_root' under the respective 'benign' or 'malignant' subfolder.\n",
        "                    # It renames the file by adding an \"IDC_\" prefix and the numerical label (0 or 1) for clarity and to prevent name clashes.\n",
        "                    shutil.copy(img_path, f\"{output_root}/{class_name}/IDC_{label}_{filename}\") \n",
        "                except: # Catches any exception that occurs during the copy operation.\n",
        "                    continue # Skips to the next image if an error occurs, preventing the script from crashing.\n",
        "\n",
        "#--- Copy BreaKHis Images --- #\n",
        "print(\"Copying BreaKHis images...\") # Prints a message indicating the start of copying BreaKHis images.\n",
        "# NOTE: The `breakhis_root` variable is commented out in the CONFIG section. This part of the code will cause an error\n",
        "# because `breakhis_root` is not defined. You would need to uncomment and define `breakhis_root` for this section to run.\n",
        "for magnification in [\"40X\", \"100X\", \"200X\", \"400X\"]: # Loops through different magnification levels of the BreaKHis dataset.\n",
        "    for class_name in [\"benign\", \"malignant\"]: # Loops through the class names.\n",
        "        # Constructs the path to images for a specific magnification and class.\n",
        "        image_paths = glob(os.path.join(breakhis_root, magnification, class_name, \"*.png\"))\n",
        "        for img_path in tqdm(image_paths, desc=f\"BreaKHis {magnification} → {class_name}\"): # Iterates through images with a progress bar.\n",
        "            try: # Starts a try-except block.\n",
        "                filename = os.path.basename(img_path) # Extracts the filename.\n",
        "                new_name = f\"BreaKHis_{magnification}_{filename}\" # Creates a new filename with a \"BreaKHis_\" prefix and magnification level.\n",
        "                # Copies the image to the 'output_root' under the respective class subfolder with the new name.\n",
        "                shutil.copy(img_path, f\"{output_root}/{class_name}/{new_name}\")\n",
        "            except: # Catches any exception.\n",
        "                continue # Skips to the next image.\n",
        "\n",
        "### --- Resize and Split Dataset --- ###\n",
        "output_dir = r\"../Preprocessed_breakhis\" # Defines the output directory for the resized and split dataset. This path is relative to the current script's location.\n",
        "img_size = (224, 224) # Defines the target image size (width, height) for resizing.\n",
        "\n",
        "# Create output folders for the resized and split dataset\n",
        "for split in ['train', 'val', 'test']: # Loops through dataset splits.\n",
        "    for cls in ['benign', 'malignant']: # Loops through class names.\n",
        "        # Creates the nested directory structure (e.g., '../Preprocessed_breakhis/train/benign').\n",
        "        os.makedirs(f\"{output_dir}/{split}/{cls}\", exist_ok=True)\n",
        "\n",
        "def preprocess_and_split(class_name): # Defines a function to take images from the merged 'output_root', resize them, and split them into train/val/test.\n",
        "    image_paths = glob(f\"{output_root}/{class_name}/*.png\") # Finds all PNG images for the given 'class_name' in the merged 'output_root'.\n",
        "    # Splits the image paths into training (80%), test (20%).\n",
        "    train, test = train_test_split(image_paths, test_size=0.2, random_state=42)\n",
        "    # Further splits the 'train' set into actual training (90% of the 80%) and validation (10% of the 80%).\n",
        "    train, val = train_test_split(train, test_size=0.1, random_state=42)\n",
        "    \n",
        "    for split, paths in zip(['train', 'val', 'test'], [train, val, test]): # Loops through the three splits and their respective image paths.\n",
        "        for img_path in tqdm(paths, desc=f\"{class_name} → {split}\"): # Iterates through each image path with a progress bar.\n",
        "            try: # Starts a try-except block.\n",
        "                img = Image.open(img_path).convert('RGB') # Opens the image and ensures it's in RGB format.\n",
        "                img = img.resize(img_size) # Resizes the image to the predefined `img_size`.\n",
        "                # Saves the resized image to the appropriate split and class subfolder within the `output_dir`.\n",
        "                # `os.path.basename(img_path)` gets just the filename (e.g., 'IDC_0_image.png' or 'BreaKHis_40X_image.png').\n",
        "                img.save(f\"{output_dir}/{split}/{class_name}/{os.path.basename(img_path)}\")\n",
        "            except: # Catches any exception during image processing or saving.\n",
        "                continue # Skips to the next image if an error occurs.\n",
        "\n",
        "preprocess_and_split(\"benign\") # Calls the preprocessing and splitting function for \"benign\" images.\n",
        "preprocess_and_split(\"malignant\") # Calls the preprocessing and splitting function for \"malignant\" images.\n",
        "\n",
        "print(\"✅ Preprocessing complete.\") # Prints a final message indicating the completion of all preprocessing tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os # Imports the 'os' module for interacting with the operating system (e.g., path manipulation, directory creation).\n",
        "from glob import glob # Imports the 'glob' function for finding files matching specific patterns.\n",
        "from PIL import Image # Imports the 'Image' module from the Pillow library for image processing.\n",
        "from tqdm import tqdm # Imports 'tqdm' for displaying progress bars.\n",
        "from sklearn.model_selection import train_test_split # Imports 'train_test_split' from scikit-learn for splitting datasets.\n",
        "\n",
        "# === CONFIG ===\n",
        "idc_root = r\"D:\\thisisimp\\paper 2024\\archive_extracted\" # Defines the root directory of the IDC (Invasive Ductal Carcinoma) dataset.\n",
        "output_dir = r\"D:\\thisisimp\\working-bc\\Preprocessed_breakhis\" # Defines the output directory where preprocessed images will be saved. The comment suggests this is an existing folder for BreakHis data, implying IDC data will be merged into it.\n",
        "img_size = (224, 224) # Defines the target size for resizing images.\n",
        "\n",
        "# === PREPROCESS FUNCTION ===\n",
        "def preprocess_and_split(class_name, label_folder): # Defines a function to preprocess and split images for a given class.\n",
        "    print(f\"\\n📂 Processing {class_name} images in {label_folder}\") # Prints a message indicating which class and folder are being processed.\n",
        "    image_paths = glob(os.path.join(label_folder, \"*.png\")) # Finds all .png image files within the specified 'label_folder'.\n",
        "\n",
        "    # Split dataset: The images from the current `label_folder` are split into train, test, and validation sets.\n",
        "    # First, split into training (80%) and test (20%).\n",
        "    train, test = train_test_split(image_paths, test_size=0.2, random_state=42)\n",
        "    # Then, split the 'train' portion further into actual training (90% of the 80%, so 72% overall) and validation (10% of the 80%, so 8% overall).\n",
        "    train, val = train_test_split(train, test_size=0.1, random_state=42)\n",
        "\n",
        "    # Loop through each split (train, val, test) and their respective image paths.\n",
        "    for split, paths in zip(['train', 'val', 'test'], [train, val, test]):\n",
        "        # Iterate through each image path in the current split with a progress bar.\n",
        "        for img_path in tqdm(paths, desc=f\"IDC {class_name} → {split}\"):\n",
        "            try: # Use a try-except block to handle potential errors during image processing.\n",
        "                img = Image.open(img_path).convert('RGB') # Opens the image and converts it to RGB format (ensures 3 channels).\n",
        "                img = img.resize(img_size) # Resizes the image to the specified `img_size` (224x224).\n",
        "\n",
        "                # Creates the destination directory if it doesn't exist.\n",
        "                # This ensures `output_dir/split/class_name` (e.g., Preprocessed_breakhis/train/benign) exists before saving.\n",
        "                os.makedirs(os.path.join(output_dir, split, class_name), exist_ok=True)\n",
        "                \n",
        "                # Constructs a new filename to avoid conflicts and provide more context.\n",
        "                # It prefixes \"IDC_\" and the class name to the original filename.\n",
        "                filename = f\"IDC_{class_name}_{os.path.basename(img_path)}\"\n",
        "                # Constructs the full destination path for the processed image.\n",
        "                dest_path = os.path.join(output_dir, split, class_name, filename)\n",
        "                img.save(dest_path) # Saves the processed image to the destination path.\n",
        "            except Exception as e: # Catches any exception that occurs during image opening, resizing, or saving.\n",
        "                print(f\"⚠️ Error with {img_path}: {e}\") # Prints an error message.\n",
        "\n",
        "# === RUN FOR EACH FOLDER IN IDC ROOT ===\n",
        "print(\"🔍 Scanning IDC folders...\\n\") # Prints a message indicating the start of scanning IDC folders.\n",
        "# Iterates through each subfolder directly within the `idc_root` directory. These are typically patient ID folders.\n",
        "for folder in glob(os.path.join(idc_root, \"*\")):\n",
        "    # For each patient folder, iterate through the two possible label subfolders ('0' for benign, '1' for malignant).\n",
        "    for label in ['0', '1']:\n",
        "        class_name = \"benign\" if label == '0' else \"malignant\" # Determines the class name (\"benign\" or \"malignant\") based on the label.\n",
        "        label_folder = os.path.join(folder, label) # Constructs the full path to the specific label folder within the current patient's directory.\n",
        "        if os.path.isdir(label_folder): # Checks if the `label_folder` actually exists and is a directory.\n",
        "            preprocess_and_split(class_name, label_folder) # Calls the preprocessing function for the detected class and its folder.\n",
        "\n",
        "print(\"\\n IDC images added to BreakHis preprocessed folder.\") # Prints a final message indicating completion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📁 Found root folder: D:\\thisisimp\\paper 2024\\archive_extracted\n",
            "\n",
            "🔍 Found 280 patient folders.\n",
            "\n",
            "📦 10253: 479 benign, 70 malignant\n",
            "📦 10254: 772 benign, 76 malignant\n",
            "📦 10255: 181 benign, 91 malignant\n",
            "📦 10256: 351 benign, 117 malignant\n",
            "📦 10257: 427 benign, 208 malignant\n",
            "📦 10258: 422 benign, 108 malignant\n",
            "📦 10259: 1434 benign, 31 malignant\n",
            "📦 10260: 928 benign, 361 malignant\n",
            "📦 10261: 590 benign, 56 malignant\n",
            "📦 10262: 1053 benign, 754 malignant\n",
            "📦 10264: 617 benign, 587 malignant\n",
            "📦 10268: 2086 benign, 23 malignant\n",
            "📦 10269: 904 benign, 250 malignant\n",
            "📦 10272: 2150 benign, 25 malignant\n",
            "📦 10273: 811 benign, 1211 malignant\n",
            "📦 10274: 659 benign, 219 malignant\n",
            "📦 10275: 297 benign, 760 malignant\n",
            "📦 10276: 591 benign, 348 malignant\n",
            "📦 10277: 785 benign, 170 malignant\n",
            "📦 10278: 1068 benign, 91 malignant\n",
            "📦 10279: 1267 benign, 427 malignant\n",
            "📦 10282: 1835 benign, 198 malignant\n",
            "📦 10285: 1011 benign, 222 malignant\n",
            "📦 10286: 458 benign, 162 malignant\n",
            "📦 10288: 2231 benign, 47 malignant\n",
            "📦 10290: 1891 benign, 140 malignant\n",
            "📦 10291: 999 benign, 213 malignant\n",
            "📦 10292: 998 benign, 487 malignant\n",
            "📦 10293: 649 benign, 221 malignant\n",
            "📦 10295: 761 benign, 134 malignant\n",
            "📦 10299: 759 benign, 1347 malignant\n",
            "📦 10300: 1464 benign, 29 malignant\n",
            "📦 10301: 1074 benign, 342 malignant\n",
            "📦 10302: 598 benign, 1309 malignant\n",
            "📦 10303: 579 benign, 773 malignant\n",
            "📦 10304: 779 benign, 101 malignant\n",
            "📦 10305: 1802 benign, 19 malignant\n",
            "📦 10306: 751 benign, 273 malignant\n",
            "📦 10307: 915 benign, 80 malignant\n",
            "📦 10308: 1383 benign, 895 malignant\n",
            "📦 12241: 37 benign, 115 malignant\n",
            "📦 12242: 668 benign, 429 malignant\n",
            "📦 12626: 1088 benign, 254 malignant\n",
            "📦 12748: 168 benign, 198 malignant\n",
            "📦 12749: 1199 benign, 563 malignant\n",
            "📦 12750: 1413 benign, 21 malignant\n",
            "📦 12751: 849 benign, 967 malignant\n",
            "📦 12752: 464 benign, 635 malignant\n",
            "📦 12810: 914 benign, 252 malignant\n",
            "📦 12811: 125 benign, 126 malignant\n",
            "📦 12817: 362 benign, 572 malignant\n",
            "📦 12818: 666 benign, 945 malignant\n",
            "📦 12819: 1404 benign, 223 malignant\n",
            "📦 12820: 753 benign, 369 malignant\n",
            "📦 12821: 1066 benign, 319 malignant\n",
            "📦 12822: 490 benign, 271 malignant\n",
            "📦 12823: 556 benign, 447 malignant\n",
            "📦 12824: 597 benign, 110 malignant\n",
            "📦 12826: 963 benign, 174 malignant\n",
            "📦 12867: 851 benign, 575 malignant\n",
            "📦 12868: 500 benign, 361 malignant\n",
            "📦 12869: 778 benign, 18 malignant\n",
            "📦 12870: 788 benign, 41 malignant\n",
            "📦 12871: 146 benign, 36 malignant\n",
            "📦 12872: 711 benign, 69 malignant\n",
            "📦 12873: 49 benign, 232 malignant\n",
            "📦 12875: 331 benign, 43 malignant\n",
            "📦 12876: 50 benign, 105 malignant\n",
            "📦 12877: 272 benign, 33 malignant\n",
            "📦 12878: 1289 benign, 185 malignant\n",
            "📦 12879: 272 benign, 144 malignant\n",
            "📦 12880: 788 benign, 1147 malignant\n",
            "📦 12881: 115 benign, 158 malignant\n",
            "📦 12882: 238 benign, 154 malignant\n",
            "📦 12883: 276 benign, 73 malignant\n",
            "📦 12884: 533 benign, 236 malignant\n",
            "📦 12886: 240 benign, 287 malignant\n",
            "📦 12890: 1313 benign, 158 malignant\n",
            "📦 12891: 442 benign, 172 malignant\n",
            "📦 12892: 133 benign, 93 malignant\n",
            "📦 12893: 216 benign, 482 malignant\n",
            "📦 12894: 1066 benign, 650 malignant\n",
            "📦 12895: 939 benign, 741 malignant\n",
            "📦 12896: 424 benign, 83 malignant\n",
            "📦 12897: 565 benign, 296 malignant\n",
            "📦 12898: 372 benign, 208 malignant\n",
            "📦 12900: 573 benign, 450 malignant\n",
            "📦 12901: 578 benign, 230 malignant\n",
            "📦 12905: 1193 benign, 21 malignant\n",
            "📦 12906: 816 benign, 887 malignant\n",
            "📦 12907: 546 benign, 504 malignant\n",
            "📦 12908: 778 benign, 262 malignant\n",
            "📦 12909: 283 benign, 514 malignant\n",
            "📦 12910: 1496 benign, 222 malignant\n",
            "📦 12911: 1041 benign, 201 malignant\n",
            "📦 12929: 90 benign, 70 malignant\n",
            "📦 12930: 835 benign, 165 malignant\n",
            "📦 12931: 477 benign, 130 malignant\n",
            "📦 12932: 433 benign, 304 malignant\n",
            "📦 12933: 125 benign, 30 malignant\n",
            "📦 12934: 1500 benign, 504 malignant\n",
            "📦 12935: 611 benign, 615 malignant\n",
            "📦 12947: 378 benign, 452 malignant\n",
            "📦 12948: 80 benign, 87 malignant\n",
            "📦 12949: 344 benign, 464 malignant\n",
            "📦 12951: 808 benign, 330 malignant\n",
            "📦 12954: 1945 benign, 64 malignant\n",
            "📦 12955: 809 benign, 253 malignant\n",
            "📦 13018: 175 benign, 128 malignant\n",
            "📦 13019: 1069 benign, 441 malignant\n",
            "📦 13020: 336 benign, 50 malignant\n",
            "📦 13021: 1089 benign, 108 malignant\n",
            "📦 13022: 1056 benign, 286 malignant\n",
            "📦 13023: 112 benign, 115 malignant\n",
            "📦 13024: 624 benign, 186 malignant\n",
            "📦 13025: 465 benign, 296 malignant\n",
            "📦 13106: 979 benign, 155 malignant\n",
            "📦 13400: 1299 benign, 64 malignant\n",
            "📦 13401: 323 benign, 244 malignant\n",
            "📦 13402: 292 benign, 519 malignant\n",
            "📦 13403: 201 benign, 111 malignant\n",
            "📦 13404: 379 benign, 178 malignant\n",
            "📦 13458: 394 benign, 49 malignant\n",
            "📦 13459: 802 benign, 224 malignant\n",
            "📦 13460: 623 benign, 46 malignant\n",
            "📦 13461: 594 benign, 70 malignant\n",
            "📦 13462: 1028 benign, 726 malignant\n",
            "📦 13591: 907 benign, 128 malignant\n",
            "📦 13613: 827 benign, 630 malignant\n",
            "📦 13616: 656 benign, 701 malignant\n",
            "📦 13617: 299 benign, 56 malignant\n",
            "📦 13666: 377 benign, 30 malignant\n",
            "📦 13687: 303 benign, 151 malignant\n",
            "📦 13688: 215 benign, 127 malignant\n",
            "📦 13689: 510 benign, 76 malignant\n",
            "📦 13691: 927 benign, 264 malignant\n",
            "📦 13692: 272 benign, 335 malignant\n",
            "📦 13693: 1935 benign, 460 malignant\n",
            "📦 13694: 360 benign, 870 malignant\n",
            "📦 13916: 1268 benign, 365 malignant\n",
            "📦 14078: 100 benign, 121 malignant\n",
            "📦 14079: 435 benign, 455 malignant\n",
            "📦 14081: 117 benign, 209 malignant\n",
            "📦 14082: 281 benign, 197 malignant\n",
            "📦 14153: 579 benign, 210 malignant\n",
            "📦 14154: 691 benign, 829 malignant\n",
            "📦 14155: 671 benign, 1206 malignant\n",
            "📦 14156: 1201 benign, 197 malignant\n",
            "📦 14157: 990 benign, 488 malignant\n",
            "📦 14188: 586 benign, 123 malignant\n",
            "📦 14189: 581 benign, 421 malignant\n",
            "📦 14190: 458 benign, 491 malignant\n",
            "📦 14191: 723 benign, 617 malignant\n",
            "📦 14192: 837 benign, 195 malignant\n",
            "📦 14209: 33 benign, 309 malignant\n",
            "📦 14210: 469 benign, 104 malignant\n",
            "📦 14211: 1287 benign, 809 malignant\n",
            "📦 14212: 167 benign, 44 malignant\n",
            "📦 14213: 169 benign, 253 malignant\n",
            "📦 14304: 410 benign, 432 malignant\n",
            "📦 14305: 714 benign, 272 malignant\n",
            "📦 14306: 264 benign, 167 malignant\n",
            "📦 14321: 426 benign, 195 malignant\n",
            "📦 15471: 448 benign, 86 malignant\n",
            "📦 15472: 1490 benign, 214 malignant\n",
            "📦 15473: 553 benign, 885 malignant\n",
            "📦 15510: 705 benign, 356 malignant\n",
            "📦 15512: 79 benign, 143 malignant\n",
            "📦 15513: 815 benign, 54 malignant\n",
            "📦 15514: 197 benign, 441 malignant\n",
            "📦 15515: 1051 benign, 111 malignant\n",
            "📦 15516: 1016 benign, 275 malignant\n",
            "📦 15632: 373 benign, 120 malignant\n",
            "📦 15633: 114 benign, 337 malignant\n",
            "📦 15634: 439 benign, 370 malignant\n",
            "📦 15839: 105 benign, 134 malignant\n",
            "📦 15840: 862 benign, 244 malignant\n",
            "📦 15902: 706 benign, 461 malignant\n",
            "📦 15903: 418 benign, 621 malignant\n",
            "📦 16014: 497 benign, 209 malignant\n",
            "📦 16085: 1913 benign, 24 malignant\n",
            "📦 16165: 937 benign, 1174 malignant\n",
            "📦 16166: 615 benign, 675 malignant\n",
            "📦 16167: 96 benign, 96 malignant\n",
            "📦 16531: 191 benign, 58 malignant\n",
            "📦 16532: 339 benign, 128 malignant\n",
            "📦 16533: 240 benign, 127 malignant\n",
            "📦 16534: 21 benign, 42 malignant\n",
            "📦 16550: 2115 benign, 187 malignant\n",
            "📦 16551: 1899 benign, 284 malignant\n",
            "📦 16552: 150 benign, 37 malignant\n",
            "📦 16553: 327 benign, 353 malignant\n",
            "📦 16554: 269 benign, 448 malignant\n",
            "📦 16555: 315 benign, 85 malignant\n",
            "📦 16568: 545 benign, 283 malignant\n",
            "📦 16569: 302 benign, 35 malignant\n",
            "📦 16570: 375 benign, 542 malignant\n",
            "📦 16895: 115 benign, 36 malignant\n",
            "📦 16896: 1017 benign, 110 malignant\n",
            "📦 8863: 772 benign, 207 malignant\n",
            "📦 8864: 805 benign, 328 malignant\n",
            "📦 8865: 657 benign, 55 malignant\n",
            "📦 8867: 1480 benign, 162 malignant\n",
            "📦 8913: 873 benign, 82 malignant\n",
            "📦 8914: 978 benign, 75 malignant\n",
            "📦 8916: 60 benign, 111 malignant\n",
            "📦 8917: 578 benign, 397 malignant\n",
            "📦 8918: 1421 benign, 120 malignant\n",
            "📦 8950: 420 benign, 190 malignant\n",
            "📦 8951: 433 benign, 180 malignant\n",
            "📦 8955: 314 benign, 181 malignant\n",
            "📦 8956: 1485 benign, 340 malignant\n",
            "📦 8957: 28 benign, 83 malignant\n",
            "📦 8959: 152 benign, 204 malignant\n",
            "📦 8974: 1372 benign, 369 malignant\n",
            "📦 8975: 1379 benign, 833 malignant\n",
            "📦 8980: 487 benign, 209 malignant\n",
            "📦 8984: 962 benign, 156 malignant\n",
            "📦 9022: 418 benign, 99 malignant\n",
            "📦 9023: 583 benign, 288 malignant\n",
            "📦 9029: 1497 benign, 137 malignant\n",
            "📦 9035: 185 benign, 51 malignant\n",
            "📦 9036: 1276 benign, 30 malignant\n",
            "📦 9037: 924 benign, 188 malignant\n",
            "📦 9041: 857 benign, 178 malignant\n",
            "📦 9043: 276 benign, 560 malignant\n",
            "📦 9044: 112 benign, 46 malignant\n",
            "📦 9073: 771 benign, 63 malignant\n",
            "📦 9075: 1420 benign, 361 malignant\n",
            "📦 9076: 832 benign, 159 malignant\n",
            "📦 9077: 360 benign, 1263 malignant\n",
            "📦 9078: 1602 benign, 186 malignant\n",
            "📦 9081: 681 benign, 180 malignant\n",
            "📦 9083: 373 benign, 198 malignant\n",
            "📦 9123: 1427 benign, 161 malignant\n",
            "📦 9124: 175 benign, 290 malignant\n",
            "📦 9125: 369 benign, 239 malignant\n",
            "📦 9126: 1147 benign, 447 malignant\n",
            "📦 9135: 604 benign, 111 malignant\n",
            "📦 9173: 1020 benign, 485 malignant\n",
            "📦 9174: 190 benign, 29 malignant\n",
            "📦 9175: 108 benign, 10 malignant\n",
            "📦 9176: 636 benign, 409 malignant\n",
            "📦 9177: 842 benign, 261 malignant\n",
            "📦 9178: 1283 benign, 160 malignant\n",
            "📦 9181: 915 benign, 161 malignant\n",
            "📦 9225: 1454 benign, 89 malignant\n",
            "📦 9226: 817 benign, 421 malignant\n",
            "📦 9227: 687 benign, 127 malignant\n",
            "📦 9228: 412 benign, 71 malignant\n",
            "📦 9250: 636 benign, 515 malignant\n",
            "📦 9254: 999 benign, 173 malignant\n",
            "📦 9255: 838 benign, 388 malignant\n",
            "📦 9256: 489 benign, 442 malignant\n",
            "📦 9257: 1001 benign, 201 malignant\n",
            "📦 9258: 264 benign, 334 malignant\n",
            "📦 9259: 1174 benign, 225 malignant\n",
            "📦 9260: 236 benign, 126 malignant\n",
            "📦 9261: 521 benign, 446 malignant\n",
            "📦 9262: 14 benign, 80 malignant\n",
            "📦 9265: 1636 benign, 41 malignant\n",
            "📦 9266: 1119 benign, 52 malignant\n",
            "📦 9267: 337 benign, 321 malignant\n",
            "📦 9290: 1368 benign, 174 malignant\n",
            "📦 9291: 733 benign, 96 malignant\n",
            "📦 9319: 385 benign, 31 malignant\n",
            "📦 9320: 1453 benign, 451 malignant\n",
            "📦 9321: 282 benign, 30 malignant\n",
            "📦 9322: 1295 benign, 167 malignant\n",
            "📦 9323: 1938 benign, 278 malignant\n",
            "📦 9324: 720 benign, 322 malignant\n",
            "📦 9325: 1060 benign, 68 malignant\n",
            "📦 9344: 225 benign, 310 malignant\n",
            "📦 9345: 554 benign, 631 malignant\n",
            "📦 9346: 634 benign, 727 malignant\n",
            "📦 9347: 359 benign, 51 malignant\n",
            "📦 9381: 1198 benign, 128 malignant\n",
            "📦 9382: 1306 benign, 346 malignant\n",
            "📦 9383: 494 benign, 70 malignant\n",
            "📦 IDC_regular_ps50_idx5: 0 benign, 0 malignant\n",
            "\n",
            "✅ Summary:\n",
            "Total benign images: 198738\n",
            "Total malignant images: 78786\n"
          ]
        }
      ],
      "source": [
        "import os # Imports the 'os' module for interacting with the operating system, like checking paths and listing directories.\n",
        "from glob import glob # Imports the 'glob' function from the 'glob' module, used for finding files matching a specific pattern.\n",
        "\n",
        "idc_root = r\"D:\\thisisimp\\paper 2024\\archive_extracted\" # Defines the root directory path where the IDC (Invasive Ductal Carcinoma) image dataset is expected to be located. The 'r' prefix makes it a raw string, useful for Windows paths.\n",
        "\n",
        "if not os.path.exists(idc_root): # Checks if the specified root directory exists.\n",
        "    print(f\"❌ The folder does not exist: {idc_root}\") # If the folder does not exist, prints an error message.\n",
        "else:\n",
        "    print(f\"📁 Found root folder: {idc_root}\\n\") # If the folder exists, prints a confirmation message.\n",
        "\n",
        "    # Scans the 'idc_root' directory and creates a list of paths to its immediate subfolders.\n",
        "    # This assumes each subfolder represents a patient ID.\n",
        "    subfolders = [f.path for f in os.scandir(idc_root) if f.is_dir()]\n",
        "    print(f\"🔍 Found {len(subfolders)} patient folders.\\n\") # Prints the number of patient subfolders found.\n",
        "\n",
        "    total_benign = 0 # Initializes a counter for the total number of benign images across all patients.\n",
        "    total_malignant = 0 # Initializes a counter for the total number of malignant images across all patients.\n",
        "\n",
        "    for subfolder in subfolders: # Iterates through each patient subfolder.\n",
        "        benign_path = os.path.join(subfolder, \"0\") # Constructs the path to the '0' subfolder, which typically contains benign images.\n",
        "        malignant_path = os.path.join(subfolder, \"1\") # Constructs the path to the '1' subfolder, which typically contains malignant images.\n",
        "\n",
        "        # Counts the number of .png files in the 'benign_path' if the path exists, otherwise sets count to 0.\n",
        "        benign_count = len(glob(os.path.join(benign_path, \"*.png\"))) if os.path.exists(benign_path) else 0\n",
        "        # Counts the number of .png files in the 'malignant_path' if the path exists, otherwise sets count to 0.\n",
        "        malignant_count = len(glob(os.path.join(malignant_path, \"*.png\"))) if os.path.exists(malignant_path) else 0\n",
        "\n",
        "        total_benign += benign_count # Adds the current patient's benign count to the overall total.\n",
        "        total_malignant += malignant_count # Adds the current patient's malignant count to the overall total.\n",
        "\n",
        "        # Prints the image counts for the current patient subfolder.\n",
        "        print(f\"📦 {os.path.basename(subfolder)}: \" \n",
        "              f\"{benign_count} benign, {malignant_count} malignant\")\n",
        "\n",
        "    print(\"\\n✅ Summary:\") # Prints a header for the summary.\n",
        "    print(f\"Total benign images: {total_benign}\") # Prints the grand total of benign images.\n",
        "    print(f\"Total malignant images: {total_malignant}\") # Prints the grand total of malignant images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔍 Validating BreakHis directory structure...\n",
            "\n",
            "✅ Found 625 images in D:\\thisisimp\\paper 2024\\BreakHis\\BreakHis - Breast Cancer Histopathological Database\\dataset_cancer_v1\\dataset_cancer_v1\\classificacao_binaria\\40X\\benign\n",
            "✅ Found 1370 images in D:\\thisisimp\\paper 2024\\BreakHis\\BreakHis - Breast Cancer Histopathological Database\\dataset_cancer_v1\\dataset_cancer_v1\\classificacao_binaria\\40X\\malignant\n",
            "✅ Found 644 images in D:\\thisisimp\\paper 2024\\BreakHis\\BreakHis - Breast Cancer Histopathological Database\\dataset_cancer_v1\\dataset_cancer_v1\\classificacao_binaria\\100X\\benign\n",
            "✅ Found 1437 images in D:\\thisisimp\\paper 2024\\BreakHis\\BreakHis - Breast Cancer Histopathological Database\\dataset_cancer_v1\\dataset_cancer_v1\\classificacao_binaria\\100X\\malignant\n",
            "✅ Found 623 images in D:\\thisisimp\\paper 2024\\BreakHis\\BreakHis - Breast Cancer Histopathological Database\\dataset_cancer_v1\\dataset_cancer_v1\\classificacao_binaria\\200X\\benign\n",
            "✅ Found 1390 images in D:\\thisisimp\\paper 2024\\BreakHis\\BreakHis - Breast Cancer Histopathological Database\\dataset_cancer_v1\\dataset_cancer_v1\\classificacao_binaria\\200X\\malignant\n",
            "✅ Found 588 images in D:\\thisisimp\\paper 2024\\BreakHis\\BreakHis - Breast Cancer Histopathological Database\\dataset_cancer_v1\\dataset_cancer_v1\\classificacao_binaria\\400X\\benign\n",
            "✅ Found 1232 images in D:\\thisisimp\\paper 2024\\BreakHis\\BreakHis - Breast Cancer Histopathological Database\\dataset_cancer_v1\\dataset_cancer_v1\\classificacao_binaria\\400X\\malignant\n",
            "\n",
            "📥 Copying BreakHis images...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "40X → benign: 100%|██████████| 625/625 [00:09<00:00, 66.25it/s]\n",
            "40X → malignant: 100%|██████████| 1370/1370 [00:20<00:00, 67.87it/s]\n",
            "100X → benign: 100%|██████████| 644/644 [00:13<00:00, 47.42it/s]\n",
            "100X → malignant: 100%|██████████| 1437/1437 [00:48<00:00, 29.64it/s]\n",
            "200X → benign: 100%|██████████| 623/623 [00:11<00:00, 54.92it/s]\n",
            "200X → malignant: 100%|██████████| 1390/1390 [00:19<00:00, 71.62it/s]\n",
            "400X → benign: 100%|██████████| 588/588 [00:07<00:00, 73.60it/s]\n",
            "400X → malignant: 100%|██████████| 1232/1232 [00:17<00:00, 72.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📦 Processing class: benign\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "benign → train: 100%|██████████| 1785/1785 [00:57<00:00, 30.97it/s]\n",
            "benign → val: 100%|██████████| 199/199 [00:06<00:00, 29.24it/s]\n",
            "benign → test: 100%|██████████| 496/496 [00:14<00:00, 33.19it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📦 Processing class: malignant\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "malignant → train: 100%|██████████| 3908/3908 [02:08<00:00, 30.40it/s]\n",
            "malignant → val: 100%|██████████| 435/435 [00:12<00:00, 34.24it/s]\n",
            "malignant → test: 100%|██████████| 1086/1086 [00:33<00:00, 32.58it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ BreakHis preprocessing complete.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ==== CONFIGURATION ====\n",
        "breakhis_root = r\"D:\\thisisimp\\paper 2024\\BreakHis\\BreakHis - Breast Cancer Histopathological Database\\dataset_cancer_v1\\dataset_cancer_v1\\classificacao_binaria\"\n",
        "merged_output = r\".\\merged_data_breakhis\"\n",
        "final_output = r\".\\Preprocessed_breakhis\"\n",
        "magnifications = [\"40X\", \"100X\", \"200X\", \"400X\"]\n",
        "classes = [\"benign\", \"malignant\"]\n",
        "img_size = (224, 224)\n",
        "split_ratios = {'test': 0.2, 'val': 0.1}\n",
        "\n",
        "# ==== CREATE OUTPUT FOLDERS ====\n",
        "for cls in classes:\n",
        "    os.makedirs(os.path.join(merged_output, cls), exist_ok=True)\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        os.makedirs(os.path.join(final_output, split, cls), exist_ok=True)\n",
        "\n",
        "# ==== CHECK FOLDER STRUCTURE ====\n",
        "print(\"\\n🔍 Validating BreakHis directory structure...\\n\")\n",
        "all_ok = True\n",
        "for mag in magnifications:\n",
        "    for cls in classes:\n",
        "        folder_path = os.path.join(breakhis_root, mag, cls)\n",
        "        if not os.path.exists(folder_path):\n",
        "            print(f\"❌ Missing: {folder_path}\")\n",
        "            all_ok = False\n",
        "        else:\n",
        "            count = len(glob(os.path.join(folder_path, \"*.png\")))\n",
        "            print(f\"✅ Found {count} images in {folder_path}\")\n",
        "\n",
        "if not all_ok:\n",
        "    raise RuntimeError(\"❗ Dataset structure is incomplete. Aborting.\")\n",
        "\n",
        "# ==== COPY IMAGES ====\n",
        "print(\"\\n📥 Copying BreakHis images...\\n\")\n",
        "for mag in magnifications:\n",
        "    for cls in classes:\n",
        "        src_dir = os.path.join(breakhis_root, mag, cls)\n",
        "        images = glob(os.path.join(src_dir, \"*.png\"))\n",
        "        for img_path in tqdm(images, desc=f\"{mag} → {cls}\"):\n",
        "            try:\n",
        "                filename = os.path.basename(img_path)\n",
        "                new_name = f\"BreaKHis_{mag}_{filename}\"\n",
        "                dst_path = os.path.join(merged_output, cls, new_name)\n",
        "                shutil.copy(img_path, dst_path)\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Error copying {img_path}: {e}\")\n",
        "\n",
        "# ==== PREPROCESS + SPLIT ====\n",
        "def preprocess_and_split(class_name):\n",
        "    print(f\"\\n📦 Processing class: {class_name}\")\n",
        "    images = glob(os.path.join(merged_output, class_name, \"*.png\"))\n",
        "\n",
        "    # Split\n",
        "    train_imgs, test_imgs = train_test_split(images, test_size=split_ratios['test'], random_state=42)\n",
        "    train_imgs, val_imgs = train_test_split(train_imgs, test_size=split_ratios['val'], random_state=42)\n",
        "\n",
        "    # Organize\n",
        "    for split, img_list in zip(['train', 'val', 'test'], [train_imgs, val_imgs, test_imgs]):\n",
        "        for img_path in tqdm(img_list, desc=f\"{class_name} → {split}\"):\n",
        "            try:\n",
        "                img = Image.open(img_path).convert(\"RGB\")\n",
        "                img = img.resize(img_size)\n",
        "                dest_path = os.path.join(final_output, split, class_name, os.path.basename(img_path))\n",
        "                img.save(dest_path)\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Error processing {img_path}: {e}\")\n",
        "\n",
        "# Run preprocessing\n",
        "for cls in classes:\n",
        "    preprocess_and_split(cls)\n",
        "\n",
        "print(\"\\n✅ BreakHis preprocessing complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 Validating BreakHis directory structure...\n",
            "\n",
            "✅ Found 625 images in: D:\\thisisimp\\paper 2024\\BreakHis\\BreakHis - Breast Cancer Histopathological Database\\dataset_cancer_v1\\dataset_cancer_v1\\classificacao_binaria\\40X\\benign\n",
            "✅ Found 1370 images in: D:\\thisisimp\\paper 2024\\BreakHis\\BreakHis - Breast Cancer Histopathological Database\\dataset_cancer_v1\\dataset_cancer_v1\\classificacao_binaria\\40X\\malignant\n",
            "✅ Found 644 images in: D:\\thisisimp\\paper 2024\\BreakHis\\BreakHis - Breast Cancer Histopathological Database\\dataset_cancer_v1\\dataset_cancer_v1\\classificacao_binaria\\100X\\benign\n",
            "✅ Found 1437 images in: D:\\thisisimp\\paper 2024\\BreakHis\\BreakHis - Breast Cancer Histopathological Database\\dataset_cancer_v1\\dataset_cancer_v1\\classificacao_binaria\\100X\\malignant\n",
            "✅ Found 623 images in: D:\\thisisimp\\paper 2024\\BreakHis\\BreakHis - Breast Cancer Histopathological Database\\dataset_cancer_v1\\dataset_cancer_v1\\classificacao_binaria\\200X\\benign\n",
            "✅ Found 1390 images in: D:\\thisisimp\\paper 2024\\BreakHis\\BreakHis - Breast Cancer Histopathological Database\\dataset_cancer_v1\\dataset_cancer_v1\\classificacao_binaria\\200X\\malignant\n",
            "✅ Found 588 images in: D:\\thisisimp\\paper 2024\\BreakHis\\BreakHis - Breast Cancer Histopathological Database\\dataset_cancer_v1\\dataset_cancer_v1\\classificacao_binaria\\400X\\benign\n",
            "✅ Found 1232 images in: D:\\thisisimp\\paper 2024\\BreakHis\\BreakHis - Breast Cancer Histopathological Database\\dataset_cancer_v1\\dataset_cancer_v1\\classificacao_binaria\\400X\\malignant\n",
            "\n",
            "🎉 All expected folders are present.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from glob import glob\n",
        "\n",
        "# Set your root path\n",
        "breakhis_root = r\"D:\\thisisimp\\paper 2024\\BreakHis\\BreakHis - Breast Cancer Histopathological Database\\dataset_cancer_v1\\dataset_cancer_v1\\classificacao_binaria\"\n",
        "\n",
        "# Define expected structure\n",
        "magnifications = [\"40X\", \"100X\", \"200X\", \"400X\"]\n",
        "classes = [\"benign\", \"malignant\"]\n",
        "\n",
        "all_ok = True  # flag to track overall status\n",
        "\n",
        "print(\"🔍 Validating BreakHis directory structure...\\n\")\n",
        "\n",
        "for mag in magnifications:\n",
        "    mag_path = os.path.join(breakhis_root, mag)\n",
        "    if not os.path.exists(mag_path):\n",
        "        print(f\"❌ Missing magnification folder: {mag_path}\")\n",
        "        all_ok = False\n",
        "        continue\n",
        "\n",
        "    for cls in classes:\n",
        "        class_path = os.path.join(mag_path, cls)\n",
        "        if not os.path.exists(class_path):\n",
        "            print(f\"❌ Missing class folder: {class_path}\")\n",
        "            all_ok = False\n",
        "            continue\n",
        "\n",
        "        # Check if folder has image files\n",
        "        images = glob(os.path.join(class_path, \"*.png\"))\n",
        "        if len(images) == 0:\n",
        "            print(f\"⚠️ No PNG images found in: {class_path}\")\n",
        "        else:\n",
        "            print(f\"✅ Found {len(images)} images in: {class_path}\")\n",
        "\n",
        "if all_ok:\n",
        "    print(\"\\n🎉 All expected folders are present.\")\n",
        "else:\n",
        "    print(\"\\n🚫 Some folders are missing. Please check paths.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔄 Merging IDC images into: D:\\thisisimp\\working-bc\\merged_data_breakhis\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IDC → benign: 100%|██████████| 479/479 [00:05<00:00, 80.62it/s]\n",
            "IDC → malignant: 100%|██████████| 70/70 [00:00<00:00, 78.03it/s]\n",
            "IDC → benign: 100%|██████████| 772/772 [00:09<00:00, 82.33it/s]\n",
            "IDC → malignant: 100%|██████████| 76/76 [00:00<00:00, 83.50it/s]\n",
            "IDC → benign: 100%|██████████| 181/181 [00:02<00:00, 79.81it/s]\n",
            "IDC → malignant: 100%|██████████| 91/91 [00:01<00:00, 84.12it/s]\n",
            "IDC → benign: 100%|██████████| 351/351 [00:04<00:00, 79.49it/s]\n",
            "IDC → malignant: 100%|██████████| 117/117 [00:01<00:00, 76.71it/s]\n",
            "IDC → benign:   3%|▎         | 11/427 [00:00<00:06, 64.21it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m     new_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIDC_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     31\u001b[0m     dest \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_root, class_name, new_name)\n\u001b[1;32m---> 32\u001b[0m     shutil\u001b[38;5;241m.\u001b[39mcopy(img_path, dest)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m⚠️ Error copying \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\Dell\\anaconda3\\Lib\\shutil.py:419\u001b[0m, in \u001b[0;36mcopy\u001b[1;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(dst):\n\u001b[0;32m    418\u001b[0m     dst \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dst, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(src))\n\u001b[1;32m--> 419\u001b[0m copyfile(src, dst, follow_symlinks\u001b[38;5;241m=\u001b[39mfollow_symlinks)\n\u001b[0;32m    420\u001b[0m copymode(src, dst, follow_symlinks\u001b[38;5;241m=\u001b[39mfollow_symlinks)\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dst\n",
            "File \u001b[1;32mc:\\Users\\Dell\\anaconda3\\Lib\\shutil.py:256\u001b[0m, in \u001b[0;36mcopyfile\u001b[1;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[0;32m    254\u001b[0m     os\u001b[38;5;241m.\u001b[39msymlink(os\u001b[38;5;241m.\u001b[39mreadlink(src), dst)\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 256\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(src, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fsrc:\n\u001b[0;32m    257\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(dst, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fdst:\n\u001b[0;32m    259\u001b[0m                 \u001b[38;5;66;03m# macOS\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# import os\n",
        "# import shutil\n",
        "# from glob import glob\n",
        "# from PIL import Image\n",
        "# from tqdm import tqdm\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# # === PATH CONFIGURATION ===\n",
        "# idc_root = r\"D:\\thisisimp\\paper 2024\\archive_extracted\"  # IDC source\n",
        "# output_root = r\"D:\\thisisimp\\working-bc\\merged_data_breakhis\"                 # Merge target (already used by BreakHis)\n",
        "# output_dir = r\"D:\\thisisimp\\working-bc\\Preprocessed_breakhis\"                 # Preprocessed BreakHis directory\n",
        "# img_size = (224, 224)\n",
        "\n",
        "# # === Ensure merge folders exist ===\n",
        "# os.makedirs(os.path.join(output_root, \"benign\"), exist_ok=True)\n",
        "# os.makedirs(os.path.join(output_root, \"malignant\"), exist_ok=True)\n",
        "\n",
        "# # === STEP 1: Merge IDC images ===\n",
        "# print(\"\\n🔄 Merging IDC images into:\", output_root)\n",
        "\n",
        "# for folder in glob(os.path.join(idc_root, \"*\")):\n",
        "#     for label in ['0', '1']:\n",
        "#         class_name = \"benign\" if label == '0' else \"malignant\"\n",
        "#         label_folder = os.path.join(folder, label)\n",
        "#         if os.path.isdir(label_folder):\n",
        "#             image_paths = glob(os.path.join(label_folder, \"*.png\"))\n",
        "#             for img_path in tqdm(image_paths, desc=f\"IDC → {class_name}\"):\n",
        "#                 try:\n",
        "#                     filename = os.path.basename(img_path)\n",
        "#                     new_name = f\"IDC_{label}_{filename}\"\n",
        "#                     dest = os.path.join(output_root, class_name, new_name)\n",
        "#                     shutil.copy(img_path, dest)\n",
        "#                 except Exception as e:\n",
        "#                     print(f\"⚠️ Error copying {img_path}: {e}\")\n",
        "\n",
        "# # === STEP 2: Preprocess and append to existing BreakHis splits ===\n",
        "# print(\"\\n🧪 Preprocessing and adding IDC images to BreakHis train/val/test folders...\")\n",
        "\n",
        "# def preprocess_and_split(class_name):\n",
        "#     image_paths = glob(os.path.join(output_root, class_name, \"*.png\"))\n",
        "#     if not image_paths:\n",
        "#         print(f\"⚠️ No images found in: {os.path.join(output_root, class_name)}\")\n",
        "#         return\n",
        "\n",
        "#     train, test = train_test_split(image_paths, test_size=0.2, random_state=42)\n",
        "#     train, val = train_test_split(train, test_size=0.1, random_state=42)\n",
        "\n",
        "#     for split, paths in zip(['train', 'val', 'test'], [train, val, test]):\n",
        "#         for img_path in tqdm(paths, desc=f\"{class_name} → {split}\"):\n",
        "#             try:\n",
        "#                 img = Image.open(img_path).convert('RGB')\n",
        "#                 img = img.resize(img_size)\n",
        "#                 filename = os.path.basename(img_path)\n",
        "#                 save_path = os.path.join(output_dir, split, class_name, filename)\n",
        "#                 img.save(save_path)\n",
        "#             except Exception as e:\n",
        "#                 print(f\"⚠️ Error processing {img_path}: {e}\")\n",
        "\n",
        "# preprocess_and_split(\"benign\")\n",
        "# preprocess_and_split(\"malignant\")\n",
        "\n",
        "# print(\"\\n✅ Done! IDC images are now merged and preprocessed into BreakHis dataset.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Initialize model\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model = BreastNet().to(device)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wlmDMgLS3kq",
        "outputId": "7f1cffe3-94b7-4b34-a866-eded9edbb2d7"
      },
      "outputs": [],
      "source": [
        "# from torchvision import datasets, transforms\n",
        "# from torch.utils.data import DataLoader\n",
        "# import torch.nn.functional as F\n",
        "# import torch.nn as nn\n",
        "# import torch\n",
        "\n",
        "# # Paths and Device\n",
        "# data_root = r\"D:\\extra\\BC\\Preprocessed\"\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# # Transforms\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.Resize((224, 224)),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize([0.5], [0.5])\n",
        "# ])\n",
        "\n",
        "# # Loaders\n",
        "# batch_size = 32\n",
        "# train_loader = DataLoader(datasets.ImageFolder(f\"{data_root}/train\", transform=transform), batch_size=batch_size, shuffle=True)\n",
        "# val_loader   = DataLoader(datasets.ImageFolder(f\"{data_root}/val\", transform=transform), batch_size=batch_size, shuffle=False)\n",
        "# test_loader  = DataLoader(datasets.ImageFolder(f\"{data_root}/test\", transform=transform), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# # Define the model (assumes BreastNet from earlier)\n",
        "# model = BreastNet().to(device)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# # Training Loop\n",
        "# num_epochs = 10\n",
        "# for epoch in range(num_epochs):\n",
        "#     model.train()\n",
        "#     total, correct, running_loss = 0, 0, 0\n",
        "#     for images, labels in train_loader:\n",
        "#         images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs = model(images)\n",
        "#         loss = criterion(outputs, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         running_loss += loss.item()\n",
        "#         _, predicted = torch.max(outputs, 1)\n",
        "#         total += labels.size(0)\n",
        "#         correct += (predicted == labels).sum().item()\n",
        "\n",
        "#     train_acc = 100 * correct / total\n",
        "#     train_loss = running_loss / len(train_loader)\n",
        "\n",
        "#     # Validation Step\n",
        "#     model.eval()\n",
        "#     val_total, val_correct, val_loss = 0, 0, 0\n",
        "#     with torch.no_grad():\n",
        "#         for images, labels in val_loader:\n",
        "#             images, labels = images.to(device), labels.to(device)\n",
        "#             outputs = model(images)\n",
        "#             loss = criterion(outputs, labels)\n",
        "#             val_loss += loss.item()\n",
        "#             _, predicted = torch.max(outputs, 1)\n",
        "#             val_total += labels.size(0)\n",
        "#             val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "#     val_acc = 100 * val_correct / val_total\n",
        "#     val_loss = val_loss / len(val_loader)\n",
        "\n",
        "#     print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
        "#           f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.2f}% | \"\n",
        "#           f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%\")\n",
        "\n",
        "# print(\"✅ Training Complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NX95FACsGudK"
      },
      "outputs": [],
      "source": [
        "# # Import required libraries\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# from torchvision import datasets, transforms\n",
        "# from torch.utils.data import DataLoader, random_split\n",
        "# import matplotlib.pyplot as plt\n",
        "# import os\n",
        "\n",
        "# # Define Channel Attention block\n",
        "# class ChannelAttention(nn.Module):\n",
        "#     def __init__(self, in_planes, ratio=8):\n",
        "#         super(ChannelAttention, self).__init__()\n",
        "#         self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "#         self.fc1 = nn.Conv2d(in_planes, in_planes // ratio, kernel_size=1, bias=False)\n",
        "#         self.relu = nn.ReLU()\n",
        "#         self.fc2 = nn.Conv2d(in_planes // ratio, in_planes, kernel_size=1, bias=False)\n",
        "#         self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         out = self.global_avg_pool(x)\n",
        "#         out = self.fc1(out)\n",
        "#         out = self.relu(out)\n",
        "#         out = self.fc2(out)\n",
        "#         return x * self.sigmoid(out)\n",
        "\n",
        "# # Define BreastNet model\n",
        "# class BreastNet(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(BreastNet, self).__init__()\n",
        "#         self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "#         self.bn1 = nn.BatchNorm2d(32)\n",
        "#         self.ca1 = ChannelAttention(32)\n",
        "#         self.pool = nn.AdaptiveAvgPool2d((8, 8))\n",
        "#         self.fc1 = nn.Linear(32 * 8 * 8, 2)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = F.relu(self.bn1(self.conv1(x)))\n",
        "#         x = self.ca1(x)\n",
        "#         x = self.pool(x)\n",
        "#         x = x.view(x.size(0), -1)\n",
        "#         x = self.fc1(x)\n",
        "#         return x\n",
        "\n",
        "# # Set device for training\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# # Image preprocessing\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.Resize((128, 128)),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize([0.5], [0.5])\n",
        "# ])\n",
        "\n",
        "# # Load dataset from directory structure\n",
        "# source_path = \"/content/Preprocessed/train\"\n",
        "# dataset = datasets.ImageFolder(root=source_path, transform=transform)\n",
        "\n",
        "# # Split into 80% train and 20% validation\n",
        "# train_size = int(0.8 * len(dataset))\n",
        "# val_size = len(dataset) - train_size\n",
        "# train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# # Loaders\n",
        "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# # Initialize model, loss function, optimizer\n",
        "# model = BreastNet().to(device)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# # Train model\n",
        "# num_epochs = 10\n",
        "# train_losses, val_losses = [], []\n",
        "\n",
        "# for epoch in range(num_epochs):\n",
        "#     model.train()\n",
        "#     train_loss, correct_train, total_train = 0, 0, 0\n",
        "\n",
        "#     for images, labels in train_loader:\n",
        "#         images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs = model(images)\n",
        "#         loss = criterion(outputs, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         train_loss += loss.item()\n",
        "#         _, predicted = torch.max(outputs, 1)\n",
        "#         total_train += labels.size(0)\n",
        "#         correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "#     train_losses.append(train_loss / len(train_loader))\n",
        "#     train_acc = 100 * correct_train / total_train\n",
        "\n",
        "#     model.eval()\n",
        "#     val_loss, correct_val, total_val = 0, 0, 0\n",
        "#     with torch.no_grad():\n",
        "#         for images, labels in val_loader:\n",
        "#             images, labels = images.to(device), labels.to(device)\n",
        "#             outputs = model(images)\n",
        "#             loss = criterion(outputs, labels)\n",
        "\n",
        "#             val_loss += loss.item()\n",
        "#             _, predicted = torch.max(outputs, 1)\n",
        "#             total_val += labels.size(0)\n",
        "#             correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "#     val_losses.append(val_loss / len(val_loader))\n",
        "#     val_acc = 100 * correct_val / total_val\n",
        "\n",
        "#     print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_losses[-1]:.4f}, Acc: {train_acc:.2f}% | Val Loss: {val_losses[-1]:.4f}, Acc: {val_acc:.2f}%\")\n",
        "\n",
        "# # Plot loss curves\n",
        "# plt.figure(figsize=(10, 5))\n",
        "# plt.plot(train_losses, label='Training Loss')\n",
        "# plt.plot(val_losses, label='Validation Loss')\n",
        "# plt.xlabel(\"Epoch\")\n",
        "# plt.ylabel(\"Loss\")\n",
        "# plt.title(\"Training vs Validation Loss\")\n",
        "# plt.legend()\n",
        "# plt.grid(True)\n",
        "# plt.show()\n",
        "\n",
        "# # Save model\n",
        "# model_path = \"breastnet_model.pth\"\n",
        "# torch.save(model.state_dict(), model_path)\n",
        "# print(f\"Model saved as {model_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Using device: cuda\n",
            "[INFO] Loading datasets...\n",
            "[INFO] Datasets loaded successfully.\n",
            "[INFO] Starting training...\n",
            "\n",
            "[Epoch 1/10]\n",
            "  [Batch 10/6245] Loss: 0.1573\n",
            "  [Batch 20/6245] Loss: 0.3558\n",
            "  [Batch 30/6245] Loss: 0.3544\n",
            "  [Batch 40/6245] Loss: 0.3608\n",
            "  [Batch 50/6245] Loss: 0.5345\n",
            "  [Batch 60/6245] Loss: 0.3699\n",
            "  [Batch 70/6245] Loss: 0.3732\n",
            "  [Batch 80/6245] Loss: 0.2773\n",
            "  [Batch 90/6245] Loss: 0.3546\n",
            "  [Batch 100/6245] Loss: 0.4487\n",
            "  [Batch 110/6245] Loss: 0.2303\n",
            "  [Batch 120/6245] Loss: 0.3035\n",
            "  [Batch 130/6245] Loss: 0.3860\n",
            "  [Batch 140/6245] Loss: 0.3410\n",
            "  [Batch 150/6245] Loss: 0.2580\n",
            "  [Batch 160/6245] Loss: 0.4476\n",
            "  [Batch 170/6245] Loss: 0.3763\n",
            "  [Batch 180/6245] Loss: 0.5279\n",
            "  [Batch 190/6245] Loss: 0.3042\n",
            "  [Batch 200/6245] Loss: 0.3583\n",
            "  [Batch 210/6245] Loss: 0.3812\n",
            "  [Batch 220/6245] Loss: 0.2765\n",
            "  [Batch 230/6245] Loss: 0.3992\n",
            "  [Batch 240/6245] Loss: 0.7122\n",
            "  [Batch 250/6245] Loss: 0.2818\n",
            "  [Batch 260/6245] Loss: 0.3970\n",
            "  [Batch 270/6245] Loss: 0.3962\n",
            "  [Batch 280/6245] Loss: 0.3946\n",
            "  [Batch 290/6245] Loss: 0.4976\n",
            "  [Batch 300/6245] Loss: 0.4178\n",
            "  [Batch 310/6245] Loss: 0.3165\n",
            "  [Batch 320/6245] Loss: 0.4600\n",
            "  [Batch 330/6245] Loss: 0.3568\n",
            "  [Batch 340/6245] Loss: 0.3010\n",
            "  [Batch 350/6245] Loss: 0.3144\n",
            "  [Batch 360/6245] Loss: 0.4522\n",
            "  [Batch 370/6245] Loss: 0.5101\n",
            "  [Batch 380/6245] Loss: 0.6091\n",
            "  [Batch 390/6245] Loss: 0.5404\n",
            "  [Batch 400/6245] Loss: 0.4752\n",
            "  [Batch 410/6245] Loss: 0.2018\n",
            "  [Batch 420/6245] Loss: 0.4040\n",
            "  [Batch 430/6245] Loss: 0.6226\n",
            "  [Batch 440/6245] Loss: 0.3448\n",
            "  [Batch 450/6245] Loss: 0.2527\n",
            "  [Batch 460/6245] Loss: 0.3606\n",
            "  [Batch 470/6245] Loss: 0.3564\n",
            "  [Batch 480/6245] Loss: 0.3538\n",
            "  [Batch 490/6245] Loss: 0.5272\n",
            "  [Batch 500/6245] Loss: 0.3188\n",
            "  [Batch 510/6245] Loss: 0.3376\n",
            "  [Batch 520/6245] Loss: 0.3689\n",
            "  [Batch 530/6245] Loss: 0.3884\n",
            "  [Batch 540/6245] Loss: 0.4503\n",
            "  [Batch 550/6245] Loss: 0.3073\n",
            "  [Batch 560/6245] Loss: 0.4886\n",
            "  [Batch 570/6245] Loss: 0.4709\n",
            "  [Batch 580/6245] Loss: 0.4217\n",
            "  [Batch 590/6245] Loss: 0.4122\n",
            "  [Batch 600/6245] Loss: 0.2783\n",
            "  [Batch 610/6245] Loss: 0.2889\n",
            "  [Batch 620/6245] Loss: 0.4406\n",
            "  [Batch 630/6245] Loss: 0.3229\n",
            "  [Batch 640/6245] Loss: 0.4741\n",
            "  [Batch 650/6245] Loss: 0.5398\n",
            "  [Batch 660/6245] Loss: 0.4564\n",
            "  [Batch 670/6245] Loss: 0.3643\n",
            "  [Batch 680/6245] Loss: 0.3653\n",
            "  [Batch 690/6245] Loss: 0.3193\n",
            "  [Batch 700/6245] Loss: 0.4968\n",
            "  [Batch 710/6245] Loss: 0.4590\n",
            "  [Batch 720/6245] Loss: 0.2170\n",
            "  [Batch 730/6245] Loss: 0.5530\n",
            "  [Batch 740/6245] Loss: 0.3639\n",
            "  [Batch 750/6245] Loss: 0.3468\n",
            "  [Batch 760/6245] Loss: 0.3330\n",
            "  [Batch 770/6245] Loss: 0.3962\n",
            "  [Batch 780/6245] Loss: 0.4008\n",
            "  [Batch 790/6245] Loss: 0.5613\n",
            "  [Batch 800/6245] Loss: 0.4716\n",
            "  [Batch 810/6245] Loss: 0.4530\n",
            "  [Batch 820/6245] Loss: 0.2768\n",
            "  [Batch 830/6245] Loss: 0.4041\n",
            "  [Batch 840/6245] Loss: 0.3276\n",
            "  [Batch 850/6245] Loss: 0.2880\n",
            "  [Batch 860/6245] Loss: 0.2659\n",
            "  [Batch 870/6245] Loss: 0.2725\n",
            "  [Batch 880/6245] Loss: 0.3361\n",
            "  [Batch 890/6245] Loss: 0.3964\n",
            "  [Batch 900/6245] Loss: 0.5243\n",
            "  [Batch 910/6245] Loss: 0.4802\n",
            "  [Batch 920/6245] Loss: 0.2700\n",
            "  [Batch 930/6245] Loss: 0.2826\n",
            "  [Batch 940/6245] Loss: 0.6551\n",
            "  [Batch 950/6245] Loss: 0.3629\n",
            "  [Batch 960/6245] Loss: 0.2956\n",
            "  [Batch 970/6245] Loss: 0.3500\n",
            "  [Batch 980/6245] Loss: 0.4685\n",
            "  [Batch 990/6245] Loss: 0.3942\n",
            "  [Batch 1000/6245] Loss: 0.5980\n",
            "  [Batch 1010/6245] Loss: 0.3193\n",
            "  [Batch 1020/6245] Loss: 0.4671\n",
            "  [Batch 1030/6245] Loss: 0.3813\n",
            "  [Batch 1040/6245] Loss: 0.2864\n",
            "  [Batch 1050/6245] Loss: 0.4360\n",
            "  [Batch 1060/6245] Loss: 0.3560\n",
            "  [Batch 1070/6245] Loss: 0.5036\n",
            "  [Batch 1080/6245] Loss: 0.2743\n",
            "  [Batch 1090/6245] Loss: 0.4618\n",
            "  [Batch 1100/6245] Loss: 0.4012\n",
            "  [Batch 1110/6245] Loss: 0.4418\n",
            "  [Batch 1120/6245] Loss: 0.1939\n",
            "  [Batch 1130/6245] Loss: 0.3405\n",
            "  [Batch 1140/6245] Loss: 0.1916\n",
            "  [Batch 1150/6245] Loss: 0.4221\n",
            "  [Batch 1160/6245] Loss: 0.3479\n",
            "  [Batch 1170/6245] Loss: 0.4414\n",
            "  [Batch 1180/6245] Loss: 0.6153\n",
            "  [Batch 1190/6245] Loss: 0.3345\n",
            "  [Batch 1200/6245] Loss: 0.3950\n",
            "  [Batch 1210/6245] Loss: 0.4120\n",
            "  [Batch 1220/6245] Loss: 0.3610\n",
            "  [Batch 1230/6245] Loss: 0.2565\n",
            "  [Batch 1240/6245] Loss: 0.2833\n",
            "  [Batch 1250/6245] Loss: 0.4116\n",
            "  [Batch 1260/6245] Loss: 0.3099\n",
            "  [Batch 1270/6245] Loss: 0.3474\n",
            "  [Batch 1280/6245] Loss: 0.2608\n",
            "  [Batch 1290/6245] Loss: 0.5533\n",
            "  [Batch 1300/6245] Loss: 0.3213\n",
            "  [Batch 1310/6245] Loss: 0.3024\n",
            "  [Batch 1320/6245] Loss: 0.2461\n",
            "  [Batch 1330/6245] Loss: 0.3929\n",
            "  [Batch 1340/6245] Loss: 0.3663\n",
            "  [Batch 1350/6245] Loss: 0.4041\n",
            "  [Batch 1360/6245] Loss: 0.2625\n",
            "  [Batch 1370/6245] Loss: 0.3060\n",
            "  [Batch 1380/6245] Loss: 0.3445\n",
            "  [Batch 1390/6245] Loss: 0.2968\n",
            "  [Batch 1400/6245] Loss: 0.3700\n",
            "  [Batch 1410/6245] Loss: 0.2565\n",
            "  [Batch 1420/6245] Loss: 0.3556\n",
            "  [Batch 1430/6245] Loss: 0.3099\n",
            "  [Batch 1440/6245] Loss: 0.2595\n",
            "  [Batch 1450/6245] Loss: 0.2887\n",
            "  [Batch 1460/6245] Loss: 0.2293\n",
            "  [Batch 1470/6245] Loss: 0.4627\n",
            "  [Batch 1480/6245] Loss: 0.4430\n",
            "  [Batch 1490/6245] Loss: 0.2424\n",
            "  [Batch 1500/6245] Loss: 0.3472\n",
            "  [Batch 1510/6245] Loss: 0.3046\n",
            "  [Batch 1520/6245] Loss: 0.4125\n",
            "  [Batch 1530/6245] Loss: 0.4347\n",
            "  [Batch 1540/6245] Loss: 0.2227\n",
            "  [Batch 1550/6245] Loss: 0.2819\n",
            "  [Batch 1560/6245] Loss: 0.2749\n",
            "  [Batch 1570/6245] Loss: 0.5032\n",
            "  [Batch 1580/6245] Loss: 0.4034\n",
            "  [Batch 1590/6245] Loss: 0.3852\n",
            "  [Batch 1600/6245] Loss: 0.2481\n",
            "  [Batch 1610/6245] Loss: 0.3415\n",
            "  [Batch 1620/6245] Loss: 0.4452\n",
            "  [Batch 1630/6245] Loss: 0.5132\n",
            "  [Batch 1640/6245] Loss: 0.4702\n",
            "  [Batch 1650/6245] Loss: 0.3482\n",
            "  [Batch 1660/6245] Loss: 0.3737\n",
            "  [Batch 1670/6245] Loss: 0.4109\n",
            "  [Batch 1680/6245] Loss: 0.3627\n",
            "  [Batch 1690/6245] Loss: 0.3856\n",
            "  [Batch 1700/6245] Loss: 0.3447\n",
            "  [Batch 1710/6245] Loss: 0.2827\n",
            "  [Batch 1720/6245] Loss: 0.3829\n",
            "  [Batch 1730/6245] Loss: 0.2460\n",
            "  [Batch 1740/6245] Loss: 0.3325\n",
            "  [Batch 1750/6245] Loss: 0.3397\n",
            "  [Batch 1760/6245] Loss: 0.2660\n",
            "  [Batch 1770/6245] Loss: 0.3107\n",
            "  [Batch 1780/6245] Loss: 0.2931\n",
            "  [Batch 1790/6245] Loss: 0.3419\n",
            "  [Batch 1800/6245] Loss: 0.2828\n",
            "  [Batch 1810/6245] Loss: 0.2646\n",
            "  [Batch 1820/6245] Loss: 0.1447\n",
            "  [Batch 1830/6245] Loss: 0.1079\n",
            "  [Batch 1840/6245] Loss: 0.3032\n",
            "  [Batch 1850/6245] Loss: 0.2707\n",
            "  [Batch 1860/6245] Loss: 0.3520\n",
            "  [Batch 1870/6245] Loss: 0.3086\n",
            "  [Batch 1880/6245] Loss: 0.3212\n",
            "  [Batch 1890/6245] Loss: 0.2689\n",
            "  [Batch 1900/6245] Loss: 0.2558\n",
            "  [Batch 1910/6245] Loss: 0.6236\n",
            "  [Batch 1920/6245] Loss: 0.3039\n",
            "  [Batch 1930/6245] Loss: 0.2800\n",
            "  [Batch 1940/6245] Loss: 0.3407\n",
            "  [Batch 1950/6245] Loss: 0.5853\n",
            "  [Batch 1960/6245] Loss: 0.4962\n",
            "  [Batch 1970/6245] Loss: 0.3322\n",
            "  [Batch 1980/6245] Loss: 0.4568\n",
            "  [Batch 1990/6245] Loss: 0.3811\n",
            "  [Batch 2000/6245] Loss: 0.3986\n",
            "  [Batch 2010/6245] Loss: 0.3728\n",
            "  [Batch 2020/6245] Loss: 0.3588\n",
            "  [Batch 2030/6245] Loss: 0.3098\n",
            "  [Batch 2040/6245] Loss: 0.3189\n",
            "  [Batch 2050/6245] Loss: 0.3880\n",
            "  [Batch 2060/6245] Loss: 0.4780\n",
            "  [Batch 2070/6245] Loss: 0.4849\n",
            "  [Batch 2080/6245] Loss: 0.4044\n",
            "  [Batch 2090/6245] Loss: 0.2862\n",
            "  [Batch 2100/6245] Loss: 0.1800\n",
            "  [Batch 2110/6245] Loss: 0.3256\n",
            "  [Batch 2120/6245] Loss: 0.3229\n",
            "  [Batch 2130/6245] Loss: 0.5816\n",
            "  [Batch 2140/6245] Loss: 0.5403\n",
            "  [Batch 2150/6245] Loss: 0.4786\n",
            "  [Batch 2160/6245] Loss: 0.2263\n",
            "  [Batch 2170/6245] Loss: 0.3957\n",
            "  [Batch 2180/6245] Loss: 0.3166\n",
            "  [Batch 2190/6245] Loss: 0.3043\n",
            "  [Batch 2200/6245] Loss: 0.5350\n",
            "  [Batch 2210/6245] Loss: 0.2678\n",
            "  [Batch 2220/6245] Loss: 0.2433\n",
            "  [Batch 2230/6245] Loss: 0.3641\n",
            "  [Batch 2240/6245] Loss: 0.3505\n",
            "  [Batch 2250/6245] Loss: 0.3225\n",
            "  [Batch 2260/6245] Loss: 0.2741\n",
            "  [Batch 2270/6245] Loss: 0.2601\n",
            "  [Batch 2280/6245] Loss: 0.4442\n",
            "  [Batch 2290/6245] Loss: 0.4034\n",
            "  [Batch 2300/6245] Loss: 0.4306\n",
            "  [Batch 2310/6245] Loss: 0.3106\n",
            "  [Batch 2320/6245] Loss: 0.1939\n",
            "  [Batch 2330/6245] Loss: 0.3488\n",
            "  [Batch 2340/6245] Loss: 0.2531\n",
            "  [Batch 2350/6245] Loss: 0.2426\n",
            "  [Batch 2360/6245] Loss: 0.1936\n",
            "  [Batch 2370/6245] Loss: 0.3787\n",
            "  [Batch 2380/6245] Loss: 0.3507\n",
            "  [Batch 2390/6245] Loss: 0.1985\n",
            "  [Batch 2400/6245] Loss: 0.5555\n",
            "  [Batch 2410/6245] Loss: 0.5543\n",
            "  [Batch 2420/6245] Loss: 0.2103\n",
            "  [Batch 2430/6245] Loss: 0.4079\n",
            "  [Batch 2440/6245] Loss: 0.4369\n",
            "  [Batch 2450/6245] Loss: 0.4075\n",
            "  [Batch 2460/6245] Loss: 0.4557\n",
            "  [Batch 2470/6245] Loss: 0.4061\n",
            "  [Batch 2480/6245] Loss: 0.3228\n",
            "  [Batch 2490/6245] Loss: 0.4011\n",
            "  [Batch 2500/6245] Loss: 0.3615\n",
            "  [Batch 2510/6245] Loss: 0.3549\n",
            "  [Batch 2520/6245] Loss: 0.7133\n",
            "  [Batch 2530/6245] Loss: 0.5608\n",
            "  [Batch 2540/6245] Loss: 0.2676\n",
            "  [Batch 2550/6245] Loss: 0.3675\n",
            "  [Batch 2560/6245] Loss: 0.6512\n",
            "  [Batch 2570/6245] Loss: 0.3806\n",
            "  [Batch 2580/6245] Loss: 0.4360\n",
            "  [Batch 2590/6245] Loss: 0.3192\n",
            "  [Batch 2600/6245] Loss: 0.2843\n",
            "  [Batch 2610/6245] Loss: 0.3236\n",
            "  [Batch 2620/6245] Loss: 0.3197\n",
            "  [Batch 2630/6245] Loss: 0.3888\n",
            "  [Batch 2640/6245] Loss: 0.2986\n",
            "  [Batch 2650/6245] Loss: 0.2150\n",
            "  [Batch 2660/6245] Loss: 0.3360\n",
            "  [Batch 2670/6245] Loss: 0.3834\n",
            "  [Batch 2680/6245] Loss: 0.3894\n",
            "  [Batch 2690/6245] Loss: 0.3394\n",
            "  [Batch 2700/6245] Loss: 0.4558\n",
            "  [Batch 2710/6245] Loss: 0.2271\n",
            "  [Batch 2720/6245] Loss: 0.4319\n",
            "  [Batch 2730/6245] Loss: 0.3161\n",
            "  [Batch 2740/6245] Loss: 0.3543\n",
            "  [Batch 2750/6245] Loss: 0.5096\n",
            "  [Batch 2760/6245] Loss: 0.2745\n",
            "  [Batch 2770/6245] Loss: 0.3729\n",
            "  [Batch 2780/6245] Loss: 0.1964\n",
            "  [Batch 2790/6245] Loss: 0.3647\n",
            "  [Batch 2800/6245] Loss: 0.4750\n",
            "  [Batch 2810/6245] Loss: 0.2068\n",
            "  [Batch 2820/6245] Loss: 0.2521\n",
            "  [Batch 2830/6245] Loss: 0.3274\n",
            "  [Batch 2840/6245] Loss: 0.2728\n",
            "  [Batch 2850/6245] Loss: 0.3528\n",
            "  [Batch 2860/6245] Loss: 0.2905\n",
            "  [Batch 2870/6245] Loss: 0.2860\n",
            "  [Batch 2880/6245] Loss: 0.4131\n",
            "  [Batch 2890/6245] Loss: 0.3487\n",
            "  [Batch 2900/6245] Loss: 0.4238\n",
            "  [Batch 2910/6245] Loss: 0.3533\n",
            "  [Batch 2920/6245] Loss: 0.2293\n",
            "  [Batch 2930/6245] Loss: 0.4970\n",
            "  [Batch 2940/6245] Loss: 0.2553\n",
            "  [Batch 2950/6245] Loss: 0.2963\n",
            "  [Batch 2960/6245] Loss: 0.2531\n",
            "  [Batch 2970/6245] Loss: 0.5232\n",
            "  [Batch 2980/6245] Loss: 0.2365\n",
            "  [Batch 2990/6245] Loss: 0.5055\n",
            "  [Batch 3000/6245] Loss: 0.6797\n",
            "  [Batch 3010/6245] Loss: 0.3583\n",
            "  [Batch 3020/6245] Loss: 0.3828\n",
            "  [Batch 3030/6245] Loss: 0.3952\n",
            "  [Batch 3040/6245] Loss: 0.1608\n",
            "  [Batch 3050/6245] Loss: 0.2500\n",
            "  [Batch 3060/6245] Loss: 0.2832\n",
            "  [Batch 3070/6245] Loss: 0.3476\n",
            "  [Batch 3080/6245] Loss: 0.6024\n",
            "  [Batch 3090/6245] Loss: 0.5575\n",
            "  [Batch 3100/6245] Loss: 0.3802\n",
            "  [Batch 3110/6245] Loss: 0.2963\n",
            "  [Batch 3120/6245] Loss: 0.2082\n",
            "  [Batch 3130/6245] Loss: 0.2533\n",
            "  [Batch 3140/6245] Loss: 0.6070\n",
            "  [Batch 3150/6245] Loss: 0.2196\n",
            "  [Batch 3160/6245] Loss: 0.4542\n",
            "  [Batch 3170/6245] Loss: 0.2602\n",
            "  [Batch 3180/6245] Loss: 0.2804\n",
            "  [Batch 3190/6245] Loss: 0.3936\n",
            "  [Batch 3200/6245] Loss: 0.4216\n",
            "  [Batch 3210/6245] Loss: 0.6039\n",
            "  [Batch 3220/6245] Loss: 0.2823\n",
            "  [Batch 3230/6245] Loss: 0.3038\n",
            "  [Batch 3240/6245] Loss: 0.2512\n",
            "  [Batch 3250/6245] Loss: 0.3257\n",
            "  [Batch 3260/6245] Loss: 0.3550\n",
            "  [Batch 3270/6245] Loss: 0.4224\n",
            "  [Batch 3280/6245] Loss: 0.2291\n",
            "  [Batch 3290/6245] Loss: 0.2034\n",
            "  [Batch 3300/6245] Loss: 0.4517\n",
            "  [Batch 3310/6245] Loss: 0.2760\n",
            "  [Batch 3320/6245] Loss: 0.2824\n",
            "  [Batch 3330/6245] Loss: 0.3555\n",
            "  [Batch 3340/6245] Loss: 0.2693\n",
            "  [Batch 3350/6245] Loss: 0.5107\n",
            "  [Batch 3360/6245] Loss: 0.3313\n",
            "  [Batch 3370/6245] Loss: 0.3908\n",
            "  [Batch 3380/6245] Loss: 0.4167\n",
            "  [Batch 3390/6245] Loss: 0.3105\n",
            "  [Batch 3400/6245] Loss: 0.2085\n",
            "  [Batch 3410/6245] Loss: 0.5284\n",
            "  [Batch 3420/6245] Loss: 0.3458\n",
            "  [Batch 3430/6245] Loss: 0.4602\n",
            "  [Batch 3440/6245] Loss: 0.2640\n",
            "  [Batch 3450/6245] Loss: 0.2657\n",
            "  [Batch 3460/6245] Loss: 0.3913\n",
            "  [Batch 3470/6245] Loss: 0.4925\n",
            "  [Batch 3480/6245] Loss: 0.1754\n",
            "  [Batch 3490/6245] Loss: 0.3731\n",
            "  [Batch 3500/6245] Loss: 0.2571\n",
            "  [Batch 3510/6245] Loss: 0.4209\n",
            "  [Batch 3520/6245] Loss: 0.4344\n",
            "  [Batch 3530/6245] Loss: 0.4465\n",
            "  [Batch 3540/6245] Loss: 0.2234\n",
            "  [Batch 3550/6245] Loss: 0.3657\n",
            "  [Batch 3560/6245] Loss: 0.4366\n",
            "  [Batch 3570/6245] Loss: 0.3297\n",
            "  [Batch 3580/6245] Loss: 0.2633\n",
            "  [Batch 3590/6245] Loss: 0.4904\n",
            "  [Batch 3600/6245] Loss: 0.4265\n",
            "  [Batch 3610/6245] Loss: 0.4357\n",
            "  [Batch 3620/6245] Loss: 0.3854\n",
            "  [Batch 3630/6245] Loss: 0.2970\n",
            "  [Batch 3640/6245] Loss: 0.3208\n",
            "  [Batch 3650/6245] Loss: 0.3115\n",
            "  [Batch 3660/6245] Loss: 0.3017\n",
            "  [Batch 3670/6245] Loss: 0.4063\n",
            "  [Batch 3680/6245] Loss: 0.4434\n",
            "  [Batch 3690/6245] Loss: 0.3218\n",
            "  [Batch 3700/6245] Loss: 0.3834\n",
            "  [Batch 3710/6245] Loss: 0.2181\n",
            "  [Batch 3720/6245] Loss: 0.3468\n",
            "  [Batch 3730/6245] Loss: 0.3147\n",
            "  [Batch 3740/6245] Loss: 0.3596\n",
            "  [Batch 3750/6245] Loss: 0.1995\n",
            "  [Batch 3760/6245] Loss: 0.5912\n",
            "  [Batch 3770/6245] Loss: 0.4497\n",
            "  [Batch 3780/6245] Loss: 0.3375\n",
            "  [Batch 3790/6245] Loss: 0.2547\n",
            "  [Batch 3800/6245] Loss: 0.4461\n",
            "  [Batch 3810/6245] Loss: 0.2224\n",
            "  [Batch 3820/6245] Loss: 0.3620\n",
            "  [Batch 3830/6245] Loss: 0.5176\n",
            "  [Batch 3840/6245] Loss: 0.2979\n",
            "  [Batch 3850/6245] Loss: 0.2653\n",
            "  [Batch 3860/6245] Loss: 0.4697\n",
            "  [Batch 3870/6245] Loss: 0.2802\n",
            "  [Batch 3880/6245] Loss: 0.6305\n",
            "  [Batch 3890/6245] Loss: 0.2242\n",
            "  [Batch 3900/6245] Loss: 0.4133\n",
            "  [Batch 3910/6245] Loss: 0.3828\n",
            "  [Batch 3920/6245] Loss: 0.3760\n",
            "  [Batch 3930/6245] Loss: 0.3753\n",
            "  [Batch 3940/6245] Loss: 0.2483\n",
            "  [Batch 3950/6245] Loss: 0.6501\n",
            "  [Batch 3960/6245] Loss: 0.4459\n",
            "  [Batch 3970/6245] Loss: 0.3307\n",
            "  [Batch 3980/6245] Loss: 0.4050\n",
            "  [Batch 3990/6245] Loss: 0.3388\n",
            "  [Batch 4000/6245] Loss: 0.3852\n",
            "  [Batch 4010/6245] Loss: 0.3428\n",
            "  [Batch 4020/6245] Loss: 0.9035\n",
            "  [Batch 4030/6245] Loss: 0.2331\n",
            "  [Batch 4040/6245] Loss: 0.4427\n",
            "  [Batch 4050/6245] Loss: 0.3236\n",
            "  [Batch 4060/6245] Loss: 0.3893\n",
            "  [Batch 4070/6245] Loss: 0.2818\n",
            "  [Batch 4080/6245] Loss: 0.4645\n",
            "  [Batch 4090/6245] Loss: 0.2679\n",
            "  [Batch 4100/6245] Loss: 0.4401\n",
            "  [Batch 4110/6245] Loss: 0.4249\n",
            "  [Batch 4120/6245] Loss: 0.3893\n",
            "  [Batch 4130/6245] Loss: 0.3753\n",
            "  [Batch 4140/6245] Loss: 0.6529\n",
            "  [Batch 4150/6245] Loss: 0.4705\n",
            "  [Batch 4160/6245] Loss: 0.3299\n",
            "  [Batch 4170/6245] Loss: 0.3021\n",
            "  [Batch 4180/6245] Loss: 0.3906\n",
            "  [Batch 4190/6245] Loss: 0.3012\n",
            "  [Batch 4200/6245] Loss: 0.3679\n",
            "  [Batch 4210/6245] Loss: 0.3557\n",
            "  [Batch 4220/6245] Loss: 0.3857\n",
            "  [Batch 4230/6245] Loss: 0.3115\n",
            "  [Batch 4240/6245] Loss: 0.2177\n",
            "  [Batch 4250/6245] Loss: 0.4519\n",
            "  [Batch 4260/6245] Loss: 0.5227\n",
            "  [Batch 4270/6245] Loss: 0.2451\n",
            "  [Batch 4280/6245] Loss: 0.2200\n",
            "  [Batch 4290/6245] Loss: 0.4578\n",
            "  [Batch 4300/6245] Loss: 0.2884\n",
            "  [Batch 4310/6245] Loss: 0.3130\n",
            "  [Batch 4320/6245] Loss: 0.2001\n",
            "  [Batch 4330/6245] Loss: 0.4424\n",
            "  [Batch 4340/6245] Loss: 0.5420\n",
            "  [Batch 4350/6245] Loss: 0.4786\n",
            "  [Batch 4360/6245] Loss: 0.5794\n",
            "  [Batch 4370/6245] Loss: 0.3854\n",
            "  [Batch 4380/6245] Loss: 0.3975\n",
            "  [Batch 4390/6245] Loss: 0.3576\n",
            "  [Batch 4400/6245] Loss: 0.3380\n",
            "  [Batch 4410/6245] Loss: 0.2159\n",
            "  [Batch 4420/6245] Loss: 0.1550\n",
            "  [Batch 4430/6245] Loss: 0.2692\n",
            "  [Batch 4440/6245] Loss: 0.1906\n",
            "  [Batch 4450/6245] Loss: 0.2872\n",
            "  [Batch 4460/6245] Loss: 0.3565\n",
            "  [Batch 4470/6245] Loss: 0.2792\n",
            "  [Batch 4480/6245] Loss: 0.3883\n",
            "  [Batch 4490/6245] Loss: 0.3452\n",
            "  [Batch 4500/6245] Loss: 0.3287\n",
            "  [Batch 4510/6245] Loss: 0.3698\n",
            "  [Batch 4520/6245] Loss: 0.2957\n",
            "  [Batch 4530/6245] Loss: 0.3031\n",
            "  [Batch 4540/6245] Loss: 0.3759\n",
            "  [Batch 4550/6245] Loss: 0.2962\n",
            "  [Batch 4560/6245] Loss: 0.5485\n",
            "  [Batch 4570/6245] Loss: 0.2247\n",
            "  [Batch 4580/6245] Loss: 0.2326\n",
            "  [Batch 4590/6245] Loss: 0.6035\n",
            "  [Batch 4600/6245] Loss: 0.3832\n",
            "  [Batch 4610/6245] Loss: 0.3890\n",
            "  [Batch 4620/6245] Loss: 0.1316\n",
            "  [Batch 4630/6245] Loss: 0.2698\n",
            "  [Batch 4640/6245] Loss: 0.6724\n",
            "  [Batch 4650/6245] Loss: 0.3265\n",
            "  [Batch 4660/6245] Loss: 0.3620\n",
            "  [Batch 4670/6245] Loss: 0.2967\n",
            "  [Batch 4680/6245] Loss: 0.4668\n",
            "  [Batch 4690/6245] Loss: 0.4895\n",
            "  [Batch 4700/6245] Loss: 0.2868\n",
            "  [Batch 4710/6245] Loss: 0.3442\n",
            "  [Batch 4720/6245] Loss: 0.6190\n",
            "  [Batch 4730/6245] Loss: 0.4480\n",
            "  [Batch 4740/6245] Loss: 0.4086\n",
            "  [Batch 4750/6245] Loss: 0.2276\n",
            "  [Batch 4760/6245] Loss: 0.3462\n",
            "  [Batch 4770/6245] Loss: 0.3054\n",
            "  [Batch 4780/6245] Loss: 0.4195\n",
            "  [Batch 4790/6245] Loss: 0.4100\n",
            "  [Batch 4800/6245] Loss: 0.4502\n",
            "  [Batch 4810/6245] Loss: 0.3756\n",
            "  [Batch 4820/6245] Loss: 0.2276\n",
            "  [Batch 4830/6245] Loss: 0.3149\n",
            "  [Batch 4840/6245] Loss: 0.2771\n",
            "  [Batch 4850/6245] Loss: 0.3428\n",
            "  [Batch 4860/6245] Loss: 0.2520\n",
            "  [Batch 4870/6245] Loss: 0.1998\n",
            "  [Batch 4880/6245] Loss: 0.4546\n",
            "  [Batch 4890/6245] Loss: 0.1879\n",
            "  [Batch 4900/6245] Loss: 0.2844\n",
            "  [Batch 4910/6245] Loss: 0.2091\n",
            "  [Batch 4920/6245] Loss: 0.4270\n",
            "  [Batch 4930/6245] Loss: 0.2826\n",
            "  [Batch 4940/6245] Loss: 0.8564\n",
            "  [Batch 4950/6245] Loss: 0.3009\n",
            "  [Batch 4960/6245] Loss: 0.2659\n",
            "  [Batch 4970/6245] Loss: 0.3057\n",
            "  [Batch 4980/6245] Loss: 0.3984\n",
            "  [Batch 4990/6245] Loss: 0.3278\n",
            "  [Batch 5000/6245] Loss: 0.1984\n",
            "  [Batch 5010/6245] Loss: 0.2315\n",
            "  [Batch 5020/6245] Loss: 0.4651\n",
            "  [Batch 5030/6245] Loss: 0.2962\n",
            "  [Batch 5040/6245] Loss: 0.3414\n",
            "  [Batch 5050/6245] Loss: 0.4199\n",
            "  [Batch 5060/6245] Loss: 0.3344\n",
            "  [Batch 5070/6245] Loss: 0.4884\n",
            "  [Batch 5080/6245] Loss: 0.3265\n",
            "  [Batch 5090/6245] Loss: 0.4425\n",
            "  [Batch 5100/6245] Loss: 0.3919\n",
            "  [Batch 5110/6245] Loss: 0.1936\n",
            "  [Batch 5120/6245] Loss: 0.3192\n",
            "  [Batch 5130/6245] Loss: 0.4180\n",
            "  [Batch 5140/6245] Loss: 0.2532\n",
            "  [Batch 5150/6245] Loss: 0.3003\n",
            "  [Batch 5160/6245] Loss: 0.4822\n",
            "  [Batch 5170/6245] Loss: 0.3060\n",
            "  [Batch 5180/6245] Loss: 0.3545\n",
            "  [Batch 5190/6245] Loss: 0.4722\n",
            "  [Batch 5200/6245] Loss: 0.2574\n",
            "  [Batch 5210/6245] Loss: 0.1831\n",
            "  [Batch 5220/6245] Loss: 0.3073\n",
            "  [Batch 5230/6245] Loss: 0.2043\n",
            "  [Batch 5240/6245] Loss: 0.3348\n",
            "  [Batch 5250/6245] Loss: 0.2500\n",
            "  [Batch 5260/6245] Loss: 0.5597\n",
            "  [Batch 5270/6245] Loss: 0.2439\n",
            "  [Batch 5280/6245] Loss: 0.2326\n",
            "  [Batch 5290/6245] Loss: 0.2898\n",
            "  [Batch 5300/6245] Loss: 0.2470\n",
            "  [Batch 5310/6245] Loss: 0.2546\n",
            "  [Batch 5320/6245] Loss: 0.3768\n",
            "  [Batch 5330/6245] Loss: 0.2130\n",
            "  [Batch 5340/6245] Loss: 0.1694\n",
            "  [Batch 5350/6245] Loss: 0.2448\n",
            "  [Batch 5360/6245] Loss: 0.2672\n",
            "  [Batch 5370/6245] Loss: 0.3068\n",
            "  [Batch 5380/6245] Loss: 0.3054\n",
            "  [Batch 5390/6245] Loss: 0.4028\n",
            "  [Batch 5400/6245] Loss: 0.5348\n",
            "  [Batch 5410/6245] Loss: 0.3232\n",
            "  [Batch 5420/6245] Loss: 0.3523\n",
            "  [Batch 5430/6245] Loss: 0.4609\n",
            "  [Batch 5440/6245] Loss: 0.2602\n",
            "  [Batch 5450/6245] Loss: 0.2659\n",
            "  [Batch 5460/6245] Loss: 0.3426\n",
            "  [Batch 5470/6245] Loss: 0.3304\n",
            "  [Batch 5480/6245] Loss: 0.2249\n",
            "  [Batch 5490/6245] Loss: 0.4124\n",
            "  [Batch 5500/6245] Loss: 0.5897\n",
            "  [Batch 5510/6245] Loss: 0.2142\n",
            "  [Batch 5520/6245] Loss: 0.3719\n",
            "  [Batch 5530/6245] Loss: 0.3257\n",
            "  [Batch 5540/6245] Loss: 0.4355\n",
            "  [Batch 5550/6245] Loss: 0.2140\n",
            "  [Batch 5560/6245] Loss: 0.4152\n",
            "  [Batch 5570/6245] Loss: 0.2742\n",
            "  [Batch 5580/6245] Loss: 0.3337\n",
            "  [Batch 5590/6245] Loss: 0.3004\n",
            "  [Batch 5600/6245] Loss: 0.2275\n",
            "  [Batch 5610/6245] Loss: 0.3951\n",
            "  [Batch 5620/6245] Loss: 0.4219\n",
            "  [Batch 5630/6245] Loss: 0.2594\n",
            "  [Batch 5640/6245] Loss: 0.3827\n",
            "  [Batch 5650/6245] Loss: 0.4052\n",
            "  [Batch 5660/6245] Loss: 0.2640\n",
            "  [Batch 5670/6245] Loss: 0.4075\n",
            "  [Batch 5680/6245] Loss: 0.7352\n",
            "  [Batch 5690/6245] Loss: 0.3190\n",
            "  [Batch 5700/6245] Loss: 0.1955\n",
            "  [Batch 5710/6245] Loss: 0.3723\n",
            "  [Batch 5720/6245] Loss: 0.2950\n",
            "  [Batch 5730/6245] Loss: 0.3001\n",
            "  [Batch 5740/6245] Loss: 0.3103\n",
            "  [Batch 5750/6245] Loss: 0.3365\n",
            "  [Batch 5760/6245] Loss: 0.3281\n",
            "  [Batch 5770/6245] Loss: 0.2789\n",
            "  [Batch 5780/6245] Loss: 0.4197\n",
            "  [Batch 5790/6245] Loss: 0.3562\n",
            "  [Batch 5800/6245] Loss: 0.5274\n",
            "  [Batch 5810/6245] Loss: 0.6058\n",
            "  [Batch 5820/6245] Loss: 0.2609\n",
            "  [Batch 5830/6245] Loss: 0.4917\n",
            "  [Batch 5840/6245] Loss: 0.1702\n",
            "  [Batch 5850/6245] Loss: 0.4099\n",
            "  [Batch 5860/6245] Loss: 0.4092\n",
            "  [Batch 5870/6245] Loss: 0.3771\n",
            "  [Batch 5880/6245] Loss: 0.3137\n",
            "  [Batch 5890/6245] Loss: 0.3494\n",
            "  [Batch 5900/6245] Loss: 0.3250\n",
            "  [Batch 5910/6245] Loss: 0.2489\n",
            "  [Batch 5920/6245] Loss: 0.2766\n",
            "  [Batch 5930/6245] Loss: 0.3869\n",
            "  [Batch 5940/6245] Loss: 0.4736\n",
            "  [Batch 5950/6245] Loss: 0.3487\n",
            "  [Batch 5960/6245] Loss: 0.2170\n",
            "  [Batch 5970/6245] Loss: 0.2556\n",
            "  [Batch 5980/6245] Loss: 0.2864\n",
            "  [Batch 5990/6245] Loss: 0.2951\n",
            "  [Batch 6000/6245] Loss: 0.1966\n",
            "  [Batch 6010/6245] Loss: 0.1922\n",
            "  [Batch 6020/6245] Loss: 0.3056\n",
            "  [Batch 6030/6245] Loss: 0.3290\n",
            "  [Batch 6040/6245] Loss: 0.2971\n",
            "  [Batch 6050/6245] Loss: 0.3030\n",
            "  [Batch 6060/6245] Loss: 0.3615\n",
            "  [Batch 6070/6245] Loss: 0.3547\n",
            "  [Batch 6080/6245] Loss: 0.4220\n",
            "  [Batch 6090/6245] Loss: 0.3547\n",
            "  [Batch 6100/6245] Loss: 0.3207\n",
            "  [Batch 6110/6245] Loss: 0.2313\n",
            "  [Batch 6120/6245] Loss: 0.2806\n",
            "  [Batch 6130/6245] Loss: 0.1682\n",
            "  [Batch 6140/6245] Loss: 0.3347\n",
            "  [Batch 6150/6245] Loss: 0.2161\n",
            "  [Batch 6160/6245] Loss: 0.1216\n",
            "  [Batch 6170/6245] Loss: 0.2117\n",
            "  [Batch 6180/6245] Loss: 0.3452\n",
            "  [Batch 6190/6245] Loss: 0.2795\n",
            "  [Batch 6200/6245] Loss: 0.5307\n",
            "  [Batch 6210/6245] Loss: 0.5896\n",
            "  [Batch 6220/6245] Loss: 0.1845\n",
            "  [Batch 6230/6245] Loss: 0.2884\n",
            "  [Batch 6240/6245] Loss: 0.7177\n",
            "  [Batch 6245/6245] Loss: 0.2412\n",
            "[Epoch 1] Train Loss: 0.3637, Acc: 84.35% | Val Loss: 0.3557, Acc: 85.25%\n",
            "[INFO] Model checkpoint saved at ./models\\breastnet_epoch1.pth\n",
            "\n",
            "[Epoch 2/10]\n",
            "  [Batch 10/6245] Loss: 0.3038\n",
            "  [Batch 20/6245] Loss: 0.2713\n",
            "  [Batch 30/6245] Loss: 0.4605\n",
            "  [Batch 40/6245] Loss: 0.2987\n",
            "  [Batch 50/6245] Loss: 0.2488\n",
            "  [Batch 60/6245] Loss: 0.3298\n",
            "  [Batch 70/6245] Loss: 0.2196\n",
            "  [Batch 80/6245] Loss: 0.3489\n",
            "  [Batch 90/6245] Loss: 0.3178\n",
            "  [Batch 100/6245] Loss: 0.2955\n",
            "  [Batch 110/6245] Loss: 0.2013\n",
            "  [Batch 120/6245] Loss: 0.2448\n",
            "  [Batch 130/6245] Loss: 0.3197\n",
            "  [Batch 140/6245] Loss: 0.4456\n",
            "  [Batch 150/6245] Loss: 0.3467\n",
            "  [Batch 160/6245] Loss: 0.3009\n",
            "  [Batch 170/6245] Loss: 0.3573\n",
            "  [Batch 180/6245] Loss: 0.2829\n",
            "  [Batch 190/6245] Loss: 0.2425\n",
            "  [Batch 200/6245] Loss: 0.5390\n",
            "  [Batch 210/6245] Loss: 0.2911\n",
            "  [Batch 220/6245] Loss: 0.2367\n",
            "  [Batch 230/6245] Loss: 0.2597\n",
            "  [Batch 240/6245] Loss: 0.1812\n",
            "  [Batch 250/6245] Loss: 0.5246\n",
            "  [Batch 260/6245] Loss: 0.3566\n",
            "  [Batch 270/6245] Loss: 0.2604\n",
            "  [Batch 280/6245] Loss: 0.4121\n",
            "  [Batch 290/6245] Loss: 0.2757\n",
            "  [Batch 300/6245] Loss: 0.2051\n",
            "  [Batch 310/6245] Loss: 0.2365\n",
            "  [Batch 320/6245] Loss: 0.3195\n",
            "  [Batch 330/6245] Loss: 0.2935\n",
            "  [Batch 340/6245] Loss: 0.3777\n",
            "  [Batch 350/6245] Loss: 0.4770\n",
            "  [Batch 360/6245] Loss: 0.3212\n",
            "  [Batch 370/6245] Loss: 0.4415\n",
            "  [Batch 380/6245] Loss: 0.2766\n",
            "  [Batch 390/6245] Loss: 0.3165\n",
            "  [Batch 400/6245] Loss: 0.4630\n",
            "  [Batch 410/6245] Loss: 0.3917\n",
            "  [Batch 420/6245] Loss: 0.3893\n",
            "  [Batch 430/6245] Loss: 0.3012\n",
            "  [Batch 440/6245] Loss: 0.3058\n",
            "  [Batch 450/6245] Loss: 0.2243\n",
            "  [Batch 460/6245] Loss: 0.3904\n",
            "  [Batch 470/6245] Loss: 0.3154\n",
            "  [Batch 480/6245] Loss: 0.2652\n",
            "  [Batch 490/6245] Loss: 0.2407\n",
            "  [Batch 500/6245] Loss: 0.2972\n",
            "  [Batch 510/6245] Loss: 0.3953\n",
            "  [Batch 520/6245] Loss: 0.2951\n",
            "  [Batch 530/6245] Loss: 0.2606\n",
            "  [Batch 540/6245] Loss: 0.2628\n",
            "  [Batch 550/6245] Loss: 0.4299\n",
            "  [Batch 560/6245] Loss: 0.1101\n",
            "  [Batch 570/6245] Loss: 0.2464\n",
            "  [Batch 580/6245] Loss: 0.1506\n",
            "  [Batch 590/6245] Loss: 0.4190\n",
            "  [Batch 600/6245] Loss: 0.3851\n",
            "  [Batch 610/6245] Loss: 0.3862\n",
            "  [Batch 620/6245] Loss: 0.2263\n",
            "  [Batch 630/6245] Loss: 0.4089\n",
            "  [Batch 640/6245] Loss: 0.3889\n",
            "  [Batch 650/6245] Loss: 0.3282\n",
            "  [Batch 660/6245] Loss: 0.1849\n",
            "  [Batch 670/6245] Loss: 0.2763\n",
            "  [Batch 680/6245] Loss: 0.4854\n",
            "  [Batch 690/6245] Loss: 0.3845\n",
            "  [Batch 700/6245] Loss: 0.3222\n",
            "  [Batch 710/6245] Loss: 0.2806\n",
            "  [Batch 720/6245] Loss: 0.4314\n",
            "  [Batch 730/6245] Loss: 0.2729\n",
            "  [Batch 740/6245] Loss: 0.4465\n",
            "  [Batch 750/6245] Loss: 0.4314\n",
            "  [Batch 760/6245] Loss: 0.5477\n",
            "  [Batch 770/6245] Loss: 0.4473\n",
            "  [Batch 780/6245] Loss: 0.3089\n",
            "  [Batch 790/6245] Loss: 0.1927\n",
            "  [Batch 800/6245] Loss: 0.4674\n",
            "  [Batch 810/6245] Loss: 0.2125\n",
            "  [Batch 820/6245] Loss: 0.3319\n",
            "  [Batch 830/6245] Loss: 0.2878\n",
            "  [Batch 840/6245] Loss: 0.2324\n",
            "  [Batch 850/6245] Loss: 0.2089\n",
            "  [Batch 860/6245] Loss: 0.6470\n",
            "  [Batch 870/6245] Loss: 0.2860\n",
            "  [Batch 880/6245] Loss: 0.3519\n",
            "  [Batch 890/6245] Loss: 0.2929\n",
            "  [Batch 900/6245] Loss: 0.4766\n",
            "  [Batch 910/6245] Loss: 0.3708\n",
            "  [Batch 920/6245] Loss: 0.2469\n",
            "  [Batch 930/6245] Loss: 0.2657\n",
            "  [Batch 940/6245] Loss: 0.4090\n",
            "  [Batch 950/6245] Loss: 0.4596\n",
            "  [Batch 960/6245] Loss: 0.3336\n",
            "  [Batch 970/6245] Loss: 0.3574\n",
            "  [Batch 980/6245] Loss: 0.1802\n",
            "  [Batch 990/6245] Loss: 0.4997\n",
            "  [Batch 1000/6245] Loss: 0.3202\n",
            "  [Batch 1010/6245] Loss: 0.3383\n",
            "  [Batch 1020/6245] Loss: 0.3264\n",
            "  [Batch 1030/6245] Loss: 0.2721\n",
            "  [Batch 1040/6245] Loss: 0.2420\n",
            "  [Batch 1050/6245] Loss: 0.4068\n",
            "  [Batch 1060/6245] Loss: 0.2493\n",
            "  [Batch 1070/6245] Loss: 0.2762\n",
            "  [Batch 1080/6245] Loss: 0.3100\n",
            "  [Batch 1090/6245] Loss: 0.1652\n",
            "  [Batch 1100/6245] Loss: 0.3425\n",
            "  [Batch 1110/6245] Loss: 0.4186\n",
            "  [Batch 1120/6245] Loss: 0.2000\n",
            "  [Batch 1130/6245] Loss: 0.2481\n",
            "  [Batch 1140/6245] Loss: 0.3626\n",
            "  [Batch 1150/6245] Loss: 0.3724\n",
            "  [Batch 1160/6245] Loss: 0.2644\n",
            "  [Batch 1170/6245] Loss: 0.2619\n",
            "  [Batch 1180/6245] Loss: 0.2976\n",
            "  [Batch 1190/6245] Loss: 0.3136\n",
            "  [Batch 1200/6245] Loss: 0.2045\n",
            "  [Batch 1210/6245] Loss: 0.2293\n",
            "  [Batch 1220/6245] Loss: 0.5089\n",
            "  [Batch 1230/6245] Loss: 0.5759\n",
            "  [Batch 1240/6245] Loss: 0.4329\n",
            "  [Batch 1250/6245] Loss: 0.2034\n",
            "  [Batch 1260/6245] Loss: 0.3826\n",
            "  [Batch 1270/6245] Loss: 0.2162\n",
            "  [Batch 1280/6245] Loss: 0.3901\n",
            "  [Batch 1290/6245] Loss: 0.3558\n",
            "  [Batch 1300/6245] Loss: 0.2604\n",
            "  [Batch 1310/6245] Loss: 0.2849\n",
            "  [Batch 1320/6245] Loss: 0.2384\n",
            "  [Batch 1330/6245] Loss: 0.3903\n",
            "  [Batch 1340/6245] Loss: 0.2604\n",
            "  [Batch 1350/6245] Loss: 0.4271\n",
            "  [Batch 1360/6245] Loss: 0.4233\n",
            "  [Batch 1370/6245] Loss: 0.2727\n",
            "  [Batch 1380/6245] Loss: 0.3141\n",
            "  [Batch 1390/6245] Loss: 0.2092\n",
            "  [Batch 1400/6245] Loss: 0.3512\n",
            "  [Batch 1410/6245] Loss: 0.2752\n",
            "  [Batch 1420/6245] Loss: 0.4450\n",
            "  [Batch 1430/6245] Loss: 0.2788\n",
            "  [Batch 1440/6245] Loss: 0.5333\n",
            "  [Batch 1450/6245] Loss: 0.4466\n",
            "  [Batch 1460/6245] Loss: 0.4965\n",
            "  [Batch 1470/6245] Loss: 0.3308\n",
            "  [Batch 1480/6245] Loss: 0.2792\n",
            "  [Batch 1490/6245] Loss: 0.3305\n",
            "  [Batch 1500/6245] Loss: 0.2987\n",
            "  [Batch 1510/6245] Loss: 0.3052\n",
            "  [Batch 1520/6245] Loss: 0.2055\n",
            "  [Batch 1530/6245] Loss: 0.3417\n",
            "  [Batch 1540/6245] Loss: 0.4313\n",
            "  [Batch 1550/6245] Loss: 0.3239\n",
            "  [Batch 1560/6245] Loss: 0.4284\n",
            "  [Batch 1570/6245] Loss: 0.2216\n",
            "  [Batch 1580/6245] Loss: 0.6139\n",
            "  [Batch 1590/6245] Loss: 0.4878\n",
            "  [Batch 1600/6245] Loss: 0.3675\n",
            "  [Batch 1610/6245] Loss: 0.3353\n",
            "  [Batch 1620/6245] Loss: 0.3017\n",
            "  [Batch 1630/6245] Loss: 0.3438\n",
            "  [Batch 1640/6245] Loss: 0.5920\n",
            "  [Batch 1650/6245] Loss: 0.2781\n",
            "  [Batch 1660/6245] Loss: 0.2687\n",
            "  [Batch 1670/6245] Loss: 0.3745\n",
            "  [Batch 1680/6245] Loss: 0.4943\n",
            "  [Batch 1690/6245] Loss: 0.4454\n",
            "  [Batch 1700/6245] Loss: 0.4188\n",
            "  [Batch 1710/6245] Loss: 0.4188\n",
            "  [Batch 1720/6245] Loss: 0.5777\n",
            "  [Batch 1730/6245] Loss: 0.2709\n",
            "  [Batch 1740/6245] Loss: 0.3501\n",
            "  [Batch 1750/6245] Loss: 0.4688\n",
            "  [Batch 1760/6245] Loss: 0.2694\n",
            "  [Batch 1770/6245] Loss: 0.4677\n",
            "  [Batch 1780/6245] Loss: 0.4621\n",
            "  [Batch 1790/6245] Loss: 0.4097\n",
            "  [Batch 1800/6245] Loss: 0.4663\n",
            "  [Batch 1810/6245] Loss: 0.2518\n",
            "  [Batch 1820/6245] Loss: 0.3076\n",
            "  [Batch 1830/6245] Loss: 0.2688\n",
            "  [Batch 1840/6245] Loss: 0.1598\n",
            "  [Batch 1850/6245] Loss: 0.1630\n",
            "  [Batch 1860/6245] Loss: 0.1319\n",
            "  [Batch 1870/6245] Loss: 0.2153\n",
            "  [Batch 1880/6245] Loss: 0.4589\n",
            "  [Batch 1890/6245] Loss: 0.2537\n",
            "  [Batch 1900/6245] Loss: 0.4589\n",
            "  [Batch 1910/6245] Loss: 0.2776\n",
            "  [Batch 1920/6245] Loss: 0.3581\n",
            "  [Batch 1930/6245] Loss: 0.3134\n",
            "  [Batch 1940/6245] Loss: 0.3588\n",
            "  [Batch 1950/6245] Loss: 0.3588\n",
            "  [Batch 1960/6245] Loss: 0.3092\n",
            "  [Batch 1970/6245] Loss: 0.3342\n",
            "  [Batch 1980/6245] Loss: 0.2386\n",
            "  [Batch 1990/6245] Loss: 0.3673\n",
            "  [Batch 2000/6245] Loss: 0.2987\n",
            "  [Batch 2010/6245] Loss: 0.2381\n",
            "  [Batch 2020/6245] Loss: 0.1506\n",
            "  [Batch 2030/6245] Loss: 0.2804\n",
            "  [Batch 2040/6245] Loss: 0.4626\n",
            "  [Batch 2050/6245] Loss: 0.1547\n",
            "  [Batch 2060/6245] Loss: 0.3035\n",
            "  [Batch 2070/6245] Loss: 0.3717\n",
            "  [Batch 2080/6245] Loss: 0.3117\n",
            "  [Batch 2090/6245] Loss: 0.2888\n",
            "  [Batch 2100/6245] Loss: 0.2731\n",
            "  [Batch 2110/6245] Loss: 0.4786\n",
            "  [Batch 2120/6245] Loss: 0.4008\n",
            "  [Batch 2130/6245] Loss: 0.6487\n",
            "  [Batch 2140/6245] Loss: 0.3387\n",
            "  [Batch 2150/6245] Loss: 0.2075\n",
            "  [Batch 2160/6245] Loss: 0.2515\n",
            "  [Batch 2170/6245] Loss: 0.3944\n",
            "  [Batch 2180/6245] Loss: 0.4637\n",
            "  [Batch 2190/6245] Loss: 0.2641\n",
            "  [Batch 2200/6245] Loss: 0.6427\n",
            "  [Batch 2210/6245] Loss: 0.1809\n",
            "  [Batch 2220/6245] Loss: 0.2487\n",
            "  [Batch 2230/6245] Loss: 0.2357\n",
            "  [Batch 2240/6245] Loss: 0.3828\n",
            "  [Batch 2250/6245] Loss: 0.2294\n",
            "  [Batch 2260/6245] Loss: 0.3503\n",
            "  [Batch 2270/6245] Loss: 0.4112\n",
            "  [Batch 2280/6245] Loss: 0.3564\n",
            "  [Batch 2290/6245] Loss: 0.4514\n",
            "  [Batch 2300/6245] Loss: 0.4120\n",
            "  [Batch 2310/6245] Loss: 0.2142\n",
            "  [Batch 2320/6245] Loss: 0.3425\n",
            "  [Batch 2330/6245] Loss: 0.3256\n",
            "  [Batch 2340/6245] Loss: 0.3192\n",
            "  [Batch 2350/6245] Loss: 0.4092\n",
            "  [Batch 2360/6245] Loss: 0.3629\n",
            "  [Batch 2370/6245] Loss: 0.3138\n",
            "  [Batch 2380/6245] Loss: 0.3137\n",
            "  [Batch 2390/6245] Loss: 0.3824\n",
            "  [Batch 2400/6245] Loss: 0.3442\n",
            "  [Batch 2410/6245] Loss: 0.2028\n",
            "  [Batch 2420/6245] Loss: 0.2634\n",
            "  [Batch 2430/6245] Loss: 0.3127\n",
            "  [Batch 2440/6245] Loss: 0.4286\n",
            "  [Batch 2450/6245] Loss: 0.4413\n",
            "  [Batch 2460/6245] Loss: 0.4607\n",
            "  [Batch 2470/6245] Loss: 0.4679\n",
            "  [Batch 2480/6245] Loss: 0.5389\n",
            "  [Batch 2490/6245] Loss: 0.2475\n",
            "  [Batch 2500/6245] Loss: 0.4188\n",
            "  [Batch 2510/6245] Loss: 0.2279\n",
            "  [Batch 2520/6245] Loss: 0.2338\n",
            "  [Batch 2530/6245] Loss: 0.4411\n",
            "  [Batch 2540/6245] Loss: 0.1869\n",
            "  [Batch 2550/6245] Loss: 0.3784\n",
            "  [Batch 2560/6245] Loss: 0.4784\n",
            "  [Batch 2570/6245] Loss: 0.4479\n",
            "  [Batch 2580/6245] Loss: 0.2808\n",
            "  [Batch 2590/6245] Loss: 0.2990\n",
            "  [Batch 2600/6245] Loss: 0.3257\n",
            "  [Batch 2610/6245] Loss: 0.4359\n",
            "  [Batch 2620/6245] Loss: 0.3354\n",
            "  [Batch 2630/6245] Loss: 0.2151\n",
            "  [Batch 2640/6245] Loss: 0.3215\n",
            "  [Batch 2650/6245] Loss: 0.2415\n",
            "  [Batch 2660/6245] Loss: 0.5019\n",
            "  [Batch 2670/6245] Loss: 0.1678\n",
            "  [Batch 2680/6245] Loss: 0.2798\n",
            "  [Batch 2690/6245] Loss: 0.2880\n",
            "  [Batch 2700/6245] Loss: 0.2273\n",
            "  [Batch 2710/6245] Loss: 0.2804\n",
            "  [Batch 2720/6245] Loss: 0.2981\n",
            "  [Batch 2730/6245] Loss: 0.6166\n",
            "  [Batch 2740/6245] Loss: 0.4939\n",
            "  [Batch 2750/6245] Loss: 0.3943\n",
            "  [Batch 2760/6245] Loss: 0.3211\n",
            "  [Batch 2770/6245] Loss: 0.3082\n",
            "  [Batch 2780/6245] Loss: 0.2839\n",
            "  [Batch 2790/6245] Loss: 0.2869\n",
            "  [Batch 2800/6245] Loss: 0.4249\n",
            "  [Batch 2810/6245] Loss: 0.2381\n",
            "  [Batch 2820/6245] Loss: 0.3881\n",
            "  [Batch 2830/6245] Loss: 0.3026\n",
            "  [Batch 2840/6245] Loss: 0.4746\n",
            "  [Batch 2850/6245] Loss: 0.1900\n",
            "  [Batch 2860/6245] Loss: 0.2907\n",
            "  [Batch 2870/6245] Loss: 0.4789\n",
            "  [Batch 2880/6245] Loss: 0.5156\n",
            "  [Batch 2890/6245] Loss: 0.2948\n",
            "  [Batch 2900/6245] Loss: 0.2952\n",
            "  [Batch 2910/6245] Loss: 0.3122\n",
            "  [Batch 2920/6245] Loss: 0.2393\n",
            "  [Batch 2930/6245] Loss: 0.3341\n",
            "  [Batch 2940/6245] Loss: 0.3741\n",
            "  [Batch 2950/6245] Loss: 0.4948\n",
            "  [Batch 2960/6245] Loss: 0.2578\n",
            "  [Batch 2970/6245] Loss: 0.1365\n",
            "  [Batch 2980/6245] Loss: 0.5290\n",
            "  [Batch 2990/6245] Loss: 0.2730\n",
            "  [Batch 3000/6245] Loss: 0.2556\n",
            "  [Batch 3010/6245] Loss: 0.4257\n",
            "  [Batch 3020/6245] Loss: 0.2660\n",
            "  [Batch 3030/6245] Loss: 0.5053\n",
            "  [Batch 3040/6245] Loss: 0.3012\n",
            "  [Batch 3050/6245] Loss: 0.4427\n",
            "  [Batch 3060/6245] Loss: 0.3544\n",
            "  [Batch 3070/6245] Loss: 0.3698\n",
            "  [Batch 3080/6245] Loss: 0.2551\n",
            "  [Batch 3090/6245] Loss: 0.3252\n",
            "  [Batch 3100/6245] Loss: 0.4300\n",
            "  [Batch 3110/6245] Loss: 0.2232\n",
            "  [Batch 3120/6245] Loss: 0.2882\n",
            "  [Batch 3130/6245] Loss: 0.2402\n",
            "  [Batch 3140/6245] Loss: 0.3271\n",
            "  [Batch 3150/6245] Loss: 0.3479\n",
            "  [Batch 3160/6245] Loss: 0.2987\n",
            "  [Batch 3170/6245] Loss: 0.4528\n",
            "  [Batch 3180/6245] Loss: 0.3181\n",
            "  [Batch 3190/6245] Loss: 0.3675\n",
            "  [Batch 3200/6245] Loss: 0.2874\n",
            "  [Batch 3210/6245] Loss: 0.3375\n",
            "  [Batch 3220/6245] Loss: 0.4355\n",
            "  [Batch 3230/6245] Loss: 0.2423\n",
            "  [Batch 3240/6245] Loss: 0.2937\n",
            "  [Batch 3250/6245] Loss: 0.3593\n",
            "  [Batch 3260/6245] Loss: 0.4832\n",
            "  [Batch 3270/6245] Loss: 0.4164\n",
            "  [Batch 3280/6245] Loss: 0.3202\n",
            "  [Batch 3290/6245] Loss: 0.2251\n",
            "  [Batch 3300/6245] Loss: 0.4250\n",
            "  [Batch 3310/6245] Loss: 0.4464\n",
            "  [Batch 3320/6245] Loss: 0.3420\n",
            "  [Batch 3330/6245] Loss: 0.3909\n",
            "  [Batch 3340/6245] Loss: 0.5231\n",
            "  [Batch 3350/6245] Loss: 0.3610\n",
            "  [Batch 3360/6245] Loss: 0.3030\n",
            "  [Batch 3370/6245] Loss: 0.4967\n",
            "  [Batch 3380/6245] Loss: 0.2196\n",
            "  [Batch 3390/6245] Loss: 0.4853\n",
            "  [Batch 3400/6245] Loss: 0.2592\n",
            "  [Batch 3410/6245] Loss: 0.3343\n",
            "  [Batch 3420/6245] Loss: 0.3031\n",
            "  [Batch 3430/6245] Loss: 0.3086\n",
            "  [Batch 3440/6245] Loss: 0.3617\n",
            "  [Batch 3450/6245] Loss: 0.4068\n",
            "  [Batch 3460/6245] Loss: 0.3355\n",
            "  [Batch 3470/6245] Loss: 0.4049\n",
            "  [Batch 3480/6245] Loss: 0.4212\n",
            "  [Batch 3490/6245] Loss: 0.1882\n",
            "  [Batch 3500/6245] Loss: 0.3345\n",
            "  [Batch 3510/6245] Loss: 0.2861\n",
            "  [Batch 3520/6245] Loss: 0.2829\n",
            "  [Batch 3530/6245] Loss: 0.2582\n",
            "  [Batch 3540/6245] Loss: 0.3564\n",
            "  [Batch 3550/6245] Loss: 0.3215\n",
            "  [Batch 3560/6245] Loss: 0.2462\n",
            "  [Batch 3570/6245] Loss: 0.3256\n",
            "  [Batch 3580/6245] Loss: 0.1699\n",
            "  [Batch 3590/6245] Loss: 0.4577\n",
            "  [Batch 3600/6245] Loss: 0.1866\n",
            "  [Batch 3610/6245] Loss: 0.3555\n",
            "  [Batch 3620/6245] Loss: 0.2271\n",
            "  [Batch 3630/6245] Loss: 0.3732\n",
            "  [Batch 3640/6245] Loss: 0.2816\n",
            "  [Batch 3650/6245] Loss: 0.3759\n",
            "  [Batch 3660/6245] Loss: 0.3076\n",
            "  [Batch 3670/6245] Loss: 0.3941\n",
            "  [Batch 3680/6245] Loss: 0.4164\n",
            "  [Batch 3690/6245] Loss: 0.3628\n",
            "  [Batch 3700/6245] Loss: 0.4742\n",
            "  [Batch 3710/6245] Loss: 0.2318\n",
            "  [Batch 3720/6245] Loss: 0.3023\n",
            "  [Batch 3730/6245] Loss: 0.3325\n",
            "  [Batch 3740/6245] Loss: 0.4110\n",
            "  [Batch 3750/6245] Loss: 0.2480\n",
            "  [Batch 3760/6245] Loss: 0.3551\n",
            "  [Batch 3770/6245] Loss: 0.4607\n",
            "  [Batch 3780/6245] Loss: 0.3282\n",
            "  [Batch 3790/6245] Loss: 0.2581\n",
            "  [Batch 3800/6245] Loss: 0.3767\n",
            "  [Batch 3810/6245] Loss: 0.2040\n",
            "  [Batch 3820/6245] Loss: 0.3448\n",
            "  [Batch 3830/6245] Loss: 0.2232\n",
            "  [Batch 3840/6245] Loss: 0.2134\n",
            "  [Batch 3850/6245] Loss: 0.4593\n",
            "  [Batch 3860/6245] Loss: 0.3660\n",
            "  [Batch 3870/6245] Loss: 0.2359\n",
            "  [Batch 3880/6245] Loss: 0.4967\n",
            "  [Batch 3890/6245] Loss: 0.4903\n",
            "  [Batch 3900/6245] Loss: 0.3640\n",
            "  [Batch 3910/6245] Loss: 0.2142\n",
            "  [Batch 3920/6245] Loss: 0.2133\n",
            "  [Batch 3930/6245] Loss: 0.2516\n",
            "  [Batch 3940/6245] Loss: 0.2943\n",
            "  [Batch 3950/6245] Loss: 0.4177\n",
            "  [Batch 3960/6245] Loss: 0.3020\n",
            "  [Batch 3970/6245] Loss: 0.2927\n",
            "  [Batch 3980/6245] Loss: 0.1444\n",
            "  [Batch 3990/6245] Loss: 0.8296\n",
            "  [Batch 4000/6245] Loss: 0.5504\n",
            "  [Batch 4010/6245] Loss: 0.3852\n",
            "  [Batch 4020/6245] Loss: 0.3678\n",
            "  [Batch 4030/6245] Loss: 0.2754\n",
            "  [Batch 4040/6245] Loss: 0.3531\n",
            "  [Batch 4050/6245] Loss: 0.4599\n",
            "  [Batch 4060/6245] Loss: 0.4294\n",
            "  [Batch 4070/6245] Loss: 0.3112\n",
            "  [Batch 4080/6245] Loss: 0.1629\n",
            "  [Batch 4090/6245] Loss: 0.3254\n",
            "  [Batch 4100/6245] Loss: 0.2108\n",
            "  [Batch 4110/6245] Loss: 0.3213\n",
            "  [Batch 4120/6245] Loss: 0.3450\n",
            "  [Batch 4130/6245] Loss: 0.6064\n",
            "  [Batch 4140/6245] Loss: 0.4306\n",
            "  [Batch 4150/6245] Loss: 0.5664\n",
            "  [Batch 4160/6245] Loss: 0.2922\n",
            "  [Batch 4170/6245] Loss: 0.3090\n",
            "  [Batch 4180/6245] Loss: 0.2935\n",
            "  [Batch 4190/6245] Loss: 0.5428\n",
            "  [Batch 4200/6245] Loss: 0.3494\n",
            "  [Batch 4210/6245] Loss: 0.3538\n",
            "  [Batch 4220/6245] Loss: 0.2264\n",
            "  [Batch 4230/6245] Loss: 0.2375\n",
            "  [Batch 4240/6245] Loss: 0.2596\n",
            "  [Batch 4250/6245] Loss: 0.4045\n",
            "  [Batch 4260/6245] Loss: 0.4079\n",
            "  [Batch 4270/6245] Loss: 0.4468\n",
            "  [Batch 4280/6245] Loss: 0.5771\n",
            "  [Batch 4290/6245] Loss: 0.3727\n",
            "  [Batch 4300/6245] Loss: 0.4503\n",
            "  [Batch 4310/6245] Loss: 0.3217\n",
            "  [Batch 4320/6245] Loss: 0.6322\n",
            "  [Batch 4330/6245] Loss: 0.4103\n",
            "  [Batch 4340/6245] Loss: 0.2506\n",
            "  [Batch 4350/6245] Loss: 0.3304\n",
            "  [Batch 4360/6245] Loss: 0.4436\n",
            "  [Batch 4370/6245] Loss: 0.3236\n",
            "  [Batch 4380/6245] Loss: 0.3976\n",
            "  [Batch 4390/6245] Loss: 0.3156\n",
            "  [Batch 4400/6245] Loss: 0.3028\n",
            "  [Batch 4410/6245] Loss: 0.2568\n",
            "  [Batch 4420/6245] Loss: 0.1966\n",
            "  [Batch 4430/6245] Loss: 0.2351\n",
            "  [Batch 4440/6245] Loss: 0.2817\n",
            "  [Batch 4450/6245] Loss: 0.4285\n",
            "  [Batch 4460/6245] Loss: 0.1792\n",
            "  [Batch 4470/6245] Loss: 0.4127\n",
            "  [Batch 4480/6245] Loss: 0.1639\n",
            "  [Batch 4490/6245] Loss: 0.5253\n",
            "  [Batch 4500/6245] Loss: 0.3832\n",
            "  [Batch 4510/6245] Loss: 0.3737\n",
            "  [Batch 4520/6245] Loss: 0.1539\n",
            "  [Batch 4530/6245] Loss: 0.3823\n",
            "  [Batch 4540/6245] Loss: 0.2316\n",
            "  [Batch 4550/6245] Loss: 0.2797\n",
            "  [Batch 4560/6245] Loss: 0.1299\n",
            "  [Batch 4570/6245] Loss: 0.5665\n",
            "  [Batch 4580/6245] Loss: 0.3120\n",
            "  [Batch 4590/6245] Loss: 0.2825\n",
            "  [Batch 4600/6245] Loss: 0.2601\n",
            "  [Batch 4610/6245] Loss: 0.4776\n",
            "  [Batch 4620/6245] Loss: 0.2950\n",
            "  [Batch 4630/6245] Loss: 0.2645\n",
            "  [Batch 4640/6245] Loss: 0.3794\n",
            "  [Batch 4650/6245] Loss: 0.2528\n",
            "  [Batch 4660/6245] Loss: 0.4013\n",
            "  [Batch 4670/6245] Loss: 0.4147\n",
            "  [Batch 4680/6245] Loss: 0.4391\n",
            "  [Batch 4690/6245] Loss: 0.5260\n",
            "  [Batch 4700/6245] Loss: 0.3594\n",
            "  [Batch 4710/6245] Loss: 0.5114\n",
            "  [Batch 4720/6245] Loss: 0.3272\n",
            "  [Batch 4730/6245] Loss: 0.3400\n",
            "  [Batch 4740/6245] Loss: 0.4658\n",
            "  [Batch 4750/6245] Loss: 0.3984\n",
            "  [Batch 4760/6245] Loss: 0.4538\n",
            "  [Batch 4770/6245] Loss: 0.2264\n",
            "  [Batch 4780/6245] Loss: 0.4032\n",
            "  [Batch 4790/6245] Loss: 0.3002\n",
            "  [Batch 4800/6245] Loss: 0.3194\n",
            "  [Batch 4810/6245] Loss: 0.3542\n",
            "  [Batch 4820/6245] Loss: 0.2398\n",
            "  [Batch 4830/6245] Loss: 0.3193\n",
            "  [Batch 4840/6245] Loss: 0.2904\n",
            "  [Batch 4850/6245] Loss: 0.1945\n",
            "  [Batch 4860/6245] Loss: 0.3202\n",
            "  [Batch 4870/6245] Loss: 0.4571\n",
            "  [Batch 4880/6245] Loss: 0.4820\n",
            "  [Batch 4890/6245] Loss: 0.2926\n",
            "  [Batch 4900/6245] Loss: 0.4775\n",
            "  [Batch 4910/6245] Loss: 0.3497\n",
            "  [Batch 4920/6245] Loss: 0.1887\n",
            "  [Batch 4930/6245] Loss: 0.3417\n",
            "  [Batch 4940/6245] Loss: 0.2638\n",
            "  [Batch 4950/6245] Loss: 0.2468\n",
            "  [Batch 4960/6245] Loss: 0.1864\n",
            "  [Batch 4970/6245] Loss: 0.4030\n",
            "  [Batch 4980/6245] Loss: 0.4612\n",
            "  [Batch 4990/6245] Loss: 0.3973\n",
            "  [Batch 5000/6245] Loss: 0.2445\n",
            "  [Batch 5010/6245] Loss: 0.5398\n",
            "  [Batch 5020/6245] Loss: 0.2474\n",
            "  [Batch 5030/6245] Loss: 0.3142\n",
            "  [Batch 5040/6245] Loss: 0.3542\n",
            "  [Batch 5050/6245] Loss: 0.2460\n",
            "  [Batch 5060/6245] Loss: 0.4019\n",
            "  [Batch 5070/6245] Loss: 0.2988\n",
            "  [Batch 5080/6245] Loss: 0.2978\n",
            "  [Batch 5090/6245] Loss: 0.2592\n",
            "  [Batch 5100/6245] Loss: 0.3808\n",
            "  [Batch 5110/6245] Loss: 0.5658\n",
            "  [Batch 5120/6245] Loss: 0.1849\n",
            "  [Batch 5130/6245] Loss: 0.2489\n",
            "  [Batch 5140/6245] Loss: 0.1912\n",
            "  [Batch 5150/6245] Loss: 0.5274\n",
            "  [Batch 5160/6245] Loss: 0.2034\n",
            "  [Batch 5170/6245] Loss: 0.1834\n",
            "  [Batch 5180/6245] Loss: 0.4324\n",
            "  [Batch 5190/6245] Loss: 0.4218\n",
            "  [Batch 5200/6245] Loss: 0.3863\n",
            "  [Batch 5210/6245] Loss: 0.1853\n",
            "  [Batch 5220/6245] Loss: 0.1885\n",
            "  [Batch 5230/6245] Loss: 0.3822\n",
            "  [Batch 5240/6245] Loss: 0.3775\n",
            "  [Batch 5250/6245] Loss: 0.2755\n",
            "  [Batch 5260/6245] Loss: 0.2709\n",
            "  [Batch 5270/6245] Loss: 0.2268\n",
            "  [Batch 5280/6245] Loss: 0.2383\n",
            "  [Batch 5290/6245] Loss: 0.2455\n",
            "  [Batch 5300/6245] Loss: 0.2082\n",
            "  [Batch 5310/6245] Loss: 0.3581\n",
            "  [Batch 5320/6245] Loss: 0.4558\n",
            "  [Batch 5330/6245] Loss: 0.2966\n",
            "  [Batch 5340/6245] Loss: 0.4382\n",
            "  [Batch 5350/6245] Loss: 0.1463\n",
            "  [Batch 5360/6245] Loss: 0.4495\n",
            "  [Batch 5370/6245] Loss: 0.1919\n",
            "  [Batch 5380/6245] Loss: 0.3038\n",
            "  [Batch 5390/6245] Loss: 0.2595\n",
            "  [Batch 5400/6245] Loss: 0.4345\n",
            "  [Batch 5410/6245] Loss: 0.1960\n",
            "  [Batch 5420/6245] Loss: 0.3414\n",
            "  [Batch 5430/6245] Loss: 0.3668\n",
            "  [Batch 5440/6245] Loss: 0.2914\n",
            "  [Batch 5450/6245] Loss: 0.2522\n",
            "  [Batch 5460/6245] Loss: 0.5178\n",
            "  [Batch 5470/6245] Loss: 0.3220\n",
            "  [Batch 5480/6245] Loss: 0.2363\n",
            "  [Batch 5490/6245] Loss: 0.3806\n",
            "  [Batch 5500/6245] Loss: 0.2373\n",
            "  [Batch 5510/6245] Loss: 0.2503\n",
            "  [Batch 5520/6245] Loss: 0.2544\n",
            "  [Batch 5530/6245] Loss: 0.2723\n",
            "  [Batch 5540/6245] Loss: 0.2649\n",
            "  [Batch 5550/6245] Loss: 0.3253\n",
            "  [Batch 5560/6245] Loss: 0.4515\n",
            "  [Batch 5570/6245] Loss: 0.4551\n",
            "  [Batch 5580/6245] Loss: 0.2888\n",
            "  [Batch 5590/6245] Loss: 0.2287\n",
            "  [Batch 5600/6245] Loss: 0.2374\n",
            "  [Batch 5610/6245] Loss: 0.2887\n",
            "  [Batch 5620/6245] Loss: 0.3680\n",
            "  [Batch 5630/6245] Loss: 0.4987\n",
            "  [Batch 5640/6245] Loss: 0.5181\n",
            "  [Batch 5650/6245] Loss: 0.3519\n",
            "  [Batch 5660/6245] Loss: 0.5715\n",
            "  [Batch 5670/6245] Loss: 0.3952\n",
            "  [Batch 5680/6245] Loss: 0.4677\n",
            "  [Batch 5690/6245] Loss: 0.2608\n",
            "  [Batch 5700/6245] Loss: 0.4524\n",
            "  [Batch 5710/6245] Loss: 0.3653\n",
            "  [Batch 5720/6245] Loss: 0.2740\n",
            "  [Batch 5730/6245] Loss: 0.4708\n",
            "  [Batch 5740/6245] Loss: 0.2627\n",
            "  [Batch 5750/6245] Loss: 0.2540\n",
            "  [Batch 5760/6245] Loss: 0.2813\n",
            "  [Batch 5770/6245] Loss: 0.5152\n",
            "  [Batch 5780/6245] Loss: 0.3382\n",
            "  [Batch 5790/6245] Loss: 0.2691\n",
            "  [Batch 5800/6245] Loss: 0.4303\n",
            "  [Batch 5810/6245] Loss: 0.2421\n",
            "  [Batch 5820/6245] Loss: 0.5791\n",
            "  [Batch 5830/6245] Loss: 0.3621\n",
            "  [Batch 5840/6245] Loss: 0.3284\n",
            "  [Batch 5850/6245] Loss: 0.2646\n",
            "  [Batch 5860/6245] Loss: 0.2128\n",
            "  [Batch 5870/6245] Loss: 0.4737\n",
            "  [Batch 5880/6245] Loss: 0.3679\n",
            "  [Batch 5890/6245] Loss: 0.3023\n",
            "  [Batch 5900/6245] Loss: 0.2766\n",
            "  [Batch 5910/6245] Loss: 0.3442\n",
            "  [Batch 5920/6245] Loss: 0.3008\n",
            "  [Batch 5930/6245] Loss: 0.3338\n",
            "  [Batch 5940/6245] Loss: 0.3304\n",
            "  [Batch 5950/6245] Loss: 0.2470\n",
            "  [Batch 5960/6245] Loss: 0.3421\n",
            "  [Batch 5970/6245] Loss: 0.5600\n",
            "  [Batch 5980/6245] Loss: 0.2237\n",
            "  [Batch 5990/6245] Loss: 0.4370\n",
            "  [Batch 6000/6245] Loss: 0.3060\n",
            "  [Batch 6010/6245] Loss: 0.4430\n",
            "  [Batch 6020/6245] Loss: 0.2796\n",
            "  [Batch 6030/6245] Loss: 0.3318\n",
            "  [Batch 6040/6245] Loss: 0.3857\n",
            "  [Batch 6050/6245] Loss: 0.3298\n",
            "  [Batch 6060/6245] Loss: 0.2659\n",
            "  [Batch 6070/6245] Loss: 0.2390\n",
            "  [Batch 6080/6245] Loss: 0.2897\n",
            "  [Batch 6090/6245] Loss: 0.3300\n",
            "  [Batch 6100/6245] Loss: 0.3503\n",
            "  [Batch 6110/6245] Loss: 0.4898\n",
            "  [Batch 6120/6245] Loss: 0.3068\n",
            "  [Batch 6130/6245] Loss: 0.4138\n",
            "  [Batch 6140/6245] Loss: 0.2812\n",
            "  [Batch 6150/6245] Loss: 0.2802\n",
            "  [Batch 6160/6245] Loss: 0.2520\n",
            "  [Batch 6170/6245] Loss: 0.2801\n",
            "  [Batch 6180/6245] Loss: 0.1706\n",
            "  [Batch 6190/6245] Loss: 0.3514\n",
            "  [Batch 6200/6245] Loss: 0.3059\n",
            "  [Batch 6210/6245] Loss: 0.3261\n",
            "  [Batch 6220/6245] Loss: 0.2230\n",
            "  [Batch 6230/6245] Loss: 0.3793\n",
            "  [Batch 6240/6245] Loss: 0.2570\n",
            "  [Batch 6245/6245] Loss: 0.1860\n",
            "[Epoch 2] Train Loss: 0.3407, Acc: 85.29% | Val Loss: 0.3284, Acc: 85.99%\n",
            "[INFO] Model checkpoint saved at ./models\\breastnet_epoch2.pth\n",
            "\n",
            "[Epoch 3/10]\n",
            "  [Batch 10/6245] Loss: 0.3089\n",
            "  [Batch 20/6245] Loss: 0.3469\n",
            "  [Batch 30/6245] Loss: 0.1675\n",
            "  [Batch 40/6245] Loss: 0.3306\n",
            "  [Batch 50/6245] Loss: 0.2930\n",
            "  [Batch 60/6245] Loss: 0.2149\n",
            "  [Batch 70/6245] Loss: 0.2582\n",
            "  [Batch 80/6245] Loss: 0.3444\n",
            "  [Batch 90/6245] Loss: 0.2744\n",
            "  [Batch 100/6245] Loss: 0.3207\n",
            "  [Batch 110/6245] Loss: 0.4306\n",
            "  [Batch 120/6245] Loss: 0.3285\n",
            "  [Batch 130/6245] Loss: 0.3921\n",
            "  [Batch 140/6245] Loss: 0.3788\n",
            "  [Batch 150/6245] Loss: 0.3509\n",
            "  [Batch 160/6245] Loss: 0.3353\n",
            "  [Batch 170/6245] Loss: 0.2106\n",
            "  [Batch 180/6245] Loss: 0.1580\n",
            "  [Batch 190/6245] Loss: 0.3696\n",
            "  [Batch 200/6245] Loss: 0.3493\n",
            "  [Batch 210/6245] Loss: 0.2303\n",
            "  [Batch 220/6245] Loss: 0.3350\n",
            "  [Batch 230/6245] Loss: 0.2526\n",
            "  [Batch 240/6245] Loss: 0.4306\n",
            "  [Batch 250/6245] Loss: 0.3477\n",
            "  [Batch 260/6245] Loss: 0.2805\n",
            "  [Batch 270/6245] Loss: 0.3261\n",
            "  [Batch 280/6245] Loss: 0.2890\n",
            "  [Batch 290/6245] Loss: 0.3447\n",
            "  [Batch 300/6245] Loss: 0.4133\n",
            "  [Batch 310/6245] Loss: 0.5071\n",
            "  [Batch 320/6245] Loss: 0.2923\n",
            "  [Batch 330/6245] Loss: 0.1974\n",
            "  [Batch 340/6245] Loss: 0.4957\n",
            "  [Batch 350/6245] Loss: 0.3116\n",
            "  [Batch 360/6245] Loss: 0.3557\n",
            "  [Batch 370/6245] Loss: 0.4430\n",
            "  [Batch 380/6245] Loss: 0.3328\n",
            "  [Batch 390/6245] Loss: 0.3174\n",
            "  [Batch 400/6245] Loss: 0.4141\n",
            "  [Batch 410/6245] Loss: 0.3968\n",
            "  [Batch 420/6245] Loss: 0.3814\n",
            "  [Batch 430/6245] Loss: 0.4194\n",
            "  [Batch 440/6245] Loss: 0.6056\n",
            "  [Batch 450/6245] Loss: 0.5539\n",
            "  [Batch 460/6245] Loss: 0.4395\n",
            "  [Batch 470/6245] Loss: 0.2192\n",
            "  [Batch 480/6245] Loss: 0.3905\n",
            "  [Batch 490/6245] Loss: 0.4465\n",
            "  [Batch 500/6245] Loss: 0.1838\n",
            "  [Batch 510/6245] Loss: 0.3038\n",
            "  [Batch 520/6245] Loss: 0.3785\n",
            "  [Batch 530/6245] Loss: 0.2815\n",
            "  [Batch 540/6245] Loss: 0.4144\n",
            "  [Batch 550/6245] Loss: 0.2064\n",
            "  [Batch 560/6245] Loss: 0.2249\n",
            "  [Batch 570/6245] Loss: 0.3291\n",
            "  [Batch 580/6245] Loss: 0.1315\n",
            "  [Batch 590/6245] Loss: 0.3601\n",
            "  [Batch 600/6245] Loss: 0.3373\n",
            "  [Batch 610/6245] Loss: 0.4466\n",
            "  [Batch 620/6245] Loss: 0.2584\n",
            "  [Batch 630/6245] Loss: 0.2616\n",
            "  [Batch 640/6245] Loss: 0.3411\n",
            "  [Batch 650/6245] Loss: 0.3516\n",
            "  [Batch 660/6245] Loss: 0.3911\n",
            "  [Batch 670/6245] Loss: 0.3676\n",
            "  [Batch 680/6245] Loss: 0.1387\n",
            "  [Batch 690/6245] Loss: 0.4182\n",
            "  [Batch 700/6245] Loss: 0.4288\n",
            "  [Batch 710/6245] Loss: 0.4033\n",
            "  [Batch 720/6245] Loss: 0.3500\n",
            "  [Batch 730/6245] Loss: 0.3289\n",
            "  [Batch 740/6245] Loss: 0.1713\n",
            "  [Batch 750/6245] Loss: 0.3516\n",
            "  [Batch 760/6245] Loss: 0.4807\n",
            "  [Batch 770/6245] Loss: 0.6148\n",
            "  [Batch 780/6245] Loss: 0.3231\n",
            "  [Batch 790/6245] Loss: 0.2844\n",
            "  [Batch 800/6245] Loss: 0.2695\n",
            "  [Batch 810/6245] Loss: 0.2344\n",
            "  [Batch 820/6245] Loss: 0.2255\n",
            "  [Batch 830/6245] Loss: 0.3389\n",
            "  [Batch 840/6245] Loss: 0.1581\n",
            "  [Batch 850/6245] Loss: 0.3330\n",
            "  [Batch 860/6245] Loss: 0.3319\n",
            "  [Batch 870/6245] Loss: 0.3411\n",
            "  [Batch 880/6245] Loss: 0.3953\n",
            "  [Batch 890/6245] Loss: 0.3821\n",
            "  [Batch 900/6245] Loss: 0.2838\n",
            "  [Batch 910/6245] Loss: 0.3091\n",
            "  [Batch 920/6245] Loss: 0.3421\n",
            "  [Batch 930/6245] Loss: 0.3761\n",
            "  [Batch 940/6245] Loss: 0.3196\n",
            "  [Batch 950/6245] Loss: 0.4274\n",
            "  [Batch 960/6245] Loss: 0.2899\n",
            "  [Batch 970/6245] Loss: 0.4623\n",
            "  [Batch 980/6245] Loss: 0.3600\n",
            "  [Batch 990/6245] Loss: 0.2947\n",
            "  [Batch 1000/6245] Loss: 0.3142\n",
            "  [Batch 1010/6245] Loss: 0.4323\n",
            "  [Batch 1020/6245] Loss: 0.3843\n",
            "  [Batch 1030/6245] Loss: 0.3585\n",
            "  [Batch 1040/6245] Loss: 0.2772\n",
            "  [Batch 1050/6245] Loss: 0.3459\n",
            "  [Batch 1060/6245] Loss: 0.4737\n",
            "  [Batch 1070/6245] Loss: 0.2308\n",
            "  [Batch 1080/6245] Loss: 0.4239\n",
            "  [Batch 1090/6245] Loss: 0.3680\n",
            "  [Batch 1100/6245] Loss: 0.3274\n",
            "  [Batch 1110/6245] Loss: 0.2845\n",
            "  [Batch 1120/6245] Loss: 0.4546\n",
            "  [Batch 1130/6245] Loss: 0.3327\n",
            "  [Batch 1140/6245] Loss: 0.3715\n",
            "  [Batch 1150/6245] Loss: 0.4728\n",
            "  [Batch 1160/6245] Loss: 0.3033\n",
            "  [Batch 1170/6245] Loss: 0.6462\n",
            "  [Batch 1180/6245] Loss: 0.1910\n",
            "  [Batch 1190/6245] Loss: 0.1869\n",
            "  [Batch 1200/6245] Loss: 0.4204\n",
            "  [Batch 1210/6245] Loss: 0.3434\n",
            "  [Batch 1220/6245] Loss: 0.3624\n",
            "  [Batch 1230/6245] Loss: 0.3814\n",
            "  [Batch 1240/6245] Loss: 0.3450\n",
            "  [Batch 1250/6245] Loss: 0.2564\n",
            "  [Batch 1260/6245] Loss: 0.4011\n",
            "  [Batch 1270/6245] Loss: 0.3286\n",
            "  [Batch 1280/6245] Loss: 0.3651\n",
            "  [Batch 1290/6245] Loss: 0.4982\n",
            "  [Batch 1300/6245] Loss: 0.3884\n",
            "  [Batch 1310/6245] Loss: 0.2674\n",
            "  [Batch 1320/6245] Loss: 0.3778\n",
            "  [Batch 1330/6245] Loss: 0.1930\n",
            "  [Batch 1340/6245] Loss: 0.1931\n",
            "  [Batch 1350/6245] Loss: 0.3703\n",
            "  [Batch 1360/6245] Loss: 0.4439\n",
            "  [Batch 1370/6245] Loss: 0.4531\n",
            "  [Batch 1380/6245] Loss: 0.2532\n",
            "  [Batch 1390/6245] Loss: 0.3106\n",
            "  [Batch 1400/6245] Loss: 0.2996\n",
            "  [Batch 1410/6245] Loss: 0.2250\n",
            "  [Batch 1420/6245] Loss: 0.3145\n",
            "  [Batch 1430/6245] Loss: 0.4982\n",
            "  [Batch 1440/6245] Loss: 0.3332\n",
            "  [Batch 1450/6245] Loss: 0.3383\n",
            "  [Batch 1460/6245] Loss: 0.2589\n",
            "  [Batch 1470/6245] Loss: 0.4233\n",
            "  [Batch 1480/6245] Loss: 0.2249\n",
            "  [Batch 1490/6245] Loss: 0.3225\n",
            "  [Batch 1500/6245] Loss: 0.2531\n",
            "  [Batch 1510/6245] Loss: 0.3914\n",
            "  [Batch 1520/6245] Loss: 0.3008\n",
            "  [Batch 1530/6245] Loss: 0.4243\n",
            "  [Batch 1540/6245] Loss: 0.2610\n",
            "  [Batch 1550/6245] Loss: 0.2963\n",
            "  [Batch 1560/6245] Loss: 0.4216\n",
            "  [Batch 1570/6245] Loss: 0.3959\n",
            "  [Batch 1580/6245] Loss: 0.2817\n",
            "  [Batch 1590/6245] Loss: 0.2578\n",
            "  [Batch 1600/6245] Loss: 0.3557\n",
            "  [Batch 1610/6245] Loss: 0.2673\n",
            "  [Batch 1620/6245] Loss: 0.2006\n",
            "  [Batch 1630/6245] Loss: 0.2948\n",
            "  [Batch 1640/6245] Loss: 0.3325\n",
            "  [Batch 1650/6245] Loss: 0.4406\n",
            "  [Batch 1660/6245] Loss: 0.4492\n",
            "  [Batch 1670/6245] Loss: 0.3594\n",
            "  [Batch 1680/6245] Loss: 0.3189\n",
            "  [Batch 1690/6245] Loss: 0.4578\n",
            "  [Batch 1700/6245] Loss: 0.3463\n",
            "  [Batch 1710/6245] Loss: 0.3478\n",
            "  [Batch 1720/6245] Loss: 0.4776\n",
            "  [Batch 1730/6245] Loss: 0.4455\n",
            "  [Batch 1740/6245] Loss: 0.2876\n",
            "  [Batch 1750/6245] Loss: 0.3423\n",
            "  [Batch 1760/6245] Loss: 0.3387\n",
            "  [Batch 1770/6245] Loss: 0.4119\n",
            "  [Batch 1780/6245] Loss: 0.4826\n",
            "  [Batch 1790/6245] Loss: 0.2113\n",
            "  [Batch 1800/6245] Loss: 0.3137\n",
            "  [Batch 1810/6245] Loss: 0.2926\n",
            "  [Batch 1820/6245] Loss: 0.2733\n",
            "  [Batch 1830/6245] Loss: 0.3589\n",
            "  [Batch 1840/6245] Loss: 0.3828\n",
            "  [Batch 1850/6245] Loss: 0.4001\n",
            "  [Batch 1860/6245] Loss: 0.2924\n",
            "  [Batch 1870/6245] Loss: 0.2852\n",
            "  [Batch 1880/6245] Loss: 0.6947\n",
            "  [Batch 1890/6245] Loss: 0.4855\n",
            "  [Batch 1900/6245] Loss: 0.5046\n",
            "  [Batch 1910/6245] Loss: 0.2209\n",
            "  [Batch 1920/6245] Loss: 0.3734\n",
            "  [Batch 1930/6245] Loss: 0.4353\n",
            "  [Batch 1940/6245] Loss: 0.3645\n",
            "  [Batch 1950/6245] Loss: 0.3456\n",
            "  [Batch 1960/6245] Loss: 0.3531\n",
            "  [Batch 1970/6245] Loss: 0.3935\n",
            "  [Batch 1980/6245] Loss: 0.4466\n",
            "  [Batch 1990/6245] Loss: 0.4610\n",
            "  [Batch 2000/6245] Loss: 0.3788\n",
            "  [Batch 2010/6245] Loss: 0.4136\n",
            "  [Batch 2020/6245] Loss: 0.2512\n",
            "  [Batch 2030/6245] Loss: 0.2304\n",
            "  [Batch 2040/6245] Loss: 0.2399\n",
            "  [Batch 2050/6245] Loss: 0.3571\n",
            "  [Batch 2060/6245] Loss: 0.2538\n",
            "  [Batch 2070/6245] Loss: 0.3972\n",
            "  [Batch 2080/6245] Loss: 0.3553\n",
            "  [Batch 2090/6245] Loss: 0.4689\n",
            "  [Batch 2100/6245] Loss: 0.2699\n",
            "  [Batch 2110/6245] Loss: 0.2612\n",
            "  [Batch 2120/6245] Loss: 0.3256\n",
            "  [Batch 2130/6245] Loss: 0.1734\n",
            "  [Batch 2140/6245] Loss: 0.2847\n",
            "  [Batch 2150/6245] Loss: 0.2119\n",
            "  [Batch 2160/6245] Loss: 0.3057\n",
            "  [Batch 2170/6245] Loss: 0.3291\n",
            "  [Batch 2180/6245] Loss: 0.2784\n",
            "  [Batch 2190/6245] Loss: 0.2642\n",
            "  [Batch 2200/6245] Loss: 0.3325\n",
            "  [Batch 2210/6245] Loss: 0.2404\n",
            "  [Batch 2220/6245] Loss: 0.3510\n",
            "  [Batch 2230/6245] Loss: 0.3350\n",
            "  [Batch 2240/6245] Loss: 0.5782\n",
            "  [Batch 2250/6245] Loss: 0.2538\n",
            "  [Batch 2260/6245] Loss: 0.3853\n",
            "  [Batch 2270/6245] Loss: 0.4021\n",
            "  [Batch 2280/6245] Loss: 0.3847\n",
            "  [Batch 2290/6245] Loss: 0.2477\n",
            "  [Batch 2300/6245] Loss: 0.3021\n",
            "  [Batch 2310/6245] Loss: 0.2767\n",
            "  [Batch 2320/6245] Loss: 0.3492\n",
            "  [Batch 2330/6245] Loss: 0.2638\n",
            "  [Batch 2340/6245] Loss: 0.2805\n",
            "  [Batch 2350/6245] Loss: 0.6023\n",
            "  [Batch 2360/6245] Loss: 0.3002\n",
            "  [Batch 2370/6245] Loss: 0.2902\n",
            "  [Batch 2380/6245] Loss: 0.3770\n",
            "  [Batch 2390/6245] Loss: 0.3124\n",
            "  [Batch 2400/6245] Loss: 0.9450\n",
            "  [Batch 2410/6245] Loss: 0.2971\n",
            "  [Batch 2420/6245] Loss: 0.3142\n",
            "  [Batch 2430/6245] Loss: 0.4043\n",
            "  [Batch 2440/6245] Loss: 0.2339\n",
            "  [Batch 2450/6245] Loss: 0.1960\n",
            "  [Batch 2460/6245] Loss: 0.2918\n",
            "  [Batch 2470/6245] Loss: 0.2402\n",
            "  [Batch 2480/6245] Loss: 0.2630\n",
            "  [Batch 2490/6245] Loss: 0.3404\n",
            "  [Batch 2500/6245] Loss: 0.3035\n",
            "  [Batch 2510/6245] Loss: 0.4639\n",
            "  [Batch 2520/6245] Loss: 0.4011\n",
            "  [Batch 2530/6245] Loss: 0.2571\n",
            "  [Batch 2540/6245] Loss: 0.2101\n",
            "  [Batch 2550/6245] Loss: 0.3250\n",
            "  [Batch 2560/6245] Loss: 0.2215\n",
            "  [Batch 2570/6245] Loss: 0.2720\n",
            "  [Batch 2580/6245] Loss: 0.2557\n",
            "  [Batch 2590/6245] Loss: 0.2542\n",
            "  [Batch 2600/6245] Loss: 0.3208\n",
            "  [Batch 2610/6245] Loss: 0.2255\n",
            "  [Batch 2620/6245] Loss: 0.3579\n",
            "  [Batch 2630/6245] Loss: 0.4599\n",
            "  [Batch 2640/6245] Loss: 0.2973\n",
            "  [Batch 2650/6245] Loss: 0.5043\n",
            "  [Batch 2660/6245] Loss: 0.2089\n",
            "  [Batch 2670/6245] Loss: 0.3092\n",
            "  [Batch 2680/6245] Loss: 0.1707\n",
            "  [Batch 2690/6245] Loss: 0.3412\n",
            "  [Batch 2700/6245] Loss: 0.3083\n",
            "  [Batch 2710/6245] Loss: 0.4521\n",
            "  [Batch 2720/6245] Loss: 0.2424\n",
            "  [Batch 2730/6245] Loss: 0.2327\n",
            "  [Batch 2740/6245] Loss: 0.1706\n",
            "  [Batch 2750/6245] Loss: 0.2212\n",
            "  [Batch 2760/6245] Loss: 0.4110\n",
            "  [Batch 2770/6245] Loss: 0.3418\n",
            "  [Batch 2780/6245] Loss: 0.4321\n",
            "  [Batch 2790/6245] Loss: 0.3379\n",
            "  [Batch 2800/6245] Loss: 0.4751\n",
            "  [Batch 2810/6245] Loss: 0.3576\n",
            "  [Batch 2820/6245] Loss: 0.3418\n",
            "  [Batch 2830/6245] Loss: 0.2164\n",
            "  [Batch 2840/6245] Loss: 0.2921\n",
            "  [Batch 2850/6245] Loss: 0.2312\n",
            "  [Batch 2860/6245] Loss: 0.3274\n",
            "  [Batch 2870/6245] Loss: 0.3121\n",
            "  [Batch 2880/6245] Loss: 0.3825\n",
            "  [Batch 2890/6245] Loss: 0.3450\n",
            "  [Batch 2900/6245] Loss: 0.2302\n",
            "  [Batch 2910/6245] Loss: 0.3044\n",
            "  [Batch 2920/6245] Loss: 0.2635\n",
            "  [Batch 2930/6245] Loss: 0.3088\n",
            "  [Batch 2940/6245] Loss: 0.4695\n",
            "  [Batch 2950/6245] Loss: 0.3711\n",
            "  [Batch 2960/6245] Loss: 0.3553\n",
            "  [Batch 2970/6245] Loss: 0.2719\n",
            "  [Batch 2980/6245] Loss: 0.5624\n",
            "  [Batch 2990/6245] Loss: 0.2127\n",
            "  [Batch 3000/6245] Loss: 0.4785\n",
            "  [Batch 3010/6245] Loss: 0.4470\n",
            "  [Batch 3020/6245] Loss: 0.3047\n",
            "  [Batch 3030/6245] Loss: 0.3434\n",
            "  [Batch 3040/6245] Loss: 0.4147\n",
            "  [Batch 3050/6245] Loss: 0.3031\n",
            "  [Batch 3060/6245] Loss: 0.3277\n",
            "  [Batch 3070/6245] Loss: 0.2186\n",
            "  [Batch 3080/6245] Loss: 0.4905\n",
            "  [Batch 3090/6245] Loss: 0.1413\n",
            "  [Batch 3100/6245] Loss: 0.3366\n",
            "  [Batch 3110/6245] Loss: 0.6651\n",
            "  [Batch 3120/6245] Loss: 0.3608\n",
            "  [Batch 3130/6245] Loss: 0.2621\n",
            "  [Batch 3140/6245] Loss: 0.3728\n",
            "  [Batch 3150/6245] Loss: 0.3581\n",
            "  [Batch 3160/6245] Loss: 0.3029\n",
            "  [Batch 3170/6245] Loss: 0.2301\n",
            "  [Batch 3180/6245] Loss: 0.2854\n",
            "  [Batch 3190/6245] Loss: 0.2039\n",
            "  [Batch 3200/6245] Loss: 0.2999\n",
            "  [Batch 3210/6245] Loss: 0.2725\n",
            "  [Batch 3220/6245] Loss: 0.4769\n",
            "  [Batch 3230/6245] Loss: 0.3790\n",
            "  [Batch 3240/6245] Loss: 0.4165\n",
            "  [Batch 3250/6245] Loss: 0.3916\n",
            "  [Batch 3260/6245] Loss: 0.5055\n",
            "  [Batch 3270/6245] Loss: 0.4190\n",
            "  [Batch 3280/6245] Loss: 0.3812\n",
            "  [Batch 3290/6245] Loss: 0.2792\n",
            "  [Batch 3300/6245] Loss: 0.4649\n",
            "  [Batch 3310/6245] Loss: 0.2973\n",
            "  [Batch 3320/6245] Loss: 0.3630\n",
            "  [Batch 3330/6245] Loss: 0.1731\n",
            "  [Batch 3340/6245] Loss: 0.3530\n",
            "  [Batch 3350/6245] Loss: 0.2440\n",
            "  [Batch 3360/6245] Loss: 0.3778\n",
            "  [Batch 3370/6245] Loss: 0.2966\n",
            "  [Batch 3380/6245] Loss: 0.3369\n",
            "  [Batch 3390/6245] Loss: 0.3654\n",
            "  [Batch 3400/6245] Loss: 0.3079\n",
            "  [Batch 3410/6245] Loss: 0.4928\n",
            "  [Batch 3420/6245] Loss: 0.2490\n",
            "  [Batch 3430/6245] Loss: 0.6142\n",
            "  [Batch 3440/6245] Loss: 0.2019\n",
            "  [Batch 3450/6245] Loss: 0.4811\n",
            "  [Batch 3460/6245] Loss: 0.4148\n",
            "  [Batch 3470/6245] Loss: 0.4601\n",
            "  [Batch 3480/6245] Loss: 0.3018\n",
            "  [Batch 3490/6245] Loss: 0.5274\n",
            "  [Batch 3500/6245] Loss: 0.2747\n",
            "  [Batch 3510/6245] Loss: 0.2524\n",
            "  [Batch 3520/6245] Loss: 0.1906\n",
            "  [Batch 3530/6245] Loss: 0.4481\n",
            "  [Batch 3540/6245] Loss: 0.2734\n",
            "  [Batch 3550/6245] Loss: 0.4102\n",
            "  [Batch 3560/6245] Loss: 0.2679\n",
            "  [Batch 3570/6245] Loss: 0.1982\n",
            "  [Batch 3580/6245] Loss: 0.4632\n",
            "  [Batch 3590/6245] Loss: 0.2423\n",
            "  [Batch 3600/6245] Loss: 0.2396\n",
            "  [Batch 3610/6245] Loss: 0.3440\n",
            "  [Batch 3620/6245] Loss: 0.3456\n",
            "  [Batch 3630/6245] Loss: 0.3223\n",
            "  [Batch 3640/6245] Loss: 0.2371\n",
            "  [Batch 3650/6245] Loss: 0.2723\n",
            "  [Batch 3660/6245] Loss: 0.2226\n",
            "  [Batch 3670/6245] Loss: 0.1854\n",
            "  [Batch 3680/6245] Loss: 0.3216\n",
            "  [Batch 3690/6245] Loss: 0.4685\n",
            "  [Batch 3700/6245] Loss: 0.2791\n",
            "  [Batch 3710/6245] Loss: 0.3484\n",
            "  [Batch 3720/6245] Loss: 0.1745\n",
            "  [Batch 3730/6245] Loss: 0.4107\n",
            "  [Batch 3740/6245] Loss: 0.1987\n",
            "  [Batch 3750/6245] Loss: 0.5171\n",
            "  [Batch 3760/6245] Loss: 0.2513\n",
            "  [Batch 3770/6245] Loss: 0.2829\n",
            "  [Batch 3780/6245] Loss: 0.2980\n",
            "  [Batch 3790/6245] Loss: 0.2227\n",
            "  [Batch 3800/6245] Loss: 0.5620\n",
            "  [Batch 3810/6245] Loss: 0.1760\n",
            "  [Batch 3820/6245] Loss: 0.3726\n",
            "  [Batch 3830/6245] Loss: 0.4067\n",
            "  [Batch 3840/6245] Loss: 0.2708\n",
            "  [Batch 3850/6245] Loss: 0.1773\n",
            "  [Batch 3860/6245] Loss: 0.3289\n",
            "  [Batch 3870/6245] Loss: 0.3635\n",
            "  [Batch 3880/6245] Loss: 0.2252\n",
            "  [Batch 3890/6245] Loss: 0.2754\n",
            "  [Batch 3900/6245] Loss: 0.3236\n",
            "  [Batch 3910/6245] Loss: 0.1503\n",
            "  [Batch 3920/6245] Loss: 0.2965\n",
            "  [Batch 3930/6245] Loss: 0.4845\n",
            "  [Batch 3940/6245] Loss: 0.2997\n",
            "  [Batch 3950/6245] Loss: 0.2564\n",
            "  [Batch 3960/6245] Loss: 0.2170\n",
            "  [Batch 3970/6245] Loss: 0.4115\n",
            "  [Batch 3980/6245] Loss: 0.3035\n",
            "  [Batch 3990/6245] Loss: 0.2685\n",
            "  [Batch 4000/6245] Loss: 0.6411\n",
            "  [Batch 4010/6245] Loss: 0.3372\n",
            "  [Batch 4020/6245] Loss: 0.2761\n",
            "  [Batch 4030/6245] Loss: 0.2722\n",
            "  [Batch 4040/6245] Loss: 0.2042\n",
            "  [Batch 4050/6245] Loss: 0.4426\n",
            "  [Batch 4060/6245] Loss: 0.3869\n",
            "  [Batch 4070/6245] Loss: 0.2114\n",
            "  [Batch 4080/6245] Loss: 0.2786\n",
            "  [Batch 4090/6245] Loss: 0.2229\n",
            "  [Batch 4100/6245] Loss: 0.3508\n",
            "  [Batch 4110/6245] Loss: 0.1831\n",
            "  [Batch 4120/6245] Loss: 0.2475\n",
            "  [Batch 4130/6245] Loss: 0.4706\n",
            "  [Batch 4140/6245] Loss: 0.2942\n",
            "  [Batch 4150/6245] Loss: 0.5489\n",
            "  [Batch 4160/6245] Loss: 0.2879\n",
            "  [Batch 4170/6245] Loss: 0.2316\n",
            "  [Batch 4180/6245] Loss: 0.2186\n",
            "  [Batch 4190/6245] Loss: 0.3724\n",
            "  [Batch 4200/6245] Loss: 0.1533\n",
            "  [Batch 4210/6245] Loss: 0.2471\n",
            "  [Batch 4220/6245] Loss: 0.3087\n",
            "  [Batch 4230/6245] Loss: 0.3869\n",
            "  [Batch 4240/6245] Loss: 0.3220\n",
            "  [Batch 4250/6245] Loss: 0.2796\n",
            "  [Batch 4260/6245] Loss: 0.3011\n",
            "  [Batch 4270/6245] Loss: 0.2023\n",
            "  [Batch 4280/6245] Loss: 0.3809\n",
            "  [Batch 4290/6245] Loss: 0.2814\n",
            "  [Batch 4300/6245] Loss: 0.3411\n",
            "  [Batch 4310/6245] Loss: 0.2964\n",
            "  [Batch 4320/6245] Loss: 0.3365\n",
            "  [Batch 4330/6245] Loss: 0.4043\n",
            "  [Batch 4340/6245] Loss: 0.2840\n",
            "  [Batch 4350/6245] Loss: 0.6283\n",
            "  [Batch 4360/6245] Loss: 0.2348\n",
            "  [Batch 4370/6245] Loss: 0.4232\n",
            "  [Batch 4380/6245] Loss: 0.1220\n",
            "  [Batch 4390/6245] Loss: 0.2596\n",
            "  [Batch 4400/6245] Loss: 0.2445\n",
            "  [Batch 4410/6245] Loss: 0.4634\n",
            "  [Batch 4420/6245] Loss: 0.1336\n",
            "  [Batch 4430/6245] Loss: 0.7362\n",
            "  [Batch 4440/6245] Loss: 0.1823\n",
            "  [Batch 4450/6245] Loss: 0.3595\n",
            "  [Batch 4460/6245] Loss: 0.3826\n",
            "  [Batch 4470/6245] Loss: 0.3464\n",
            "  [Batch 4480/6245] Loss: 0.3400\n",
            "  [Batch 4490/6245] Loss: 0.3343\n",
            "  [Batch 4500/6245] Loss: 0.4308\n",
            "  [Batch 4510/6245] Loss: 0.2903\n",
            "  [Batch 4520/6245] Loss: 0.2855\n",
            "  [Batch 4530/6245] Loss: 0.1202\n",
            "  [Batch 4540/6245] Loss: 0.4253\n",
            "  [Batch 4550/6245] Loss: 0.4728\n",
            "  [Batch 4560/6245] Loss: 0.3666\n",
            "  [Batch 4570/6245] Loss: 0.6291\n",
            "  [Batch 4580/6245] Loss: 0.1732\n",
            "  [Batch 4590/6245] Loss: 0.2809\n",
            "  [Batch 4600/6245] Loss: 0.3845\n",
            "  [Batch 4610/6245] Loss: 0.1533\n",
            "  [Batch 4620/6245] Loss: 0.2544\n",
            "  [Batch 4630/6245] Loss: 0.2903\n",
            "  [Batch 4640/6245] Loss: 0.1945\n",
            "  [Batch 4650/6245] Loss: 0.2294\n",
            "  [Batch 4660/6245] Loss: 0.1668\n",
            "  [Batch 4670/6245] Loss: 0.3327\n",
            "  [Batch 4680/6245] Loss: 0.4279\n",
            "  [Batch 4690/6245] Loss: 0.2616\n",
            "  [Batch 4700/6245] Loss: 0.2712\n",
            "  [Batch 4710/6245] Loss: 0.5273\n",
            "  [Batch 4720/6245] Loss: 0.3338\n",
            "  [Batch 4730/6245] Loss: 0.3772\n",
            "  [Batch 4740/6245] Loss: 0.2778\n",
            "  [Batch 4750/6245] Loss: 0.1536\n",
            "  [Batch 4760/6245] Loss: 0.4633\n",
            "  [Batch 4770/6245] Loss: 0.2267\n",
            "  [Batch 4780/6245] Loss: 0.3521\n",
            "  [Batch 4790/6245] Loss: 0.3295\n",
            "  [Batch 4800/6245] Loss: 0.4550\n",
            "  [Batch 4810/6245] Loss: 0.1920\n",
            "  [Batch 4820/6245] Loss: 0.2635\n",
            "  [Batch 4830/6245] Loss: 0.3193\n",
            "  [Batch 4840/6245] Loss: 0.3130\n",
            "  [Batch 4850/6245] Loss: 0.3158\n",
            "  [Batch 4860/6245] Loss: 0.2589\n",
            "  [Batch 4870/6245] Loss: 0.2383\n",
            "  [Batch 4880/6245] Loss: 0.3033\n",
            "  [Batch 4890/6245] Loss: 0.2522\n",
            "  [Batch 4900/6245] Loss: 0.3052\n",
            "  [Batch 4910/6245] Loss: 0.3273\n",
            "  [Batch 4920/6245] Loss: 0.3115\n",
            "  [Batch 4930/6245] Loss: 0.3518\n",
            "  [Batch 4940/6245] Loss: 0.3974\n",
            "  [Batch 4950/6245] Loss: 0.3015\n",
            "  [Batch 4960/6245] Loss: 0.3230\n",
            "  [Batch 4970/6245] Loss: 0.1657\n",
            "  [Batch 4980/6245] Loss: 0.2911\n",
            "  [Batch 4990/6245] Loss: 0.3617\n",
            "  [Batch 5000/6245] Loss: 0.1431\n",
            "  [Batch 5010/6245] Loss: 0.3340\n",
            "  [Batch 5020/6245] Loss: 0.2683\n",
            "  [Batch 5030/6245] Loss: 0.4257\n",
            "  [Batch 5040/6245] Loss: 0.1786\n",
            "  [Batch 5050/6245] Loss: 0.2392\n",
            "  [Batch 5060/6245] Loss: 0.1375\n",
            "  [Batch 5070/6245] Loss: 0.4636\n",
            "  [Batch 5080/6245] Loss: 0.2274\n",
            "  [Batch 5090/6245] Loss: 0.2798\n",
            "  [Batch 5100/6245] Loss: 0.3260\n",
            "  [Batch 5110/6245] Loss: 0.3851\n",
            "  [Batch 5120/6245] Loss: 0.1770\n",
            "  [Batch 5130/6245] Loss: 0.4648\n",
            "  [Batch 5140/6245] Loss: 0.3510\n",
            "  [Batch 5150/6245] Loss: 0.3350\n",
            "  [Batch 5160/6245] Loss: 0.2289\n",
            "  [Batch 5170/6245] Loss: 0.2676\n",
            "  [Batch 5180/6245] Loss: 0.2158\n",
            "  [Batch 5190/6245] Loss: 0.4724\n",
            "  [Batch 5200/6245] Loss: 0.2667\n",
            "  [Batch 5210/6245] Loss: 0.3196\n",
            "  [Batch 5220/6245] Loss: 0.2136\n",
            "  [Batch 5230/6245] Loss: 0.2443\n",
            "  [Batch 5240/6245] Loss: 0.3349\n",
            "  [Batch 5250/6245] Loss: 0.2228\n",
            "  [Batch 5260/6245] Loss: 0.2814\n",
            "  [Batch 5270/6245] Loss: 0.4697\n",
            "  [Batch 5280/6245] Loss: 0.3392\n",
            "  [Batch 5290/6245] Loss: 0.2743\n",
            "  [Batch 5300/6245] Loss: 0.2543\n",
            "  [Batch 5310/6245] Loss: 0.3537\n",
            "  [Batch 5320/6245] Loss: 0.3304\n",
            "  [Batch 5330/6245] Loss: 0.3884\n",
            "  [Batch 5340/6245] Loss: 0.4027\n",
            "  [Batch 5350/6245] Loss: 0.3670\n",
            "  [Batch 5360/6245] Loss: 0.3083\n",
            "  [Batch 5370/6245] Loss: 0.3419\n",
            "  [Batch 5380/6245] Loss: 0.1686\n",
            "  [Batch 5390/6245] Loss: 0.3509\n",
            "  [Batch 5400/6245] Loss: 0.3074\n",
            "  [Batch 5410/6245] Loss: 0.3862\n",
            "  [Batch 5420/6245] Loss: 0.3457\n",
            "  [Batch 5430/6245] Loss: 0.4873\n",
            "  [Batch 5440/6245] Loss: 0.3761\n",
            "  [Batch 5450/6245] Loss: 0.3201\n",
            "  [Batch 5460/6245] Loss: 0.2856\n",
            "  [Batch 5470/6245] Loss: 0.2401\n",
            "  [Batch 5480/6245] Loss: 0.4943\n",
            "  [Batch 5490/6245] Loss: 0.2934\n",
            "  [Batch 5500/6245] Loss: 0.3004\n",
            "  [Batch 5510/6245] Loss: 0.2304\n",
            "  [Batch 5520/6245] Loss: 0.1587\n",
            "  [Batch 5530/6245] Loss: 0.3462\n",
            "  [Batch 5540/6245] Loss: 0.5710\n",
            "  [Batch 5550/6245] Loss: 0.1907\n",
            "  [Batch 5560/6245] Loss: 0.4317\n",
            "  [Batch 5570/6245] Loss: 0.1774\n",
            "  [Batch 5580/6245] Loss: 0.3927\n",
            "  [Batch 5590/6245] Loss: 0.3256\n",
            "  [Batch 5600/6245] Loss: 0.4578\n",
            "  [Batch 5610/6245] Loss: 0.3559\n",
            "  [Batch 5620/6245] Loss: 0.3998\n",
            "  [Batch 5630/6245] Loss: 0.3523\n",
            "  [Batch 5640/6245] Loss: 0.2682\n",
            "  [Batch 5650/6245] Loss: 0.3542\n",
            "  [Batch 5660/6245] Loss: 0.5853\n",
            "  [Batch 5670/6245] Loss: 0.5393\n",
            "  [Batch 5680/6245] Loss: 0.4600\n",
            "  [Batch 5690/6245] Loss: 0.2347\n",
            "  [Batch 5700/6245] Loss: 0.3609\n",
            "  [Batch 5710/6245] Loss: 0.3555\n",
            "  [Batch 5720/6245] Loss: 0.4725\n",
            "  [Batch 5730/6245] Loss: 0.2588\n",
            "  [Batch 5740/6245] Loss: 0.5534\n",
            "  [Batch 5750/6245] Loss: 0.2994\n",
            "  [Batch 5760/6245] Loss: 0.6132\n",
            "  [Batch 5770/6245] Loss: 0.2139\n",
            "  [Batch 5780/6245] Loss: 0.2394\n",
            "  [Batch 5790/6245] Loss: 0.3685\n",
            "  [Batch 5800/6245] Loss: 0.1557\n",
            "  [Batch 5810/6245] Loss: 0.2178\n",
            "  [Batch 5820/6245] Loss: 0.3134\n",
            "  [Batch 5830/6245] Loss: 0.4649\n",
            "  [Batch 5840/6245] Loss: 0.4361\n",
            "  [Batch 5850/6245] Loss: 0.3406\n",
            "  [Batch 5860/6245] Loss: 0.4180\n",
            "  [Batch 5870/6245] Loss: 0.1714\n",
            "  [Batch 5880/6245] Loss: 0.3410\n",
            "  [Batch 5890/6245] Loss: 0.3794\n",
            "  [Batch 5900/6245] Loss: 0.6486\n",
            "  [Batch 5910/6245] Loss: 0.4000\n",
            "  [Batch 5920/6245] Loss: 0.4235\n",
            "  [Batch 5930/6245] Loss: 0.5551\n",
            "  [Batch 5940/6245] Loss: 0.3232\n",
            "  [Batch 5950/6245] Loss: 0.5163\n",
            "  [Batch 5960/6245] Loss: 0.4653\n",
            "  [Batch 5970/6245] Loss: 0.3791\n",
            "  [Batch 5980/6245] Loss: 0.2067\n",
            "  [Batch 5990/6245] Loss: 0.3984\n",
            "  [Batch 6000/6245] Loss: 0.3777\n",
            "  [Batch 6010/6245] Loss: 0.3650\n",
            "  [Batch 6020/6245] Loss: 0.1938\n",
            "  [Batch 6030/6245] Loss: 0.1819\n",
            "  [Batch 6040/6245] Loss: 0.4837\n",
            "  [Batch 6050/6245] Loss: 0.3332\n",
            "  [Batch 6060/6245] Loss: 0.3760\n",
            "  [Batch 6070/6245] Loss: 0.3474\n",
            "  [Batch 6080/6245] Loss: 0.5337\n",
            "  [Batch 6090/6245] Loss: 0.3037\n",
            "  [Batch 6100/6245] Loss: 0.3552\n",
            "  [Batch 6110/6245] Loss: 0.2901\n",
            "  [Batch 6120/6245] Loss: 0.3862\n",
            "  [Batch 6130/6245] Loss: 0.4500\n",
            "  [Batch 6140/6245] Loss: 0.1742\n",
            "  [Batch 6150/6245] Loss: 0.4117\n",
            "  [Batch 6160/6245] Loss: 0.2496\n",
            "  [Batch 6170/6245] Loss: 0.2746\n",
            "  [Batch 6180/6245] Loss: 0.3244\n",
            "  [Batch 6190/6245] Loss: 0.4908\n",
            "  [Batch 6200/6245] Loss: 0.4083\n",
            "  [Batch 6210/6245] Loss: 0.3547\n",
            "  [Batch 6220/6245] Loss: 0.2854\n",
            "  [Batch 6230/6245] Loss: 0.2884\n",
            "  [Batch 6240/6245] Loss: 0.2994\n",
            "  [Batch 6245/6245] Loss: 0.0952\n",
            "[Epoch 3] Train Loss: 0.3322, Acc: 85.75% | Val Loss: 0.3192, Acc: 86.38%\n",
            "[INFO] Model checkpoint saved at ./models\\breastnet_epoch3.pth\n",
            "\n",
            "[Epoch 4/10]\n",
            "  [Batch 10/6245] Loss: 0.2225\n",
            "  [Batch 20/6245] Loss: 0.3021\n",
            "  [Batch 30/6245] Loss: 0.2839\n",
            "  [Batch 40/6245] Loss: 0.4464\n",
            "  [Batch 50/6245] Loss: 0.5129\n",
            "  [Batch 60/6245] Loss: 0.3907\n",
            "  [Batch 70/6245] Loss: 0.3270\n",
            "  [Batch 80/6245] Loss: 0.4022\n",
            "  [Batch 90/6245] Loss: 0.3574\n",
            "  [Batch 100/6245] Loss: 0.3827\n",
            "  [Batch 110/6245] Loss: 0.2235\n",
            "  [Batch 120/6245] Loss: 0.4983\n",
            "  [Batch 130/6245] Loss: 0.4057\n",
            "  [Batch 140/6245] Loss: 0.3705\n",
            "  [Batch 150/6245] Loss: 0.3300\n",
            "  [Batch 160/6245] Loss: 0.2187\n",
            "  [Batch 170/6245] Loss: 0.2228\n",
            "  [Batch 180/6245] Loss: 0.5299\n",
            "  [Batch 190/6245] Loss: 0.3063\n",
            "  [Batch 200/6245] Loss: 0.1717\n",
            "  [Batch 210/6245] Loss: 0.2537\n",
            "  [Batch 220/6245] Loss: 0.4814\n",
            "  [Batch 230/6245] Loss: 0.2903\n",
            "  [Batch 240/6245] Loss: 0.3939\n",
            "  [Batch 250/6245] Loss: 0.3502\n",
            "  [Batch 260/6245] Loss: 0.3978\n",
            "  [Batch 270/6245] Loss: 0.5799\n",
            "  [Batch 280/6245] Loss: 0.2873\n",
            "  [Batch 290/6245] Loss: 0.3346\n",
            "  [Batch 300/6245] Loss: 0.7262\n",
            "  [Batch 310/6245] Loss: 0.4080\n",
            "  [Batch 320/6245] Loss: 0.5322\n",
            "  [Batch 330/6245] Loss: 0.3338\n",
            "  [Batch 340/6245] Loss: 0.5316\n",
            "  [Batch 350/6245] Loss: 0.2518\n",
            "  [Batch 360/6245] Loss: 0.4104\n",
            "  [Batch 370/6245] Loss: 0.4258\n",
            "  [Batch 380/6245] Loss: 0.1447\n",
            "  [Batch 390/6245] Loss: 0.4765\n",
            "  [Batch 400/6245] Loss: 0.3223\n",
            "  [Batch 410/6245] Loss: 0.3820\n",
            "  [Batch 420/6245] Loss: 0.5434\n",
            "  [Batch 430/6245] Loss: 0.2411\n",
            "  [Batch 440/6245] Loss: 0.3406\n",
            "  [Batch 450/6245] Loss: 0.1849\n",
            "  [Batch 460/6245] Loss: 0.4639\n",
            "  [Batch 470/6245] Loss: 0.4394\n",
            "  [Batch 480/6245] Loss: 0.3827\n",
            "  [Batch 490/6245] Loss: 0.2891\n",
            "  [Batch 500/6245] Loss: 0.3064\n",
            "  [Batch 510/6245] Loss: 0.4449\n",
            "  [Batch 520/6245] Loss: 0.3249\n",
            "  [Batch 530/6245] Loss: 0.7044\n",
            "  [Batch 540/6245] Loss: 0.3137\n",
            "  [Batch 550/6245] Loss: 0.3001\n",
            "  [Batch 560/6245] Loss: 0.3867\n",
            "  [Batch 570/6245] Loss: 0.5277\n",
            "  [Batch 580/6245] Loss: 0.2503\n",
            "  [Batch 590/6245] Loss: 0.3660\n",
            "  [Batch 600/6245] Loss: 0.3055\n",
            "  [Batch 610/6245] Loss: 0.2405\n",
            "  [Batch 620/6245] Loss: 0.3547\n",
            "  [Batch 630/6245] Loss: 0.1255\n",
            "  [Batch 640/6245] Loss: 0.3592\n",
            "  [Batch 650/6245] Loss: 0.2920\n",
            "  [Batch 660/6245] Loss: 0.5204\n",
            "  [Batch 670/6245] Loss: 0.3494\n",
            "  [Batch 680/6245] Loss: 0.2640\n",
            "  [Batch 690/6245] Loss: 0.3182\n",
            "  [Batch 700/6245] Loss: 0.3066\n",
            "  [Batch 710/6245] Loss: 0.3675\n",
            "  [Batch 720/6245] Loss: 0.3013\n",
            "  [Batch 730/6245] Loss: 0.2395\n",
            "  [Batch 740/6245] Loss: 0.2757\n",
            "  [Batch 750/6245] Loss: 0.1622\n",
            "  [Batch 760/6245] Loss: 0.4333\n",
            "  [Batch 770/6245] Loss: 0.3067\n",
            "  [Batch 780/6245] Loss: 0.3686\n",
            "  [Batch 790/6245] Loss: 0.5592\n",
            "  [Batch 800/6245] Loss: 0.4108\n",
            "  [Batch 810/6245] Loss: 0.2699\n",
            "  [Batch 820/6245] Loss: 0.1785\n",
            "  [Batch 830/6245] Loss: 0.2778\n",
            "  [Batch 840/6245] Loss: 0.1795\n",
            "  [Batch 850/6245] Loss: 0.2669\n",
            "  [Batch 860/6245] Loss: 0.3204\n",
            "  [Batch 870/6245] Loss: 0.4823\n",
            "  [Batch 880/6245] Loss: 0.4831\n",
            "  [Batch 890/6245] Loss: 0.4189\n",
            "  [Batch 900/6245] Loss: 0.3041\n",
            "  [Batch 910/6245] Loss: 0.4669\n",
            "  [Batch 920/6245] Loss: 0.2721\n",
            "  [Batch 930/6245] Loss: 0.2708\n",
            "  [Batch 940/6245] Loss: 0.6166\n",
            "  [Batch 950/6245] Loss: 0.2060\n",
            "  [Batch 960/6245] Loss: 0.1813\n",
            "  [Batch 970/6245] Loss: 0.4162\n",
            "  [Batch 980/6245] Loss: 0.2705\n",
            "  [Batch 990/6245] Loss: 0.3254\n",
            "  [Batch 1000/6245] Loss: 0.5497\n",
            "  [Batch 1010/6245] Loss: 0.3546\n",
            "  [Batch 1020/6245] Loss: 0.2442\n",
            "  [Batch 1030/6245] Loss: 0.3388\n",
            "  [Batch 1040/6245] Loss: 0.2523\n",
            "  [Batch 1050/6245] Loss: 0.2724\n",
            "  [Batch 1060/6245] Loss: 0.3393\n",
            "  [Batch 1070/6245] Loss: 0.1811\n",
            "  [Batch 1080/6245] Loss: 0.4185\n",
            "  [Batch 1090/6245] Loss: 0.5665\n",
            "  [Batch 1100/6245] Loss: 0.2418\n",
            "  [Batch 1110/6245] Loss: 0.3419\n",
            "  [Batch 1120/6245] Loss: 0.2910\n",
            "  [Batch 1130/6245] Loss: 0.3528\n",
            "  [Batch 1140/6245] Loss: 0.4071\n",
            "  [Batch 1150/6245] Loss: 0.3050\n",
            "  [Batch 1160/6245] Loss: 0.5737\n",
            "  [Batch 1170/6245] Loss: 0.2782\n",
            "  [Batch 1180/6245] Loss: 0.4237\n",
            "  [Batch 1190/6245] Loss: 0.2197\n",
            "  [Batch 1200/6245] Loss: 0.3868\n",
            "  [Batch 1210/6245] Loss: 0.2288\n",
            "  [Batch 1220/6245] Loss: 0.3089\n",
            "  [Batch 1230/6245] Loss: 0.2123\n",
            "  [Batch 1240/6245] Loss: 0.3947\n",
            "  [Batch 1250/6245] Loss: 0.4063\n",
            "  [Batch 1260/6245] Loss: 0.3617\n",
            "  [Batch 1270/6245] Loss: 0.2925\n",
            "  [Batch 1280/6245] Loss: 0.4187\n",
            "  [Batch 1290/6245] Loss: 0.4095\n",
            "  [Batch 1300/6245] Loss: 0.3074\n",
            "  [Batch 1310/6245] Loss: 0.3946\n",
            "  [Batch 1320/6245] Loss: 0.3670\n",
            "  [Batch 1330/6245] Loss: 0.2878\n",
            "  [Batch 1340/6245] Loss: 0.3156\n",
            "  [Batch 1350/6245] Loss: 0.3870\n",
            "  [Batch 1360/6245] Loss: 0.4578\n",
            "  [Batch 1370/6245] Loss: 0.3556\n",
            "  [Batch 1380/6245] Loss: 0.3736\n",
            "  [Batch 1390/6245] Loss: 0.5082\n",
            "  [Batch 1400/6245] Loss: 0.3520\n",
            "  [Batch 1410/6245] Loss: 0.3875\n",
            "  [Batch 1420/6245] Loss: 0.4147\n",
            "  [Batch 1430/6245] Loss: 0.2648\n",
            "  [Batch 1440/6245] Loss: 0.4170\n",
            "  [Batch 1450/6245] Loss: 0.2847\n",
            "  [Batch 1460/6245] Loss: 0.3157\n",
            "  [Batch 1470/6245] Loss: 0.4353\n",
            "  [Batch 1480/6245] Loss: 0.5296\n",
            "  [Batch 1490/6245] Loss: 0.3929\n",
            "  [Batch 1500/6245] Loss: 0.1561\n",
            "  [Batch 1510/6245] Loss: 0.5479\n",
            "  [Batch 1520/6245] Loss: 0.4706\n",
            "  [Batch 1530/6245] Loss: 0.3669\n",
            "  [Batch 1540/6245] Loss: 0.2691\n",
            "  [Batch 1550/6245] Loss: 0.2691\n",
            "  [Batch 1560/6245] Loss: 0.3461\n",
            "  [Batch 1570/6245] Loss: 0.4495\n",
            "  [Batch 1580/6245] Loss: 0.4231\n",
            "  [Batch 1590/6245] Loss: 0.5545\n",
            "  [Batch 1600/6245] Loss: 0.4023\n",
            "  [Batch 1610/6245] Loss: 0.3419\n",
            "  [Batch 1620/6245] Loss: 0.3471\n",
            "  [Batch 1630/6245] Loss: 0.4859\n",
            "  [Batch 1640/6245] Loss: 0.3182\n",
            "  [Batch 1650/6245] Loss: 0.3955\n",
            "  [Batch 1660/6245] Loss: 0.1870\n",
            "  [Batch 1670/6245] Loss: 0.3869\n",
            "  [Batch 1680/6245] Loss: 0.2594\n",
            "  [Batch 1690/6245] Loss: 0.4042\n",
            "  [Batch 1700/6245] Loss: 0.4309\n",
            "  [Batch 1710/6245] Loss: 0.4363\n",
            "  [Batch 1720/6245] Loss: 0.5651\n",
            "  [Batch 1730/6245] Loss: 0.3097\n",
            "  [Batch 1740/6245] Loss: 0.2058\n",
            "  [Batch 1750/6245] Loss: 0.2787\n",
            "  [Batch 1760/6245] Loss: 0.3582\n",
            "  [Batch 1770/6245] Loss: 0.2661\n",
            "  [Batch 1780/6245] Loss: 0.3785\n",
            "  [Batch 1790/6245] Loss: 0.3637\n",
            "  [Batch 1800/6245] Loss: 0.3036\n",
            "  [Batch 1810/6245] Loss: 0.2324\n",
            "  [Batch 1820/6245] Loss: 0.3356\n",
            "  [Batch 1830/6245] Loss: 0.2357\n",
            "  [Batch 1840/6245] Loss: 0.4325\n",
            "  [Batch 1850/6245] Loss: 0.1381\n",
            "  [Batch 1860/6245] Loss: 0.3214\n",
            "  [Batch 1870/6245] Loss: 0.4962\n",
            "  [Batch 1880/6245] Loss: 0.3464\n",
            "  [Batch 1890/6245] Loss: 0.3237\n",
            "  [Batch 1900/6245] Loss: 0.3130\n",
            "  [Batch 1910/6245] Loss: 0.3658\n",
            "  [Batch 1920/6245] Loss: 0.4708\n",
            "  [Batch 1930/6245] Loss: 0.2311\n",
            "  [Batch 1940/6245] Loss: 0.1372\n",
            "  [Batch 1950/6245] Loss: 0.3476\n",
            "  [Batch 1960/6245] Loss: 0.1580\n",
            "  [Batch 1970/6245] Loss: 0.2100\n",
            "  [Batch 1980/6245] Loss: 0.3289\n",
            "  [Batch 1990/6245] Loss: 0.2261\n",
            "  [Batch 2000/6245] Loss: 0.4369\n",
            "  [Batch 2010/6245] Loss: 0.2374\n",
            "  [Batch 2020/6245] Loss: 0.3249\n",
            "  [Batch 2030/6245] Loss: 0.3164\n",
            "  [Batch 2040/6245] Loss: 0.2838\n",
            "  [Batch 2050/6245] Loss: 0.3091\n",
            "  [Batch 2060/6245] Loss: 0.2096\n",
            "  [Batch 2070/6245] Loss: 0.2866\n",
            "  [Batch 2080/6245] Loss: 0.3333\n",
            "  [Batch 2090/6245] Loss: 0.2535\n",
            "  [Batch 2100/6245] Loss: 0.4041\n",
            "  [Batch 2110/6245] Loss: 0.2529\n",
            "  [Batch 2120/6245] Loss: 0.2083\n",
            "  [Batch 2130/6245] Loss: 0.2838\n",
            "  [Batch 2140/6245] Loss: 0.2500\n",
            "  [Batch 2150/6245] Loss: 0.3899\n",
            "  [Batch 2160/6245] Loss: 0.2526\n",
            "  [Batch 2170/6245] Loss: 0.2087\n",
            "  [Batch 2180/6245] Loss: 0.4057\n",
            "  [Batch 2190/6245] Loss: 0.4257\n",
            "  [Batch 2200/6245] Loss: 0.3369\n",
            "  [Batch 2210/6245] Loss: 0.4726\n",
            "  [Batch 2220/6245] Loss: 0.3085\n",
            "  [Batch 2230/6245] Loss: 0.5043\n",
            "  [Batch 2240/6245] Loss: 0.3584\n",
            "  [Batch 2250/6245] Loss: 0.2721\n",
            "  [Batch 2260/6245] Loss: 0.3086\n",
            "  [Batch 2270/6245] Loss: 0.4280\n",
            "  [Batch 2280/6245] Loss: 0.3390\n",
            "  [Batch 2290/6245] Loss: 0.2613\n",
            "  [Batch 2300/6245] Loss: 0.2223\n",
            "  [Batch 2310/6245] Loss: 0.4319\n",
            "  [Batch 2320/6245] Loss: 0.3316\n",
            "  [Batch 2330/6245] Loss: 0.5232\n",
            "  [Batch 2340/6245] Loss: 0.3846\n",
            "  [Batch 2350/6245] Loss: 0.3415\n",
            "  [Batch 2360/6245] Loss: 0.3423\n",
            "  [Batch 2370/6245] Loss: 0.2319\n",
            "  [Batch 2380/6245] Loss: 0.3256\n",
            "  [Batch 2390/6245] Loss: 0.3967\n",
            "  [Batch 2400/6245] Loss: 0.2552\n",
            "  [Batch 2410/6245] Loss: 0.3030\n",
            "  [Batch 2420/6245] Loss: 0.4296\n",
            "  [Batch 2430/6245] Loss: 0.2368\n",
            "  [Batch 2440/6245] Loss: 0.3553\n",
            "  [Batch 2450/6245] Loss: 0.5896\n",
            "  [Batch 2460/6245] Loss: 0.2926\n",
            "  [Batch 2470/6245] Loss: 0.2392\n",
            "  [Batch 2480/6245] Loss: 0.2316\n",
            "  [Batch 2490/6245] Loss: 0.2354\n",
            "  [Batch 2500/6245] Loss: 0.4226\n",
            "  [Batch 2510/6245] Loss: 0.4429\n",
            "  [Batch 2520/6245] Loss: 0.4323\n",
            "  [Batch 2530/6245] Loss: 0.1880\n",
            "  [Batch 2540/6245] Loss: 0.4300\n",
            "  [Batch 2550/6245] Loss: 0.3735\n",
            "  [Batch 2560/6245] Loss: 0.2033\n",
            "  [Batch 2570/6245] Loss: 0.3321\n",
            "  [Batch 2580/6245] Loss: 0.2139\n",
            "  [Batch 2590/6245] Loss: 0.3714\n",
            "  [Batch 2600/6245] Loss: 0.3519\n",
            "  [Batch 2610/6245] Loss: 0.4929\n",
            "  [Batch 2620/6245] Loss: 0.2360\n",
            "  [Batch 2630/6245] Loss: 0.4058\n",
            "  [Batch 2640/6245] Loss: 0.2168\n",
            "  [Batch 2650/6245] Loss: 0.3971\n",
            "  [Batch 2660/6245] Loss: 0.3439\n",
            "  [Batch 2670/6245] Loss: 0.2693\n",
            "  [Batch 2680/6245] Loss: 0.2851\n",
            "  [Batch 2690/6245] Loss: 0.2479\n",
            "  [Batch 2700/6245] Loss: 0.3938\n",
            "  [Batch 2710/6245] Loss: 0.2205\n",
            "  [Batch 2720/6245] Loss: 0.3906\n",
            "  [Batch 2730/6245] Loss: 0.2083\n",
            "  [Batch 2740/6245] Loss: 0.2217\n",
            "  [Batch 2750/6245] Loss: 0.2493\n",
            "  [Batch 2760/6245] Loss: 0.3070\n",
            "  [Batch 2770/6245] Loss: 0.1943\n",
            "  [Batch 2780/6245] Loss: 0.2187\n",
            "  [Batch 2790/6245] Loss: 0.3951\n",
            "  [Batch 2800/6245] Loss: 0.3018\n",
            "  [Batch 2810/6245] Loss: 0.2075\n",
            "  [Batch 2820/6245] Loss: 0.3044\n",
            "  [Batch 2830/6245] Loss: 0.5111\n",
            "  [Batch 2840/6245] Loss: 0.4672\n",
            "  [Batch 2850/6245] Loss: 0.2380\n",
            "  [Batch 2860/6245] Loss: 0.2933\n",
            "  [Batch 2870/6245] Loss: 0.4188\n",
            "  [Batch 2880/6245] Loss: 0.3174\n",
            "  [Batch 2890/6245] Loss: 0.3460\n",
            "  [Batch 2900/6245] Loss: 0.3203\n",
            "  [Batch 2910/6245] Loss: 0.2408\n",
            "  [Batch 2920/6245] Loss: 0.3562\n",
            "  [Batch 2930/6245] Loss: 0.3356\n",
            "  [Batch 2940/6245] Loss: 0.4276\n",
            "  [Batch 2950/6245] Loss: 0.5723\n",
            "  [Batch 2960/6245] Loss: 0.4322\n",
            "  [Batch 2970/6245] Loss: 0.2795\n",
            "  [Batch 2980/6245] Loss: 0.1932\n",
            "  [Batch 2990/6245] Loss: 0.3495\n",
            "  [Batch 3000/6245] Loss: 0.3152\n",
            "  [Batch 3010/6245] Loss: 0.5252\n",
            "  [Batch 3020/6245] Loss: 0.4304\n",
            "  [Batch 3030/6245] Loss: 0.4971\n",
            "  [Batch 3040/6245] Loss: 0.3121\n",
            "  [Batch 3050/6245] Loss: 0.3188\n",
            "  [Batch 3060/6245] Loss: 0.2729\n",
            "  [Batch 3070/6245] Loss: 0.5553\n",
            "  [Batch 3080/6245] Loss: 0.3930\n",
            "  [Batch 3090/6245] Loss: 0.2294\n",
            "  [Batch 3100/6245] Loss: 0.4161\n",
            "  [Batch 3110/6245] Loss: 0.1246\n",
            "  [Batch 3120/6245] Loss: 0.3270\n",
            "  [Batch 3130/6245] Loss: 0.3243\n",
            "  [Batch 3140/6245] Loss: 0.4225\n",
            "  [Batch 3150/6245] Loss: 0.5116\n",
            "  [Batch 3160/6245] Loss: 0.2572\n",
            "  [Batch 3170/6245] Loss: 0.3544\n",
            "  [Batch 3180/6245] Loss: 0.3786\n",
            "  [Batch 3190/6245] Loss: 0.2567\n",
            "  [Batch 3200/6245] Loss: 0.4181\n",
            "  [Batch 3210/6245] Loss: 0.1940\n",
            "  [Batch 3220/6245] Loss: 0.4096\n",
            "  [Batch 3230/6245] Loss: 0.3162\n",
            "  [Batch 3240/6245] Loss: 0.3886\n",
            "  [Batch 3250/6245] Loss: 0.2214\n",
            "  [Batch 3260/6245] Loss: 0.4438\n",
            "  [Batch 3270/6245] Loss: 0.4218\n",
            "  [Batch 3280/6245] Loss: 0.2400\n",
            "  [Batch 3290/6245] Loss: 0.2153\n",
            "  [Batch 3300/6245] Loss: 0.3439\n",
            "  [Batch 3310/6245] Loss: 0.1321\n",
            "  [Batch 3320/6245] Loss: 0.3541\n",
            "  [Batch 3330/6245] Loss: 0.3052\n",
            "  [Batch 3340/6245] Loss: 0.2798\n",
            "  [Batch 3350/6245] Loss: 0.2705\n",
            "  [Batch 3360/6245] Loss: 0.3917\n",
            "  [Batch 3370/6245] Loss: 0.3827\n",
            "  [Batch 3380/6245] Loss: 0.5234\n",
            "  [Batch 3390/6245] Loss: 0.2529\n",
            "  [Batch 3400/6245] Loss: 0.4766\n",
            "  [Batch 3410/6245] Loss: 0.3555\n",
            "  [Batch 3420/6245] Loss: 0.3319\n",
            "  [Batch 3430/6245] Loss: 0.3528\n",
            "  [Batch 3440/6245] Loss: 0.3054\n",
            "  [Batch 3450/6245] Loss: 0.2690\n",
            "  [Batch 3460/6245] Loss: 0.3269\n",
            "  [Batch 3470/6245] Loss: 0.4718\n",
            "  [Batch 3480/6245] Loss: 0.3400\n",
            "  [Batch 3490/6245] Loss: 0.2573\n",
            "  [Batch 3500/6245] Loss: 0.2934\n",
            "  [Batch 3510/6245] Loss: 0.3596\n",
            "  [Batch 3520/6245] Loss: 0.2129\n",
            "  [Batch 3530/6245] Loss: 0.3636\n",
            "  [Batch 3540/6245] Loss: 0.1965\n",
            "  [Batch 3550/6245] Loss: 0.2778\n",
            "  [Batch 3560/6245] Loss: 0.2728\n",
            "  [Batch 3570/6245] Loss: 0.3402\n",
            "  [Batch 3580/6245] Loss: 0.3530\n",
            "  [Batch 3590/6245] Loss: 0.3045\n",
            "  [Batch 3600/6245] Loss: 0.2666\n",
            "  [Batch 3610/6245] Loss: 0.1677\n",
            "  [Batch 3620/6245] Loss: 0.2442\n",
            "  [Batch 3630/6245] Loss: 0.2810\n",
            "  [Batch 3640/6245] Loss: 0.3371\n",
            "  [Batch 3650/6245] Loss: 0.2252\n",
            "  [Batch 3660/6245] Loss: 0.3878\n",
            "  [Batch 3670/6245] Loss: 0.4017\n",
            "  [Batch 3680/6245] Loss: 0.5067\n",
            "  [Batch 3690/6245] Loss: 0.2734\n",
            "  [Batch 3700/6245] Loss: 0.3292\n",
            "  [Batch 3710/6245] Loss: 0.2622\n",
            "  [Batch 3720/6245] Loss: 0.4083\n",
            "  [Batch 3730/6245] Loss: 0.3588\n",
            "  [Batch 3740/6245] Loss: 0.4789\n",
            "  [Batch 3750/6245] Loss: 0.3182\n",
            "  [Batch 3760/6245] Loss: 0.2705\n",
            "  [Batch 3770/6245] Loss: 0.2945\n",
            "  [Batch 3780/6245] Loss: 0.2003\n",
            "  [Batch 3790/6245] Loss: 0.5211\n",
            "  [Batch 3800/6245] Loss: 0.1786\n",
            "  [Batch 3810/6245] Loss: 0.2438\n",
            "  [Batch 3820/6245] Loss: 0.1776\n",
            "  [Batch 3830/6245] Loss: 0.3868\n",
            "  [Batch 3840/6245] Loss: 0.4702\n",
            "  [Batch 3850/6245] Loss: 0.3229\n",
            "  [Batch 3860/6245] Loss: 0.4125\n",
            "  [Batch 3870/6245] Loss: 0.1527\n",
            "  [Batch 3880/6245] Loss: 0.3996\n",
            "  [Batch 3890/6245] Loss: 0.3062\n",
            "  [Batch 3900/6245] Loss: 0.3027\n",
            "  [Batch 3910/6245] Loss: 0.2977\n",
            "  [Batch 3920/6245] Loss: 0.4229\n",
            "  [Batch 3930/6245] Loss: 0.2888\n",
            "  [Batch 3940/6245] Loss: 0.3029\n",
            "  [Batch 3950/6245] Loss: 0.2040\n",
            "  [Batch 3960/6245] Loss: 0.2016\n",
            "  [Batch 3970/6245] Loss: 0.2859\n",
            "  [Batch 3980/6245] Loss: 0.3465\n",
            "  [Batch 3990/6245] Loss: 0.1398\n",
            "  [Batch 4000/6245] Loss: 0.2619\n",
            "  [Batch 4010/6245] Loss: 0.3139\n",
            "  [Batch 4020/6245] Loss: 0.2615\n",
            "  [Batch 4030/6245] Loss: 0.4242\n",
            "  [Batch 4040/6245] Loss: 0.5838\n",
            "  [Batch 4050/6245] Loss: 0.1879\n",
            "  [Batch 4060/6245] Loss: 0.6818\n",
            "  [Batch 4070/6245] Loss: 0.3883\n",
            "  [Batch 4080/6245] Loss: 0.3194\n",
            "  [Batch 4090/6245] Loss: 0.2508\n",
            "  [Batch 4100/6245] Loss: 0.3120\n",
            "  [Batch 4110/6245] Loss: 0.4353\n",
            "  [Batch 4120/6245] Loss: 0.3162\n",
            "  [Batch 4130/6245] Loss: 0.3147\n",
            "  [Batch 4140/6245] Loss: 0.3899\n",
            "  [Batch 4150/6245] Loss: 0.3448\n",
            "  [Batch 4160/6245] Loss: 0.3165\n",
            "  [Batch 4170/6245] Loss: 0.2098\n",
            "  [Batch 4180/6245] Loss: 0.4340\n",
            "  [Batch 4190/6245] Loss: 0.3402\n",
            "  [Batch 4200/6245] Loss: 0.2819\n",
            "  [Batch 4210/6245] Loss: 0.5805\n",
            "  [Batch 4220/6245] Loss: 0.2243\n",
            "  [Batch 4230/6245] Loss: 0.1769\n",
            "  [Batch 4240/6245] Loss: 0.2415\n",
            "  [Batch 4250/6245] Loss: 0.3275\n",
            "  [Batch 4260/6245] Loss: 0.2623\n",
            "  [Batch 4270/6245] Loss: 0.4037\n",
            "  [Batch 4280/6245] Loss: 0.2501\n",
            "  [Batch 4290/6245] Loss: 0.2669\n",
            "  [Batch 4300/6245] Loss: 0.2563\n",
            "  [Batch 4310/6245] Loss: 0.3546\n",
            "  [Batch 4320/6245] Loss: 0.2175\n",
            "  [Batch 4330/6245] Loss: 0.4162\n",
            "  [Batch 4340/6245] Loss: 0.3738\n",
            "  [Batch 4350/6245] Loss: 0.3887\n",
            "  [Batch 4360/6245] Loss: 0.4582\n",
            "  [Batch 4370/6245] Loss: 0.2478\n",
            "  [Batch 4380/6245] Loss: 0.3434\n",
            "  [Batch 4390/6245] Loss: 0.3232\n",
            "  [Batch 4400/6245] Loss: 0.2011\n",
            "  [Batch 4410/6245] Loss: 0.2469\n",
            "  [Batch 4420/6245] Loss: 0.3708\n",
            "  [Batch 4430/6245] Loss: 0.1691\n",
            "  [Batch 4440/6245] Loss: 0.3132\n",
            "  [Batch 4450/6245] Loss: 0.3940\n",
            "  [Batch 4460/6245] Loss: 0.2742\n",
            "  [Batch 4470/6245] Loss: 0.3515\n",
            "  [Batch 4480/6245] Loss: 0.3724\n",
            "  [Batch 4490/6245] Loss: 0.2675\n",
            "  [Batch 4500/6245] Loss: 0.2989\n",
            "  [Batch 4510/6245] Loss: 0.4013\n",
            "  [Batch 4520/6245] Loss: 0.3564\n",
            "  [Batch 4530/6245] Loss: 0.2125\n",
            "  [Batch 4540/6245] Loss: 0.3324\n",
            "  [Batch 4550/6245] Loss: 0.4448\n",
            "  [Batch 4560/6245] Loss: 0.3594\n",
            "  [Batch 4570/6245] Loss: 0.1662\n",
            "  [Batch 4580/6245] Loss: 0.3076\n",
            "  [Batch 4590/6245] Loss: 0.3931\n",
            "  [Batch 4600/6245] Loss: 0.3803\n",
            "  [Batch 4610/6245] Loss: 0.2747\n",
            "  [Batch 4620/6245] Loss: 0.2738\n",
            "  [Batch 4630/6245] Loss: 0.2477\n",
            "  [Batch 4640/6245] Loss: 0.1214\n",
            "  [Batch 4650/6245] Loss: 0.2749\n",
            "  [Batch 4660/6245] Loss: 0.2006\n",
            "  [Batch 4670/6245] Loss: 0.3781\n",
            "  [Batch 4680/6245] Loss: 0.2532\n",
            "  [Batch 4690/6245] Loss: 0.2139\n",
            "  [Batch 4700/6245] Loss: 0.1926\n",
            "  [Batch 4710/6245] Loss: 0.3238\n",
            "  [Batch 4720/6245] Loss: 0.2940\n",
            "  [Batch 4730/6245] Loss: 0.2604\n",
            "  [Batch 4740/6245] Loss: 0.2346\n",
            "  [Batch 4750/6245] Loss: 0.4411\n",
            "  [Batch 4760/6245] Loss: 0.2105\n",
            "  [Batch 4770/6245] Loss: 0.2213\n",
            "  [Batch 4780/6245] Loss: 0.3920\n",
            "  [Batch 4790/6245] Loss: 0.2397\n",
            "  [Batch 4800/6245] Loss: 0.3002\n",
            "  [Batch 4810/6245] Loss: 0.2678\n",
            "  [Batch 4820/6245] Loss: 0.3395\n",
            "  [Batch 4830/6245] Loss: 0.2045\n",
            "  [Batch 4840/6245] Loss: 0.3297\n",
            "  [Batch 4850/6245] Loss: 0.2516\n",
            "  [Batch 4860/6245] Loss: 0.4724\n",
            "  [Batch 4870/6245] Loss: 0.2982\n",
            "  [Batch 4880/6245] Loss: 0.2693\n",
            "  [Batch 4890/6245] Loss: 0.4213\n",
            "  [Batch 4900/6245] Loss: 0.2993\n",
            "  [Batch 4910/6245] Loss: 0.2216\n",
            "  [Batch 4920/6245] Loss: 0.4094\n",
            "  [Batch 4930/6245] Loss: 0.6267\n",
            "  [Batch 4940/6245] Loss: 0.4472\n",
            "  [Batch 4950/6245] Loss: 0.1711\n",
            "  [Batch 4960/6245] Loss: 0.2132\n",
            "  [Batch 4970/6245] Loss: 0.2072\n",
            "  [Batch 4980/6245] Loss: 0.3316\n",
            "  [Batch 4990/6245] Loss: 0.4367\n",
            "  [Batch 5000/6245] Loss: 0.1243\n",
            "  [Batch 5010/6245] Loss: 0.3187\n",
            "  [Batch 5020/6245] Loss: 0.3877\n",
            "  [Batch 5030/6245] Loss: 0.3530\n",
            "  [Batch 5040/6245] Loss: 0.5203\n",
            "  [Batch 5050/6245] Loss: 0.3498\n",
            "  [Batch 5060/6245] Loss: 0.3001\n",
            "  [Batch 5070/6245] Loss: 0.2693\n",
            "  [Batch 5080/6245] Loss: 0.3138\n",
            "  [Batch 5090/6245] Loss: 0.3032\n",
            "  [Batch 5100/6245] Loss: 0.2907\n",
            "  [Batch 5110/6245] Loss: 0.4243\n",
            "  [Batch 5120/6245] Loss: 0.3035\n",
            "  [Batch 5130/6245] Loss: 0.2548\n",
            "  [Batch 5140/6245] Loss: 0.4341\n",
            "  [Batch 5150/6245] Loss: 0.3433\n",
            "  [Batch 5160/6245] Loss: 0.3315\n",
            "  [Batch 5170/6245] Loss: 0.2451\n",
            "  [Batch 5180/6245] Loss: 0.2269\n",
            "  [Batch 5190/6245] Loss: 0.2805\n",
            "  [Batch 5200/6245] Loss: 0.4684\n",
            "  [Batch 5210/6245] Loss: 0.2659\n",
            "  [Batch 5220/6245] Loss: 0.2044\n",
            "  [Batch 5230/6245] Loss: 0.2451\n",
            "  [Batch 5240/6245] Loss: 0.3595\n",
            "  [Batch 5250/6245] Loss: 0.2808\n",
            "  [Batch 5260/6245] Loss: 0.2179\n",
            "  [Batch 5270/6245] Loss: 0.4044\n",
            "  [Batch 5280/6245] Loss: 0.4459\n",
            "  [Batch 5290/6245] Loss: 0.3225\n",
            "  [Batch 5300/6245] Loss: 0.2837\n",
            "  [Batch 5310/6245] Loss: 0.4253\n",
            "  [Batch 5320/6245] Loss: 0.4253\n",
            "  [Batch 5330/6245] Loss: 0.5270\n",
            "  [Batch 5340/6245] Loss: 0.2679\n",
            "  [Batch 5350/6245] Loss: 0.2575\n",
            "  [Batch 5360/6245] Loss: 0.5305\n",
            "  [Batch 5370/6245] Loss: 0.4570\n",
            "  [Batch 5380/6245] Loss: 0.3988\n",
            "  [Batch 5390/6245] Loss: 0.3478\n",
            "  [Batch 5400/6245] Loss: 0.2073\n",
            "  [Batch 5410/6245] Loss: 0.1556\n",
            "  [Batch 5420/6245] Loss: 0.2950\n",
            "  [Batch 5430/6245] Loss: 0.2192\n",
            "  [Batch 5440/6245] Loss: 0.2161\n",
            "  [Batch 5450/6245] Loss: 0.3823\n",
            "  [Batch 5460/6245] Loss: 0.2544\n",
            "  [Batch 5470/6245] Loss: 0.3053\n",
            "  [Batch 5480/6245] Loss: 0.6526\n",
            "  [Batch 5490/6245] Loss: 0.2609\n",
            "  [Batch 5500/6245] Loss: 0.2377\n",
            "  [Batch 5510/6245] Loss: 0.3722\n",
            "  [Batch 5520/6245] Loss: 0.1768\n",
            "  [Batch 5530/6245] Loss: 0.5015\n",
            "  [Batch 5540/6245] Loss: 0.3142\n",
            "  [Batch 5550/6245] Loss: 0.3845\n",
            "  [Batch 5560/6245] Loss: 0.3554\n",
            "  [Batch 5570/6245] Loss: 0.2175\n",
            "  [Batch 5580/6245] Loss: 0.2775\n",
            "  [Batch 5590/6245] Loss: 0.2698\n",
            "  [Batch 5600/6245] Loss: 0.1935\n",
            "  [Batch 5610/6245] Loss: 0.2199\n",
            "  [Batch 5620/6245] Loss: 0.2061\n",
            "  [Batch 5630/6245] Loss: 0.3181\n",
            "  [Batch 5640/6245] Loss: 0.3386\n",
            "  [Batch 5650/6245] Loss: 0.4997\n",
            "  [Batch 5660/6245] Loss: 0.2556\n",
            "  [Batch 5670/6245] Loss: 0.3236\n",
            "  [Batch 5680/6245] Loss: 0.1924\n",
            "  [Batch 5690/6245] Loss: 0.1266\n",
            "  [Batch 5700/6245] Loss: 0.3765\n",
            "  [Batch 5710/6245] Loss: 0.5103\n",
            "  [Batch 5720/6245] Loss: 0.2822\n",
            "  [Batch 5730/6245] Loss: 0.4277\n",
            "  [Batch 5740/6245] Loss: 0.3408\n",
            "  [Batch 5750/6245] Loss: 0.1888\n",
            "  [Batch 5760/6245] Loss: 0.4413\n",
            "  [Batch 5770/6245] Loss: 0.2370\n",
            "  [Batch 5780/6245] Loss: 0.1490\n",
            "  [Batch 5790/6245] Loss: 0.2711\n",
            "  [Batch 5800/6245] Loss: 0.2553\n",
            "  [Batch 5810/6245] Loss: 0.1749\n",
            "  [Batch 5820/6245] Loss: 0.2571\n",
            "  [Batch 5830/6245] Loss: 0.3843\n",
            "  [Batch 5840/6245] Loss: 0.2487\n",
            "  [Batch 5850/6245] Loss: 0.3125\n",
            "  [Batch 5860/6245] Loss: 0.3617\n",
            "  [Batch 5870/6245] Loss: 0.3553\n",
            "  [Batch 5880/6245] Loss: 0.1693\n",
            "  [Batch 5890/6245] Loss: 0.2966\n",
            "  [Batch 5900/6245] Loss: 0.2902\n",
            "  [Batch 5910/6245] Loss: 0.2906\n",
            "  [Batch 5920/6245] Loss: 0.2676\n",
            "  [Batch 5930/6245] Loss: 0.2292\n",
            "  [Batch 5940/6245] Loss: 0.2271\n",
            "  [Batch 5950/6245] Loss: 0.2315\n",
            "  [Batch 5960/6245] Loss: 0.2035\n",
            "  [Batch 5970/6245] Loss: 0.3768\n",
            "  [Batch 5980/6245] Loss: 0.3951\n",
            "  [Batch 5990/6245] Loss: 0.4147\n",
            "  [Batch 6000/6245] Loss: 0.1443\n",
            "  [Batch 6010/6245] Loss: 0.4909\n",
            "  [Batch 6020/6245] Loss: 0.4248\n",
            "  [Batch 6030/6245] Loss: 0.2802\n",
            "  [Batch 6040/6245] Loss: 0.2148\n",
            "  [Batch 6050/6245] Loss: 0.2300\n",
            "  [Batch 6060/6245] Loss: 0.2580\n",
            "  [Batch 6070/6245] Loss: 0.2637\n",
            "  [Batch 6080/6245] Loss: 0.4822\n",
            "  [Batch 6090/6245] Loss: 0.2855\n",
            "  [Batch 6100/6245] Loss: 0.4483\n",
            "  [Batch 6110/6245] Loss: 0.2095\n",
            "  [Batch 6120/6245] Loss: 0.2412\n",
            "  [Batch 6130/6245] Loss: 0.4246\n",
            "  [Batch 6140/6245] Loss: 0.4321\n",
            "  [Batch 6150/6245] Loss: 0.1903\n",
            "  [Batch 6160/6245] Loss: 0.2991\n",
            "  [Batch 6170/6245] Loss: 0.1319\n",
            "  [Batch 6180/6245] Loss: 0.2962\n",
            "  [Batch 6190/6245] Loss: 0.2114\n",
            "  [Batch 6200/6245] Loss: 0.4037\n",
            "  [Batch 6210/6245] Loss: 0.4041\n",
            "  [Batch 6220/6245] Loss: 0.2688\n",
            "  [Batch 6230/6245] Loss: 0.2712\n",
            "  [Batch 6240/6245] Loss: 0.1723\n",
            "  [Batch 6245/6245] Loss: 0.7257\n",
            "[Epoch 4] Train Loss: 0.3263, Acc: 85.97% | Val Loss: 0.3173, Acc: 86.24%\n",
            "[INFO] Model checkpoint saved at ./models\\breastnet_epoch4.pth\n",
            "\n",
            "[Epoch 5/10]\n",
            "  [Batch 10/6245] Loss: 0.2604\n",
            "  [Batch 20/6245] Loss: 0.3030\n",
            "  [Batch 30/6245] Loss: 0.2094\n",
            "  [Batch 40/6245] Loss: 0.4002\n",
            "  [Batch 50/6245] Loss: 0.3129\n",
            "  [Batch 60/6245] Loss: 0.2452\n",
            "  [Batch 70/6245] Loss: 0.4990\n",
            "  [Batch 80/6245] Loss: 0.3038\n",
            "  [Batch 90/6245] Loss: 0.3433\n",
            "  [Batch 100/6245] Loss: 0.1444\n",
            "  [Batch 110/6245] Loss: 0.5072\n",
            "  [Batch 120/6245] Loss: 0.1868\n",
            "  [Batch 130/6245] Loss: 0.2530\n",
            "  [Batch 140/6245] Loss: 0.1867\n",
            "  [Batch 150/6245] Loss: 0.3103\n",
            "  [Batch 160/6245] Loss: 0.4239\n",
            "  [Batch 170/6245] Loss: 0.3940\n",
            "  [Batch 180/6245] Loss: 0.4903\n",
            "  [Batch 190/6245] Loss: 0.2316\n",
            "  [Batch 200/6245] Loss: 0.5162\n",
            "  [Batch 210/6245] Loss: 0.3191\n",
            "  [Batch 220/6245] Loss: 0.2035\n",
            "  [Batch 230/6245] Loss: 0.4948\n",
            "  [Batch 240/6245] Loss: 0.3294\n",
            "  [Batch 250/6245] Loss: 0.2493\n",
            "  [Batch 260/6245] Loss: 0.2945\n",
            "  [Batch 270/6245] Loss: 0.3574\n",
            "  [Batch 280/6245] Loss: 0.4051\n",
            "  [Batch 290/6245] Loss: 0.3122\n",
            "  [Batch 300/6245] Loss: 0.2970\n",
            "  [Batch 310/6245] Loss: 0.2493\n",
            "  [Batch 320/6245] Loss: 0.2511\n",
            "  [Batch 330/6245] Loss: 0.2615\n",
            "  [Batch 340/6245] Loss: 0.2650\n",
            "  [Batch 350/6245] Loss: 0.2832\n",
            "  [Batch 360/6245] Loss: 0.3609\n",
            "  [Batch 370/6245] Loss: 0.3136\n",
            "  [Batch 380/6245] Loss: 0.2379\n",
            "  [Batch 390/6245] Loss: 0.2708\n",
            "  [Batch 400/6245] Loss: 0.3551\n",
            "  [Batch 410/6245] Loss: 0.3734\n",
            "  [Batch 420/6245] Loss: 0.2714\n",
            "  [Batch 430/6245] Loss: 0.1366\n",
            "  [Batch 440/6245] Loss: 0.2426\n",
            "  [Batch 450/6245] Loss: 0.2008\n",
            "  [Batch 460/6245] Loss: 0.1605\n",
            "  [Batch 470/6245] Loss: 0.2628\n",
            "  [Batch 480/6245] Loss: 0.2830\n",
            "  [Batch 490/6245] Loss: 0.2165\n",
            "  [Batch 500/6245] Loss: 0.1850\n",
            "  [Batch 510/6245] Loss: 0.2692\n",
            "  [Batch 520/6245] Loss: 0.1840\n",
            "  [Batch 530/6245] Loss: 0.4445\n",
            "  [Batch 540/6245] Loss: 0.5444\n",
            "  [Batch 550/6245] Loss: 0.3228\n",
            "  [Batch 560/6245] Loss: 0.3612\n",
            "  [Batch 570/6245] Loss: 0.1512\n",
            "  [Batch 580/6245] Loss: 0.3743\n",
            "  [Batch 590/6245] Loss: 0.1574\n",
            "  [Batch 600/6245] Loss: 0.4123\n",
            "  [Batch 610/6245] Loss: 0.2811\n",
            "  [Batch 620/6245] Loss: 0.2085\n",
            "  [Batch 630/6245] Loss: 0.2168\n",
            "  [Batch 640/6245] Loss: 0.2580\n",
            "  [Batch 650/6245] Loss: 0.6159\n",
            "  [Batch 660/6245] Loss: 0.3306\n",
            "  [Batch 670/6245] Loss: 0.4597\n",
            "  [Batch 680/6245] Loss: 0.1681\n",
            "  [Batch 690/6245] Loss: 0.4664\n",
            "  [Batch 700/6245] Loss: 0.4461\n",
            "  [Batch 710/6245] Loss: 0.2826\n",
            "  [Batch 720/6245] Loss: 0.3914\n",
            "  [Batch 730/6245] Loss: 0.3494\n",
            "  [Batch 740/6245] Loss: 0.3038\n",
            "  [Batch 750/6245] Loss: 0.2610\n",
            "  [Batch 760/6245] Loss: 0.3643\n",
            "  [Batch 770/6245] Loss: 0.3983\n",
            "  [Batch 780/6245] Loss: 0.4066\n",
            "  [Batch 790/6245] Loss: 0.2522\n",
            "  [Batch 800/6245] Loss: 0.2200\n",
            "  [Batch 810/6245] Loss: 0.3530\n",
            "  [Batch 820/6245] Loss: 0.2543\n",
            "  [Batch 830/6245] Loss: 0.3056\n",
            "  [Batch 840/6245] Loss: 0.4600\n",
            "  [Batch 850/6245] Loss: 0.2626\n",
            "  [Batch 860/6245] Loss: 0.3223\n",
            "  [Batch 870/6245] Loss: 0.2078\n",
            "  [Batch 880/6245] Loss: 0.3079\n",
            "  [Batch 890/6245] Loss: 0.1418\n",
            "  [Batch 900/6245] Loss: 0.2699\n",
            "  [Batch 910/6245] Loss: 0.3979\n",
            "  [Batch 920/6245] Loss: 0.3113\n",
            "  [Batch 930/6245] Loss: 0.2602\n",
            "  [Batch 940/6245] Loss: 0.4170\n",
            "  [Batch 950/6245] Loss: 0.2814\n",
            "  [Batch 960/6245] Loss: 0.3255\n",
            "  [Batch 970/6245] Loss: 0.2357\n",
            "  [Batch 980/6245] Loss: 0.2223\n",
            "  [Batch 990/6245] Loss: 0.2689\n",
            "  [Batch 1000/6245] Loss: 0.4424\n",
            "  [Batch 1010/6245] Loss: 0.2932\n",
            "  [Batch 1020/6245] Loss: 0.5860\n",
            "  [Batch 1030/6245] Loss: 0.2959\n",
            "  [Batch 1040/6245] Loss: 0.3817\n",
            "  [Batch 1050/6245] Loss: 0.2190\n",
            "  [Batch 1060/6245] Loss: 0.4738\n",
            "  [Batch 1070/6245] Loss: 0.2301\n",
            "  [Batch 1080/6245] Loss: 0.3632\n",
            "  [Batch 1090/6245] Loss: 0.3109\n",
            "  [Batch 1100/6245] Loss: 0.2095\n",
            "  [Batch 1110/6245] Loss: 0.4877\n",
            "  [Batch 1120/6245] Loss: 0.3635\n",
            "  [Batch 1130/6245] Loss: 0.2277\n",
            "  [Batch 1140/6245] Loss: 0.2872\n",
            "  [Batch 1150/6245] Loss: 0.4194\n",
            "  [Batch 1160/6245] Loss: 0.2765\n",
            "  [Batch 1170/6245] Loss: 0.2916\n",
            "  [Batch 1180/6245] Loss: 0.2091\n",
            "  [Batch 1190/6245] Loss: 0.2248\n",
            "  [Batch 1200/6245] Loss: 0.2640\n",
            "  [Batch 1210/6245] Loss: 0.3892\n",
            "  [Batch 1220/6245] Loss: 0.4998\n",
            "  [Batch 1230/6245] Loss: 0.3782\n",
            "  [Batch 1240/6245] Loss: 0.3770\n",
            "  [Batch 1250/6245] Loss: 0.3045\n",
            "  [Batch 1260/6245] Loss: 0.0956\n",
            "  [Batch 1270/6245] Loss: 0.1921\n",
            "  [Batch 1280/6245] Loss: 0.3232\n",
            "  [Batch 1290/6245] Loss: 0.4249\n",
            "  [Batch 1300/6245] Loss: 0.2349\n",
            "  [Batch 1310/6245] Loss: 0.2283\n",
            "  [Batch 1320/6245] Loss: 0.4037\n",
            "  [Batch 1330/6245] Loss: 0.2241\n",
            "  [Batch 1340/6245] Loss: 0.2621\n",
            "  [Batch 1350/6245] Loss: 0.2222\n",
            "  [Batch 1360/6245] Loss: 0.3767\n",
            "  [Batch 1370/6245] Loss: 0.3034\n",
            "  [Batch 1380/6245] Loss: 0.3317\n",
            "  [Batch 1390/6245] Loss: 0.2331\n",
            "  [Batch 1400/6245] Loss: 0.4796\n",
            "  [Batch 1410/6245] Loss: 0.3754\n",
            "  [Batch 1420/6245] Loss: 0.4257\n",
            "  [Batch 1430/6245] Loss: 0.3688\n",
            "  [Batch 1440/6245] Loss: 0.2208\n",
            "  [Batch 1450/6245] Loss: 0.3483\n",
            "  [Batch 1460/6245] Loss: 0.2859\n",
            "  [Batch 1470/6245] Loss: 0.2254\n",
            "  [Batch 1480/6245] Loss: 0.3280\n",
            "  [Batch 1490/6245] Loss: 0.4396\n",
            "  [Batch 1500/6245] Loss: 0.5091\n",
            "  [Batch 1510/6245] Loss: 0.2360\n",
            "  [Batch 1520/6245] Loss: 0.2184\n",
            "  [Batch 1530/6245] Loss: 0.4559\n",
            "  [Batch 1540/6245] Loss: 0.2806\n",
            "  [Batch 1550/6245] Loss: 0.5201\n",
            "  [Batch 1560/6245] Loss: 0.3783\n",
            "  [Batch 1570/6245] Loss: 0.3875\n",
            "  [Batch 1580/6245] Loss: 0.2387\n",
            "  [Batch 1590/6245] Loss: 0.2938\n",
            "  [Batch 1600/6245] Loss: 0.3225\n",
            "  [Batch 1610/6245] Loss: 0.3915\n",
            "  [Batch 1620/6245] Loss: 0.4944\n",
            "  [Batch 1630/6245] Loss: 0.2416\n",
            "  [Batch 1640/6245] Loss: 0.3879\n",
            "  [Batch 1650/6245] Loss: 0.3440\n",
            "  [Batch 1660/6245] Loss: 0.1916\n",
            "  [Batch 1670/6245] Loss: 0.2660\n",
            "  [Batch 1680/6245] Loss: 0.4494\n",
            "  [Batch 1690/6245] Loss: 0.1573\n",
            "  [Batch 1700/6245] Loss: 0.1765\n",
            "  [Batch 1710/6245] Loss: 0.3978\n",
            "  [Batch 1720/6245] Loss: 0.3490\n",
            "  [Batch 1730/6245] Loss: 0.1702\n",
            "  [Batch 1740/6245] Loss: 0.3422\n",
            "  [Batch 1750/6245] Loss: 0.2244\n",
            "  [Batch 1760/6245] Loss: 0.2080\n",
            "  [Batch 1770/6245] Loss: 0.3040\n",
            "  [Batch 1780/6245] Loss: 0.2886\n",
            "  [Batch 1790/6245] Loss: 0.2175\n",
            "  [Batch 1800/6245] Loss: 0.3472\n",
            "  [Batch 1810/6245] Loss: 0.4084\n",
            "  [Batch 1820/6245] Loss: 0.3570\n",
            "  [Batch 1830/6245] Loss: 0.3320\n",
            "  [Batch 1840/6245] Loss: 0.4002\n",
            "  [Batch 1850/6245] Loss: 0.4458\n",
            "  [Batch 1860/6245] Loss: 0.2438\n",
            "  [Batch 1870/6245] Loss: 0.3181\n",
            "  [Batch 1880/6245] Loss: 0.2121\n",
            "  [Batch 1890/6245] Loss: 0.3483\n",
            "  [Batch 1900/6245] Loss: 0.3218\n",
            "  [Batch 1910/6245] Loss: 0.5109\n",
            "  [Batch 1920/6245] Loss: 0.2098\n",
            "  [Batch 1930/6245] Loss: 0.2567\n",
            "  [Batch 1940/6245] Loss: 0.2625\n",
            "  [Batch 1950/6245] Loss: 0.2129\n",
            "  [Batch 1960/6245] Loss: 0.2064\n",
            "  [Batch 1970/6245] Loss: 0.2063\n",
            "  [Batch 1980/6245] Loss: 0.5994\n",
            "  [Batch 1990/6245] Loss: 0.3004\n",
            "  [Batch 2000/6245] Loss: 0.2339\n",
            "  [Batch 2010/6245] Loss: 0.3108\n",
            "  [Batch 2020/6245] Loss: 0.1998\n",
            "  [Batch 2030/6245] Loss: 0.4370\n",
            "  [Batch 2040/6245] Loss: 0.3700\n",
            "  [Batch 2050/6245] Loss: 0.2600\n",
            "  [Batch 2060/6245] Loss: 0.3263\n",
            "  [Batch 2070/6245] Loss: 0.2830\n",
            "  [Batch 2080/6245] Loss: 0.2728\n",
            "  [Batch 2090/6245] Loss: 0.2951\n",
            "  [Batch 2100/6245] Loss: 0.4647\n",
            "  [Batch 2110/6245] Loss: 0.2549\n",
            "  [Batch 2120/6245] Loss: 0.3420\n",
            "  [Batch 2130/6245] Loss: 0.2069\n",
            "  [Batch 2140/6245] Loss: 0.3657\n",
            "  [Batch 2150/6245] Loss: 0.3784\n",
            "  [Batch 2160/6245] Loss: 0.4458\n",
            "  [Batch 2170/6245] Loss: 0.2814\n",
            "  [Batch 2180/6245] Loss: 0.3864\n",
            "  [Batch 2190/6245] Loss: 0.5164\n",
            "  [Batch 2200/6245] Loss: 0.2065\n",
            "  [Batch 2210/6245] Loss: 0.2375\n",
            "  [Batch 2220/6245] Loss: 0.3748\n",
            "  [Batch 2230/6245] Loss: 0.2052\n",
            "  [Batch 2240/6245] Loss: 0.3481\n",
            "  [Batch 2250/6245] Loss: 0.3403\n",
            "  [Batch 2260/6245] Loss: 0.2809\n",
            "  [Batch 2270/6245] Loss: 0.3698\n",
            "  [Batch 2280/6245] Loss: 0.2484\n",
            "  [Batch 2290/6245] Loss: 0.3602\n",
            "  [Batch 2300/6245] Loss: 0.2316\n",
            "  [Batch 2310/6245] Loss: 0.2038\n",
            "  [Batch 2320/6245] Loss: 0.2054\n",
            "  [Batch 2330/6245] Loss: 0.3042\n",
            "  [Batch 2340/6245] Loss: 0.4577\n",
            "  [Batch 2350/6245] Loss: 0.3008\n",
            "  [Batch 2360/6245] Loss: 0.1726\n",
            "  [Batch 2370/6245] Loss: 0.4153\n",
            "  [Batch 2380/6245] Loss: 0.4263\n",
            "  [Batch 2390/6245] Loss: 0.4148\n",
            "  [Batch 2400/6245] Loss: 0.2582\n",
            "  [Batch 2410/6245] Loss: 0.3103\n",
            "  [Batch 2420/6245] Loss: 0.4916\n",
            "  [Batch 2430/6245] Loss: 0.2463\n",
            "  [Batch 2440/6245] Loss: 0.3747\n",
            "  [Batch 2450/6245] Loss: 0.3061\n",
            "  [Batch 2460/6245] Loss: 0.2249\n",
            "  [Batch 2470/6245] Loss: 0.4102\n",
            "  [Batch 2480/6245] Loss: 0.4353\n",
            "  [Batch 2490/6245] Loss: 0.4478\n",
            "  [Batch 2500/6245] Loss: 0.2821\n",
            "  [Batch 2510/6245] Loss: 0.2944\n",
            "  [Batch 2520/6245] Loss: 0.4259\n",
            "  [Batch 2530/6245] Loss: 0.1325\n",
            "  [Batch 2540/6245] Loss: 0.2302\n",
            "  [Batch 2550/6245] Loss: 0.8763\n",
            "  [Batch 2560/6245] Loss: 0.3203\n",
            "  [Batch 2570/6245] Loss: 0.3236\n",
            "  [Batch 2580/6245] Loss: 0.3291\n",
            "  [Batch 2590/6245] Loss: 0.4995\n",
            "  [Batch 2600/6245] Loss: 0.1716\n",
            "  [Batch 2610/6245] Loss: 0.2322\n",
            "  [Batch 2620/6245] Loss: 0.2721\n",
            "  [Batch 2630/6245] Loss: 0.2881\n",
            "  [Batch 2640/6245] Loss: 0.6035\n",
            "  [Batch 2650/6245] Loss: 0.2508\n",
            "  [Batch 2660/6245] Loss: 0.3644\n",
            "  [Batch 2670/6245] Loss: 0.3849\n",
            "  [Batch 2680/6245] Loss: 0.3238\n",
            "  [Batch 2690/6245] Loss: 0.1978\n",
            "  [Batch 2700/6245] Loss: 0.4019\n",
            "  [Batch 2710/6245] Loss: 0.3408\n",
            "  [Batch 2720/6245] Loss: 0.4037\n",
            "  [Batch 2730/6245] Loss: 0.2745\n",
            "  [Batch 2740/6245] Loss: 0.1560\n",
            "  [Batch 2750/6245] Loss: 0.2559\n",
            "  [Batch 2760/6245] Loss: 0.4135\n",
            "  [Batch 2770/6245] Loss: 0.3185\n",
            "  [Batch 2780/6245] Loss: 0.2195\n",
            "  [Batch 2790/6245] Loss: 0.2745\n",
            "  [Batch 2800/6245] Loss: 0.3417\n",
            "  [Batch 2810/6245] Loss: 0.4838\n",
            "  [Batch 2820/6245] Loss: 0.3482\n",
            "  [Batch 2830/6245] Loss: 0.3792\n",
            "  [Batch 2840/6245] Loss: 0.3201\n",
            "  [Batch 2850/6245] Loss: 0.2635\n",
            "  [Batch 2860/6245] Loss: 0.2991\n",
            "  [Batch 2870/6245] Loss: 0.2233\n",
            "  [Batch 2880/6245] Loss: 0.3386\n",
            "  [Batch 2890/6245] Loss: 0.3858\n",
            "  [Batch 2900/6245] Loss: 0.2632\n",
            "  [Batch 2910/6245] Loss: 0.4004\n",
            "  [Batch 2920/6245] Loss: 0.2529\n",
            "  [Batch 2930/6245] Loss: 0.4935\n",
            "  [Batch 2940/6245] Loss: 0.5013\n",
            "  [Batch 2950/6245] Loss: 0.3813\n",
            "  [Batch 2960/6245] Loss: 0.2975\n",
            "  [Batch 2970/6245] Loss: 0.2692\n",
            "  [Batch 2980/6245] Loss: 0.2707\n",
            "  [Batch 2990/6245] Loss: 0.3661\n",
            "  [Batch 3000/6245] Loss: 0.4022\n",
            "  [Batch 3010/6245] Loss: 0.4212\n",
            "  [Batch 3020/6245] Loss: 0.2506\n",
            "  [Batch 3030/6245] Loss: 0.4752\n",
            "  [Batch 3040/6245] Loss: 0.4121\n",
            "  [Batch 3050/6245] Loss: 0.1648\n",
            "  [Batch 3060/6245] Loss: 0.3856\n",
            "  [Batch 3070/6245] Loss: 0.3028\n",
            "  [Batch 3080/6245] Loss: 0.3007\n",
            "  [Batch 3090/6245] Loss: 0.1952\n",
            "  [Batch 3100/6245] Loss: 0.3972\n",
            "  [Batch 3110/6245] Loss: 0.4058\n",
            "  [Batch 3120/6245] Loss: 0.3633\n",
            "  [Batch 3130/6245] Loss: 0.4972\n",
            "  [Batch 3140/6245] Loss: 0.3305\n",
            "  [Batch 3150/6245] Loss: 0.2857\n",
            "  [Batch 3160/6245] Loss: 0.1785\n",
            "  [Batch 3170/6245] Loss: 0.3194\n",
            "  [Batch 3180/6245] Loss: 0.4643\n",
            "  [Batch 3190/6245] Loss: 0.3560\n",
            "  [Batch 3200/6245] Loss: 0.3710\n",
            "  [Batch 3210/6245] Loss: 0.6458\n",
            "  [Batch 3220/6245] Loss: 0.2946\n",
            "  [Batch 3230/6245] Loss: 0.2613\n",
            "  [Batch 3240/6245] Loss: 0.4553\n",
            "  [Batch 3250/6245] Loss: 0.2431\n",
            "  [Batch 3260/6245] Loss: 0.2345\n",
            "  [Batch 3270/6245] Loss: 0.6507\n",
            "  [Batch 3280/6245] Loss: 0.3143\n",
            "  [Batch 3290/6245] Loss: 0.2221\n",
            "  [Batch 3300/6245] Loss: 0.3534\n",
            "  [Batch 3310/6245] Loss: 0.2646\n",
            "  [Batch 3320/6245] Loss: 0.3904\n",
            "  [Batch 3330/6245] Loss: 0.3720\n",
            "  [Batch 3340/6245] Loss: 0.2579\n",
            "  [Batch 3350/6245] Loss: 0.4660\n",
            "  [Batch 3360/6245] Loss: 0.3885\n",
            "  [Batch 3370/6245] Loss: 0.3194\n",
            "  [Batch 3380/6245] Loss: 0.3242\n",
            "  [Batch 3390/6245] Loss: 0.3747\n",
            "  [Batch 3400/6245] Loss: 0.2777\n",
            "  [Batch 3410/6245] Loss: 0.4853\n",
            "  [Batch 3420/6245] Loss: 0.1816\n",
            "  [Batch 3430/6245] Loss: 0.5550\n",
            "  [Batch 3440/6245] Loss: 0.2092\n",
            "  [Batch 3450/6245] Loss: 0.3032\n",
            "  [Batch 3460/6245] Loss: 0.2039\n",
            "  [Batch 3470/6245] Loss: 0.2245\n",
            "  [Batch 3480/6245] Loss: 0.4950\n",
            "  [Batch 3490/6245] Loss: 0.3745\n",
            "  [Batch 3500/6245] Loss: 0.2073\n",
            "  [Batch 3510/6245] Loss: 0.6051\n",
            "  [Batch 3520/6245] Loss: 0.1952\n",
            "  [Batch 3530/6245] Loss: 0.4694\n",
            "  [Batch 3540/6245] Loss: 0.3265\n",
            "  [Batch 3550/6245] Loss: 0.2219\n",
            "  [Batch 3560/6245] Loss: 0.2434\n",
            "  [Batch 3570/6245] Loss: 0.1605\n",
            "  [Batch 3580/6245] Loss: 0.1264\n",
            "  [Batch 3590/6245] Loss: 0.3250\n",
            "  [Batch 3600/6245] Loss: 0.2391\n",
            "  [Batch 3610/6245] Loss: 0.4389\n",
            "  [Batch 3620/6245] Loss: 0.3015\n",
            "  [Batch 3630/6245] Loss: 0.2429\n",
            "  [Batch 3640/6245] Loss: 0.2784\n",
            "  [Batch 3650/6245] Loss: 0.3249\n",
            "  [Batch 3660/6245] Loss: 0.3756\n",
            "  [Batch 3670/6245] Loss: 0.2317\n",
            "  [Batch 3680/6245] Loss: 0.1635\n",
            "  [Batch 3690/6245] Loss: 0.4415\n",
            "  [Batch 3700/6245] Loss: 0.5336\n",
            "  [Batch 3710/6245] Loss: 0.3107\n",
            "  [Batch 3720/6245] Loss: 0.2925\n",
            "  [Batch 3730/6245] Loss: 0.5092\n",
            "  [Batch 3740/6245] Loss: 0.3762\n",
            "  [Batch 3750/6245] Loss: 0.3420\n",
            "  [Batch 3760/6245] Loss: 0.4373\n",
            "  [Batch 3770/6245] Loss: 0.4690\n",
            "  [Batch 3780/6245] Loss: 0.3683\n",
            "  [Batch 3790/6245] Loss: 0.1356\n",
            "  [Batch 3800/6245] Loss: 0.3415\n",
            "  [Batch 3810/6245] Loss: 0.3951\n",
            "  [Batch 3820/6245] Loss: 0.3257\n",
            "  [Batch 3830/6245] Loss: 0.2966\n",
            "  [Batch 3840/6245] Loss: 0.4569\n",
            "  [Batch 3850/6245] Loss: 0.3045\n",
            "  [Batch 3860/6245] Loss: 0.4617\n",
            "  [Batch 3870/6245] Loss: 0.4127\n",
            "  [Batch 3880/6245] Loss: 0.5817\n",
            "  [Batch 3890/6245] Loss: 0.2631\n",
            "  [Batch 3900/6245] Loss: 0.4329\n",
            "  [Batch 3910/6245] Loss: 0.2256\n",
            "  [Batch 3920/6245] Loss: 0.3081\n",
            "  [Batch 3930/6245] Loss: 0.3976\n",
            "  [Batch 3940/6245] Loss: 0.3536\n",
            "  [Batch 3950/6245] Loss: 0.2861\n",
            "  [Batch 3960/6245] Loss: 0.1847\n",
            "  [Batch 3970/6245] Loss: 0.3993\n",
            "  [Batch 3980/6245] Loss: 0.3640\n",
            "  [Batch 3990/6245] Loss: 0.3954\n",
            "  [Batch 4000/6245] Loss: 0.2320\n",
            "  [Batch 4010/6245] Loss: 0.5360\n",
            "  [Batch 4020/6245] Loss: 0.4728\n",
            "  [Batch 4030/6245] Loss: 0.4039\n",
            "  [Batch 4040/6245] Loss: 0.2556\n",
            "  [Batch 4050/6245] Loss: 0.2201\n",
            "  [Batch 4060/6245] Loss: 0.2930\n",
            "  [Batch 4070/6245] Loss: 0.2386\n",
            "  [Batch 4080/6245] Loss: 0.2498\n",
            "  [Batch 4090/6245] Loss: 0.2710\n",
            "  [Batch 4100/6245] Loss: 0.2184\n",
            "  [Batch 4110/6245] Loss: 0.2998\n",
            "  [Batch 4120/6245] Loss: 0.3317\n",
            "  [Batch 4130/6245] Loss: 0.2873\n",
            "  [Batch 4140/6245] Loss: 0.1847\n",
            "  [Batch 4150/6245] Loss: 0.3462\n",
            "  [Batch 4160/6245] Loss: 0.2610\n",
            "  [Batch 4170/6245] Loss: 0.5229\n",
            "  [Batch 4180/6245] Loss: 0.2686\n",
            "  [Batch 4190/6245] Loss: 0.2145\n",
            "  [Batch 4200/6245] Loss: 0.1562\n",
            "  [Batch 4210/6245] Loss: 0.3469\n",
            "  [Batch 4220/6245] Loss: 0.4277\n",
            "  [Batch 4230/6245] Loss: 0.3199\n",
            "  [Batch 4240/6245] Loss: 0.4889\n",
            "  [Batch 4250/6245] Loss: 0.1526\n",
            "  [Batch 4260/6245] Loss: 0.3280\n",
            "  [Batch 4270/6245] Loss: 0.1965\n",
            "  [Batch 4280/6245] Loss: 0.6165\n",
            "  [Batch 4290/6245] Loss: 0.2885\n",
            "  [Batch 4300/6245] Loss: 0.2536\n",
            "  [Batch 4310/6245] Loss: 0.3084\n",
            "  [Batch 4320/6245] Loss: 0.2906\n",
            "  [Batch 4330/6245] Loss: 0.5629\n",
            "  [Batch 4340/6245] Loss: 0.4079\n",
            "  [Batch 4350/6245] Loss: 0.1383\n",
            "  [Batch 4360/6245] Loss: 0.2872\n",
            "  [Batch 4370/6245] Loss: 0.2825\n",
            "  [Batch 4380/6245] Loss: 0.3593\n",
            "  [Batch 4390/6245] Loss: 0.3125\n",
            "  [Batch 4400/6245] Loss: 0.1674\n",
            "  [Batch 4410/6245] Loss: 0.3570\n",
            "  [Batch 4420/6245] Loss: 0.2376\n",
            "  [Batch 4430/6245] Loss: 0.3738\n",
            "  [Batch 4440/6245] Loss: 0.2575\n",
            "  [Batch 4450/6245] Loss: 0.3454\n",
            "  [Batch 4460/6245] Loss: 0.4695\n",
            "  [Batch 4470/6245] Loss: 0.2033\n",
            "  [Batch 4480/6245] Loss: 0.2840\n",
            "  [Batch 4490/6245] Loss: 0.2151\n",
            "  [Batch 4500/6245] Loss: 0.2497\n",
            "  [Batch 4510/6245] Loss: 0.3302\n",
            "  [Batch 4520/6245] Loss: 0.2041\n",
            "  [Batch 4530/6245] Loss: 0.3378\n",
            "  [Batch 4540/6245] Loss: 0.3127\n",
            "  [Batch 4550/6245] Loss: 0.2169\n",
            "  [Batch 4560/6245] Loss: 0.3910\n",
            "  [Batch 4570/6245] Loss: 0.2625\n",
            "  [Batch 4580/6245] Loss: 0.4461\n",
            "  [Batch 4590/6245] Loss: 0.4364\n",
            "  [Batch 4600/6245] Loss: 0.2729\n",
            "  [Batch 4610/6245] Loss: 0.4221\n",
            "  [Batch 4620/6245] Loss: 0.2985\n",
            "  [Batch 4630/6245] Loss: 0.2322\n",
            "  [Batch 4640/6245] Loss: 0.2807\n",
            "  [Batch 4650/6245] Loss: 0.1933\n",
            "  [Batch 4660/6245] Loss: 0.2923\n",
            "  [Batch 4670/6245] Loss: 0.1860\n",
            "  [Batch 4680/6245] Loss: 0.3038\n",
            "  [Batch 4690/6245] Loss: 0.3267\n",
            "  [Batch 4700/6245] Loss: 0.3402\n",
            "  [Batch 4710/6245] Loss: 0.2683\n",
            "  [Batch 4720/6245] Loss: 0.2436\n",
            "  [Batch 4730/6245] Loss: 0.4693\n",
            "  [Batch 4740/6245] Loss: 0.3613\n",
            "  [Batch 4750/6245] Loss: 0.2544\n",
            "  [Batch 4760/6245] Loss: 0.5059\n",
            "  [Batch 4770/6245] Loss: 0.5691\n",
            "  [Batch 4780/6245] Loss: 0.3155\n",
            "  [Batch 4790/6245] Loss: 0.3337\n",
            "  [Batch 4800/6245] Loss: 0.1706\n",
            "  [Batch 4810/6245] Loss: 0.4288\n",
            "  [Batch 4820/6245] Loss: 0.3021\n",
            "  [Batch 4830/6245] Loss: 0.3468\n",
            "  [Batch 4840/6245] Loss: 0.2152\n",
            "  [Batch 4850/6245] Loss: 0.2787\n",
            "  [Batch 4860/6245] Loss: 0.3277\n",
            "  [Batch 4870/6245] Loss: 0.2916\n",
            "  [Batch 4880/6245] Loss: 0.4907\n",
            "  [Batch 4890/6245] Loss: 0.2741\n",
            "  [Batch 4900/6245] Loss: 0.4755\n",
            "  [Batch 4910/6245] Loss: 0.3588\n",
            "  [Batch 4920/6245] Loss: 0.3652\n",
            "  [Batch 4930/6245] Loss: 0.2826\n",
            "  [Batch 4940/6245] Loss: 0.3463\n",
            "  [Batch 4950/6245] Loss: 0.2074\n",
            "  [Batch 4960/6245] Loss: 0.1916\n",
            "  [Batch 4970/6245] Loss: 0.4230\n",
            "  [Batch 4980/6245] Loss: 0.2181\n",
            "  [Batch 4990/6245] Loss: 0.2068\n",
            "  [Batch 5000/6245] Loss: 0.1857\n",
            "  [Batch 5010/6245] Loss: 0.4689\n",
            "  [Batch 5020/6245] Loss: 0.1673\n",
            "  [Batch 5030/6245] Loss: 0.3691\n",
            "  [Batch 5040/6245] Loss: 0.1808\n",
            "  [Batch 5050/6245] Loss: 0.1286\n",
            "  [Batch 5060/6245] Loss: 0.4507\n",
            "  [Batch 5070/6245] Loss: 0.3717\n",
            "  [Batch 5080/6245] Loss: 0.2207\n",
            "  [Batch 5090/6245] Loss: 0.2661\n",
            "  [Batch 5100/6245] Loss: 0.3159\n",
            "  [Batch 5110/6245] Loss: 0.2476\n",
            "  [Batch 5120/6245] Loss: 0.4183\n",
            "  [Batch 5130/6245] Loss: 0.5290\n",
            "  [Batch 5140/6245] Loss: 0.4547\n",
            "  [Batch 5150/6245] Loss: 0.2747\n",
            "  [Batch 5160/6245] Loss: 0.1846\n",
            "  [Batch 5170/6245] Loss: 0.2794\n",
            "  [Batch 5180/6245] Loss: 0.3296\n",
            "  [Batch 5190/6245] Loss: 0.2863\n",
            "  [Batch 5200/6245] Loss: 0.1833\n",
            "  [Batch 5210/6245] Loss: 0.3507\n",
            "  [Batch 5220/6245] Loss: 0.3545\n",
            "  [Batch 5230/6245] Loss: 0.5202\n",
            "  [Batch 5240/6245] Loss: 0.3591\n",
            "  [Batch 5250/6245] Loss: 0.5121\n",
            "  [Batch 5260/6245] Loss: 0.1377\n",
            "  [Batch 5270/6245] Loss: 0.2659\n",
            "  [Batch 5280/6245] Loss: 0.2816\n",
            "  [Batch 5290/6245] Loss: 0.5110\n",
            "  [Batch 5300/6245] Loss: 0.3887\n",
            "  [Batch 5310/6245] Loss: 0.2980\n",
            "  [Batch 5320/6245] Loss: 0.3245\n",
            "  [Batch 5330/6245] Loss: 0.2393\n",
            "  [Batch 5340/6245] Loss: 0.6003\n",
            "  [Batch 5350/6245] Loss: 0.4383\n",
            "  [Batch 5360/6245] Loss: 0.3606\n",
            "  [Batch 5370/6245] Loss: 0.4368\n",
            "  [Batch 5380/6245] Loss: 0.1371\n",
            "  [Batch 5390/6245] Loss: 0.2710\n",
            "  [Batch 5400/6245] Loss: 0.4702\n",
            "  [Batch 5410/6245] Loss: 0.1864\n",
            "  [Batch 5420/6245] Loss: 0.4618\n",
            "  [Batch 5430/6245] Loss: 0.3729\n",
            "  [Batch 5440/6245] Loss: 0.1736\n",
            "  [Batch 5450/6245] Loss: 0.5104\n",
            "  [Batch 5460/6245] Loss: 0.2733\n",
            "  [Batch 5470/6245] Loss: 0.2356\n",
            "  [Batch 5480/6245] Loss: 0.2561\n",
            "  [Batch 5490/6245] Loss: 0.4074\n",
            "  [Batch 5500/6245] Loss: 0.2466\n",
            "  [Batch 5510/6245] Loss: 0.2933\n",
            "  [Batch 5520/6245] Loss: 0.4242\n",
            "  [Batch 5530/6245] Loss: 0.1777\n",
            "  [Batch 5540/6245] Loss: 0.2801\n",
            "  [Batch 5550/6245] Loss: 0.4633\n",
            "  [Batch 5560/6245] Loss: 0.2137\n",
            "  [Batch 5570/6245] Loss: 0.2108\n",
            "  [Batch 5580/6245] Loss: 0.1983\n",
            "  [Batch 5590/6245] Loss: 0.4216\n",
            "  [Batch 5600/6245] Loss: 0.2094\n",
            "  [Batch 5610/6245] Loss: 0.1642\n",
            "  [Batch 5620/6245] Loss: 0.3723\n",
            "  [Batch 5630/6245] Loss: 0.5402\n",
            "  [Batch 5640/6245] Loss: 0.2597\n",
            "  [Batch 5650/6245] Loss: 0.2996\n",
            "  [Batch 5660/6245] Loss: 0.3445\n",
            "  [Batch 5670/6245] Loss: 0.3085\n",
            "  [Batch 5680/6245] Loss: 0.2997\n",
            "  [Batch 5690/6245] Loss: 0.2915\n",
            "  [Batch 5700/6245] Loss: 0.3796\n",
            "  [Batch 5710/6245] Loss: 0.2420\n",
            "  [Batch 5720/6245] Loss: 0.3197\n",
            "  [Batch 5730/6245] Loss: 0.2698\n",
            "  [Batch 5740/6245] Loss: 0.4261\n",
            "  [Batch 5750/6245] Loss: 0.2895\n",
            "  [Batch 5760/6245] Loss: 0.4210\n",
            "  [Batch 5770/6245] Loss: 0.2559\n",
            "  [Batch 5780/6245] Loss: 0.2086\n",
            "  [Batch 5790/6245] Loss: 0.4274\n",
            "  [Batch 5800/6245] Loss: 0.3259\n",
            "  [Batch 5810/6245] Loss: 0.2735\n",
            "  [Batch 5820/6245] Loss: 0.2478\n",
            "  [Batch 5830/6245] Loss: 0.1531\n",
            "  [Batch 5840/6245] Loss: 0.4628\n",
            "  [Batch 5850/6245] Loss: 0.1492\n",
            "  [Batch 5860/6245] Loss: 0.2788\n",
            "  [Batch 5870/6245] Loss: 0.3091\n",
            "  [Batch 5880/6245] Loss: 0.4202\n",
            "  [Batch 5890/6245] Loss: 0.1987\n",
            "  [Batch 5900/6245] Loss: 0.5751\n",
            "  [Batch 5910/6245] Loss: 0.5292\n",
            "  [Batch 5920/6245] Loss: 0.3423\n",
            "  [Batch 5930/6245] Loss: 0.5479\n",
            "  [Batch 5940/6245] Loss: 0.3014\n",
            "  [Batch 5950/6245] Loss: 0.4169\n",
            "  [Batch 5960/6245] Loss: 0.2689\n",
            "  [Batch 5970/6245] Loss: 0.3695\n",
            "  [Batch 5980/6245] Loss: 0.3694\n",
            "  [Batch 5990/6245] Loss: 0.2116\n",
            "  [Batch 6000/6245] Loss: 0.3199\n",
            "  [Batch 6010/6245] Loss: 0.3348\n",
            "  [Batch 6020/6245] Loss: 0.3822\n",
            "  [Batch 6030/6245] Loss: 0.2884\n",
            "  [Batch 6040/6245] Loss: 0.2305\n",
            "  [Batch 6050/6245] Loss: 0.5371\n",
            "  [Batch 6060/6245] Loss: 0.2650\n",
            "  [Batch 6070/6245] Loss: 0.3096\n",
            "  [Batch 6080/6245] Loss: 0.3398\n",
            "  [Batch 6090/6245] Loss: 0.3842\n",
            "  [Batch 6100/6245] Loss: 0.2074\n",
            "  [Batch 6110/6245] Loss: 0.2867\n",
            "  [Batch 6120/6245] Loss: 0.2481\n",
            "  [Batch 6130/6245] Loss: 0.3437\n",
            "  [Batch 6140/6245] Loss: 0.4167\n",
            "  [Batch 6150/6245] Loss: 0.2788\n",
            "  [Batch 6160/6245] Loss: 0.3313\n",
            "  [Batch 6170/6245] Loss: 0.2486\n",
            "  [Batch 6180/6245] Loss: 0.4898\n",
            "  [Batch 6190/6245] Loss: 0.2315\n",
            "  [Batch 6200/6245] Loss: 0.2909\n",
            "  [Batch 6210/6245] Loss: 0.4417\n",
            "  [Batch 6220/6245] Loss: 0.4321\n",
            "  [Batch 6230/6245] Loss: 0.3059\n",
            "  [Batch 6240/6245] Loss: 0.3171\n",
            "  [Batch 6245/6245] Loss: 0.3655\n",
            "[Epoch 5] Train Loss: 0.3219, Acc: 86.16% | Val Loss: 0.3112, Acc: 86.59%\n",
            "[INFO] Model checkpoint saved at ./models\\breastnet_epoch5.pth\n",
            "\n",
            "[Epoch 6/10]\n",
            "  [Batch 10/6245] Loss: 0.3794\n",
            "  [Batch 20/6245] Loss: 0.1737\n",
            "  [Batch 30/6245] Loss: 0.1802\n",
            "  [Batch 40/6245] Loss: 0.2887\n",
            "  [Batch 50/6245] Loss: 0.3142\n",
            "  [Batch 60/6245] Loss: 0.2077\n",
            "  [Batch 70/6245] Loss: 0.3054\n",
            "  [Batch 80/6245] Loss: 0.2583\n",
            "  [Batch 90/6245] Loss: 0.4818\n",
            "  [Batch 100/6245] Loss: 0.3477\n",
            "  [Batch 110/6245] Loss: 0.3002\n",
            "  [Batch 120/6245] Loss: 0.1947\n",
            "  [Batch 130/6245] Loss: 0.3114\n",
            "  [Batch 140/6245] Loss: 0.2297\n",
            "  [Batch 150/6245] Loss: 0.3317\n",
            "  [Batch 160/6245] Loss: 0.2210\n",
            "  [Batch 170/6245] Loss: 0.4757\n",
            "  [Batch 180/6245] Loss: 0.1855\n",
            "  [Batch 190/6245] Loss: 0.4503\n",
            "  [Batch 200/6245] Loss: 0.2375\n",
            "  [Batch 210/6245] Loss: 0.3280\n",
            "  [Batch 220/6245] Loss: 0.2934\n",
            "  [Batch 230/6245] Loss: 0.2330\n",
            "  [Batch 240/6245] Loss: 0.3394\n",
            "  [Batch 250/6245] Loss: 0.4147\n",
            "  [Batch 260/6245] Loss: 0.2510\n",
            "  [Batch 270/6245] Loss: 0.3516\n",
            "  [Batch 280/6245] Loss: 0.2411\n",
            "  [Batch 290/6245] Loss: 0.2626\n",
            "  [Batch 300/6245] Loss: 0.1178\n",
            "  [Batch 310/6245] Loss: 0.4060\n",
            "  [Batch 320/6245] Loss: 0.3463\n",
            "  [Batch 330/6245] Loss: 0.4538\n",
            "  [Batch 340/6245] Loss: 0.2814\n",
            "  [Batch 350/6245] Loss: 0.2175\n",
            "  [Batch 360/6245] Loss: 0.1732\n",
            "  [Batch 370/6245] Loss: 0.7393\n",
            "  [Batch 380/6245] Loss: 0.2411\n",
            "  [Batch 390/6245] Loss: 0.3542\n",
            "  [Batch 400/6245] Loss: 0.3327\n",
            "  [Batch 410/6245] Loss: 0.3608\n",
            "  [Batch 420/6245] Loss: 0.2964\n",
            "  [Batch 430/6245] Loss: 0.2149\n",
            "  [Batch 440/6245] Loss: 0.1938\n",
            "  [Batch 450/6245] Loss: 0.1547\n",
            "  [Batch 460/6245] Loss: 0.2492\n",
            "  [Batch 470/6245] Loss: 0.3190\n",
            "  [Batch 480/6245] Loss: 0.2825\n",
            "  [Batch 490/6245] Loss: 0.2946\n",
            "  [Batch 500/6245] Loss: 0.4986\n",
            "  [Batch 510/6245] Loss: 0.3142\n",
            "  [Batch 520/6245] Loss: 0.2468\n",
            "  [Batch 530/6245] Loss: 0.2785\n",
            "  [Batch 540/6245] Loss: 0.3600\n",
            "  [Batch 550/6245] Loss: 0.2501\n",
            "  [Batch 560/6245] Loss: 0.4176\n",
            "  [Batch 570/6245] Loss: 0.3391\n",
            "  [Batch 580/6245] Loss: 0.2506\n",
            "  [Batch 590/6245] Loss: 0.1758\n",
            "  [Batch 600/6245] Loss: 0.2190\n",
            "  [Batch 610/6245] Loss: 0.1986\n",
            "  [Batch 620/6245] Loss: 0.1896\n",
            "  [Batch 630/6245] Loss: 0.2359\n",
            "  [Batch 640/6245] Loss: 0.5123\n",
            "  [Batch 650/6245] Loss: 0.2119\n",
            "  [Batch 660/6245] Loss: 0.3467\n",
            "  [Batch 670/6245] Loss: 0.1969\n",
            "  [Batch 680/6245] Loss: 0.3127\n",
            "  [Batch 690/6245] Loss: 0.3362\n",
            "  [Batch 700/6245] Loss: 0.3299\n",
            "  [Batch 710/6245] Loss: 0.2399\n",
            "  [Batch 720/6245] Loss: 0.3442\n",
            "  [Batch 730/6245] Loss: 0.3121\n",
            "  [Batch 740/6245] Loss: 0.5102\n",
            "  [Batch 750/6245] Loss: 0.5267\n",
            "  [Batch 760/6245] Loss: 0.4806\n",
            "  [Batch 770/6245] Loss: 0.3699\n",
            "  [Batch 780/6245] Loss: 0.2089\n",
            "  [Batch 790/6245] Loss: 0.1624\n",
            "  [Batch 800/6245] Loss: 0.0817\n",
            "  [Batch 810/6245] Loss: 0.2278\n",
            "  [Batch 820/6245] Loss: 0.5340\n",
            "  [Batch 830/6245] Loss: 0.3943\n",
            "  [Batch 840/6245] Loss: 0.3722\n",
            "  [Batch 850/6245] Loss: 0.2698\n",
            "  [Batch 860/6245] Loss: 0.7595\n",
            "  [Batch 870/6245] Loss: 0.3088\n",
            "  [Batch 880/6245] Loss: 0.2718\n",
            "  [Batch 890/6245] Loss: 0.3887\n",
            "  [Batch 900/6245] Loss: 0.2309\n",
            "  [Batch 910/6245] Loss: 0.2578\n",
            "  [Batch 920/6245] Loss: 0.5794\n",
            "  [Batch 930/6245] Loss: 0.1829\n",
            "  [Batch 940/6245] Loss: 0.4492\n",
            "  [Batch 950/6245] Loss: 0.5513\n",
            "  [Batch 960/6245] Loss: 0.3421\n",
            "  [Batch 970/6245] Loss: 0.2210\n",
            "  [Batch 980/6245] Loss: 0.2800\n",
            "  [Batch 990/6245] Loss: 0.4163\n",
            "  [Batch 1000/6245] Loss: 0.1932\n",
            "  [Batch 1010/6245] Loss: 0.3217\n",
            "  [Batch 1020/6245] Loss: 0.2709\n",
            "  [Batch 1030/6245] Loss: 0.3666\n",
            "  [Batch 1040/6245] Loss: 0.2745\n",
            "  [Batch 1050/6245] Loss: 0.1989\n",
            "  [Batch 1060/6245] Loss: 0.3540\n",
            "  [Batch 1070/6245] Loss: 0.3484\n",
            "  [Batch 1080/6245] Loss: 0.2909\n",
            "  [Batch 1090/6245] Loss: 0.3435\n",
            "  [Batch 1100/6245] Loss: 0.3638\n",
            "  [Batch 1110/6245] Loss: 0.2115\n",
            "  [Batch 1120/6245] Loss: 0.2422\n",
            "  [Batch 1130/6245] Loss: 0.3011\n",
            "  [Batch 1140/6245] Loss: 0.3260\n",
            "  [Batch 1150/6245] Loss: 0.4171\n",
            "  [Batch 1160/6245] Loss: 0.4770\n",
            "  [Batch 1170/6245] Loss: 0.3202\n",
            "  [Batch 1180/6245] Loss: 0.4133\n",
            "  [Batch 1190/6245] Loss: 0.2002\n",
            "  [Batch 1200/6245] Loss: 0.2374\n",
            "  [Batch 1210/6245] Loss: 0.2873\n",
            "  [Batch 1220/6245] Loss: 0.3154\n",
            "  [Batch 1230/6245] Loss: 0.3891\n",
            "  [Batch 1240/6245] Loss: 0.2056\n",
            "  [Batch 1250/6245] Loss: 0.4239\n",
            "  [Batch 1260/6245] Loss: 0.3427\n",
            "  [Batch 1270/6245] Loss: 0.1987\n",
            "  [Batch 1280/6245] Loss: 0.2865\n",
            "  [Batch 1290/6245] Loss: 0.3262\n",
            "  [Batch 1300/6245] Loss: 0.3838\n",
            "  [Batch 1310/6245] Loss: 0.4925\n",
            "  [Batch 1320/6245] Loss: 0.3278\n",
            "  [Batch 1330/6245] Loss: 0.3889\n",
            "  [Batch 1340/6245] Loss: 0.2341\n",
            "  [Batch 1350/6245] Loss: 0.3624\n",
            "  [Batch 1360/6245] Loss: 0.3244\n",
            "  [Batch 1370/6245] Loss: 0.1671\n",
            "  [Batch 1380/6245] Loss: 0.5015\n",
            "  [Batch 1390/6245] Loss: 0.3041\n",
            "  [Batch 1400/6245] Loss: 0.3218\n",
            "  [Batch 1410/6245] Loss: 0.2740\n",
            "  [Batch 1420/6245] Loss: 0.3562\n",
            "  [Batch 1430/6245] Loss: 0.1604\n",
            "  [Batch 1440/6245] Loss: 0.2492\n",
            "  [Batch 1450/6245] Loss: 0.3914\n",
            "  [Batch 1460/6245] Loss: 0.3421\n",
            "  [Batch 1470/6245] Loss: 0.2852\n",
            "  [Batch 1480/6245] Loss: 0.3789\n",
            "  [Batch 1490/6245] Loss: 0.5060\n",
            "  [Batch 1500/6245] Loss: 0.4204\n",
            "  [Batch 1510/6245] Loss: 0.2569\n",
            "  [Batch 1520/6245] Loss: 0.3915\n",
            "  [Batch 1530/6245] Loss: 0.2185\n",
            "  [Batch 1540/6245] Loss: 0.2855\n",
            "  [Batch 1550/6245] Loss: 0.3292\n",
            "  [Batch 1560/6245] Loss: 0.1145\n",
            "  [Batch 1570/6245] Loss: 0.3430\n",
            "  [Batch 1580/6245] Loss: 0.4517\n",
            "  [Batch 1590/6245] Loss: 0.4265\n",
            "  [Batch 1600/6245] Loss: 0.2552\n",
            "  [Batch 1610/6245] Loss: 0.2578\n",
            "  [Batch 1620/6245] Loss: 0.2654\n",
            "  [Batch 1630/6245] Loss: 0.2745\n",
            "  [Batch 1640/6245] Loss: 0.4319\n",
            "  [Batch 1650/6245] Loss: 0.2811\n",
            "  [Batch 1660/6245] Loss: 0.3327\n",
            "  [Batch 1670/6245] Loss: 0.2046\n",
            "  [Batch 1680/6245] Loss: 0.2154\n",
            "  [Batch 1690/6245] Loss: 0.4098\n",
            "  [Batch 1700/6245] Loss: 0.3063\n",
            "  [Batch 1710/6245] Loss: 0.2470\n",
            "  [Batch 1720/6245] Loss: 0.1984\n",
            "  [Batch 1730/6245] Loss: 0.3611\n",
            "  [Batch 1740/6245] Loss: 0.3355\n",
            "  [Batch 1750/6245] Loss: 0.5128\n",
            "  [Batch 1760/6245] Loss: 0.3428\n",
            "  [Batch 1770/6245] Loss: 0.4059\n",
            "  [Batch 1780/6245] Loss: 0.2079\n",
            "  [Batch 1790/6245] Loss: 0.2724\n",
            "  [Batch 1800/6245] Loss: 0.3997\n",
            "  [Batch 1810/6245] Loss: 0.4677\n",
            "  [Batch 1820/6245] Loss: 0.3459\n",
            "  [Batch 1830/6245] Loss: 0.3587\n",
            "  [Batch 1840/6245] Loss: 0.2888\n",
            "  [Batch 1850/6245] Loss: 0.2833\n",
            "  [Batch 1860/6245] Loss: 0.3733\n",
            "  [Batch 1870/6245] Loss: 0.2836\n",
            "  [Batch 1880/6245] Loss: 0.5276\n",
            "  [Batch 1890/6245] Loss: 0.2580\n",
            "  [Batch 1900/6245] Loss: 0.3773\n",
            "  [Batch 1910/6245] Loss: 0.1481\n",
            "  [Batch 1920/6245] Loss: 0.2057\n",
            "  [Batch 1930/6245] Loss: 0.2725\n",
            "  [Batch 1940/6245] Loss: 0.2496\n",
            "  [Batch 1950/6245] Loss: 0.4062\n",
            "  [Batch 1960/6245] Loss: 0.2424\n",
            "  [Batch 1970/6245] Loss: 0.2812\n",
            "  [Batch 1980/6245] Loss: 0.2771\n",
            "  [Batch 1990/6245] Loss: 0.2170\n",
            "  [Batch 2000/6245] Loss: 0.4432\n",
            "  [Batch 2010/6245] Loss: 0.3007\n",
            "  [Batch 2020/6245] Loss: 0.2128\n",
            "  [Batch 2030/6245] Loss: 0.2785\n",
            "  [Batch 2040/6245] Loss: 0.2887\n",
            "  [Batch 2050/6245] Loss: 0.2235\n",
            "  [Batch 2060/6245] Loss: 0.3152\n",
            "  [Batch 2070/6245] Loss: 0.2594\n",
            "  [Batch 2080/6245] Loss: 0.3365\n",
            "  [Batch 2090/6245] Loss: 0.1997\n",
            "  [Batch 2100/6245] Loss: 0.2735\n",
            "  [Batch 2110/6245] Loss: 0.3266\n",
            "  [Batch 2120/6245] Loss: 0.2894\n",
            "  [Batch 2130/6245] Loss: 0.2934\n",
            "  [Batch 2140/6245] Loss: 0.2656\n",
            "  [Batch 2150/6245] Loss: 0.2254\n",
            "  [Batch 2160/6245] Loss: 0.2123\n",
            "  [Batch 2170/6245] Loss: 0.2345\n",
            "  [Batch 2180/6245] Loss: 0.3733\n",
            "  [Batch 2190/6245] Loss: 0.2294\n",
            "  [Batch 2200/6245] Loss: 0.4299\n",
            "  [Batch 2210/6245] Loss: 0.2608\n",
            "  [Batch 2220/6245] Loss: 0.3119\n",
            "  [Batch 2230/6245] Loss: 0.2717\n",
            "  [Batch 2240/6245] Loss: 0.2599\n",
            "  [Batch 2250/6245] Loss: 0.3337\n",
            "  [Batch 2260/6245] Loss: 0.3497\n",
            "  [Batch 2270/6245] Loss: 0.2845\n",
            "  [Batch 2280/6245] Loss: 0.2400\n",
            "  [Batch 2290/6245] Loss: 0.4344\n",
            "  [Batch 2300/6245] Loss: 0.4008\n",
            "  [Batch 2310/6245] Loss: 0.3909\n",
            "  [Batch 2320/6245] Loss: 0.1751\n",
            "  [Batch 2330/6245] Loss: 0.3252\n",
            "  [Batch 2340/6245] Loss: 0.3572\n",
            "  [Batch 2350/6245] Loss: 0.5476\n",
            "  [Batch 2360/6245] Loss: 0.4214\n",
            "  [Batch 2370/6245] Loss: 0.1637\n",
            "  [Batch 2380/6245] Loss: 0.4404\n",
            "  [Batch 2390/6245] Loss: 0.2566\n",
            "  [Batch 2400/6245] Loss: 0.3940\n",
            "  [Batch 2410/6245] Loss: 0.2661\n",
            "  [Batch 2420/6245] Loss: 0.3053\n",
            "  [Batch 2430/6245] Loss: 0.3272\n",
            "  [Batch 2440/6245] Loss: 0.2537\n",
            "  [Batch 2450/6245] Loss: 0.3713\n",
            "  [Batch 2460/6245] Loss: 0.2202\n",
            "  [Batch 2470/6245] Loss: 0.2977\n",
            "  [Batch 2480/6245] Loss: 0.3754\n",
            "  [Batch 2490/6245] Loss: 0.2445\n",
            "  [Batch 2500/6245] Loss: 0.3263\n",
            "  [Batch 2510/6245] Loss: 0.3371\n",
            "  [Batch 2520/6245] Loss: 0.3877\n",
            "  [Batch 2530/6245] Loss: 0.2464\n",
            "  [Batch 2540/6245] Loss: 0.1797\n",
            "  [Batch 2550/6245] Loss: 0.3312\n",
            "  [Batch 2560/6245] Loss: 0.3602\n",
            "  [Batch 2570/6245] Loss: 0.2294\n",
            "  [Batch 2580/6245] Loss: 0.1917\n",
            "  [Batch 2590/6245] Loss: 0.3809\n",
            "  [Batch 2600/6245] Loss: 0.5672\n",
            "  [Batch 2610/6245] Loss: 0.3557\n",
            "  [Batch 2620/6245] Loss: 0.1737\n",
            "  [Batch 2630/6245] Loss: 0.3849\n",
            "  [Batch 2640/6245] Loss: 0.2810\n",
            "  [Batch 2650/6245] Loss: 0.2038\n",
            "  [Batch 2660/6245] Loss: 0.3986\n",
            "  [Batch 2670/6245] Loss: 0.4534\n",
            "  [Batch 2680/6245] Loss: 0.3189\n",
            "  [Batch 2690/6245] Loss: 0.4167\n",
            "  [Batch 2700/6245] Loss: 0.3372\n",
            "  [Batch 2710/6245] Loss: 0.4247\n",
            "  [Batch 2720/6245] Loss: 0.5596\n",
            "  [Batch 2730/6245] Loss: 0.3939\n",
            "  [Batch 2740/6245] Loss: 0.3745\n",
            "  [Batch 2750/6245] Loss: 0.3307\n",
            "  [Batch 2760/6245] Loss: 0.3041\n",
            "  [Batch 2770/6245] Loss: 0.2327\n",
            "  [Batch 2780/6245] Loss: 0.4198\n",
            "  [Batch 2790/6245] Loss: 0.4258\n",
            "  [Batch 2800/6245] Loss: 0.3242\n",
            "  [Batch 2810/6245] Loss: 0.3859\n",
            "  [Batch 2820/6245] Loss: 0.2190\n",
            "  [Batch 2830/6245] Loss: 0.3041\n",
            "  [Batch 2840/6245] Loss: 0.2919\n",
            "  [Batch 2850/6245] Loss: 0.2729\n",
            "  [Batch 2860/6245] Loss: 0.3309\n",
            "  [Batch 2870/6245] Loss: 0.4025\n",
            "  [Batch 2880/6245] Loss: 0.3327\n",
            "  [Batch 2890/6245] Loss: 0.2857\n",
            "  [Batch 2900/6245] Loss: 0.5119\n",
            "  [Batch 2910/6245] Loss: 0.4418\n",
            "  [Batch 2920/6245] Loss: 0.2060\n",
            "  [Batch 2930/6245] Loss: 0.2958\n",
            "  [Batch 2940/6245] Loss: 0.2676\n",
            "  [Batch 2950/6245] Loss: 0.2679\n",
            "  [Batch 2960/6245] Loss: 0.2205\n",
            "  [Batch 2970/6245] Loss: 0.2634\n",
            "  [Batch 2980/6245] Loss: 0.4882\n",
            "  [Batch 2990/6245] Loss: 0.2306\n",
            "  [Batch 3000/6245] Loss: 0.2507\n",
            "  [Batch 3010/6245] Loss: 0.5071\n",
            "  [Batch 3020/6245] Loss: 0.2001\n",
            "  [Batch 3030/6245] Loss: 0.1915\n",
            "  [Batch 3040/6245] Loss: 0.2984\n",
            "  [Batch 3050/6245] Loss: 0.2431\n",
            "  [Batch 3060/6245] Loss: 0.2728\n",
            "  [Batch 3070/6245] Loss: 0.3313\n",
            "  [Batch 3080/6245] Loss: 0.4829\n",
            "  [Batch 3090/6245] Loss: 0.2853\n",
            "  [Batch 3100/6245] Loss: 0.2065\n",
            "  [Batch 3110/6245] Loss: 0.2571\n",
            "  [Batch 3120/6245] Loss: 0.1806\n",
            "  [Batch 3130/6245] Loss: 0.3321\n",
            "  [Batch 3140/6245] Loss: 0.4733\n",
            "  [Batch 3150/6245] Loss: 0.2410\n",
            "  [Batch 3160/6245] Loss: 0.3760\n",
            "  [Batch 3170/6245] Loss: 0.4804\n",
            "  [Batch 3180/6245] Loss: 0.3514\n",
            "  [Batch 3190/6245] Loss: 0.3568\n",
            "  [Batch 3200/6245] Loss: 0.2842\n",
            "  [Batch 3210/6245] Loss: 0.2284\n",
            "  [Batch 3220/6245] Loss: 0.2958\n",
            "  [Batch 3230/6245] Loss: 0.6051\n",
            "  [Batch 3240/6245] Loss: 0.2583\n",
            "  [Batch 3250/6245] Loss: 0.3176\n",
            "  [Batch 3260/6245] Loss: 0.2354\n",
            "  [Batch 3270/6245] Loss: 0.2292\n",
            "  [Batch 3280/6245] Loss: 0.2174\n",
            "  [Batch 3290/6245] Loss: 0.3300\n",
            "  [Batch 3300/6245] Loss: 0.3233\n",
            "  [Batch 3310/6245] Loss: 0.2446\n",
            "  [Batch 3320/6245] Loss: 0.3707\n",
            "  [Batch 3330/6245] Loss: 0.4768\n",
            "  [Batch 3340/6245] Loss: 0.3309\n",
            "  [Batch 3350/6245] Loss: 0.3403\n",
            "  [Batch 3360/6245] Loss: 0.2820\n",
            "  [Batch 3370/6245] Loss: 0.2522\n",
            "  [Batch 3380/6245] Loss: 0.4252\n",
            "  [Batch 3390/6245] Loss: 0.1342\n",
            "  [Batch 3400/6245] Loss: 0.2551\n",
            "  [Batch 3410/6245] Loss: 0.2210\n",
            "  [Batch 3420/6245] Loss: 0.3613\n",
            "  [Batch 3430/6245] Loss: 0.3434\n",
            "  [Batch 3440/6245] Loss: 0.2717\n",
            "  [Batch 3450/6245] Loss: 0.1394\n",
            "  [Batch 3460/6245] Loss: 0.2209\n",
            "  [Batch 3470/6245] Loss: 0.2601\n",
            "  [Batch 3480/6245] Loss: 0.5802\n",
            "  [Batch 3490/6245] Loss: 0.2893\n",
            "  [Batch 3500/6245] Loss: 0.2887\n",
            "  [Batch 3510/6245] Loss: 0.5363\n",
            "  [Batch 3520/6245] Loss: 0.1637\n",
            "  [Batch 3530/6245] Loss: 0.1626\n",
            "  [Batch 3540/6245] Loss: 0.3645\n",
            "  [Batch 3550/6245] Loss: 0.4625\n",
            "  [Batch 3560/6245] Loss: 0.4252\n",
            "  [Batch 3570/6245] Loss: 0.2525\n",
            "  [Batch 3580/6245] Loss: 0.1750\n",
            "  [Batch 3590/6245] Loss: 0.5356\n",
            "  [Batch 3600/6245] Loss: 0.2373\n",
            "  [Batch 3610/6245] Loss: 0.3488\n",
            "  [Batch 3620/6245] Loss: 0.3437\n",
            "  [Batch 3630/6245] Loss: 0.2423\n",
            "  [Batch 3640/6245] Loss: 0.3434\n",
            "  [Batch 3650/6245] Loss: 0.2834\n",
            "  [Batch 3660/6245] Loss: 0.2576\n",
            "  [Batch 3670/6245] Loss: 0.3597\n",
            "  [Batch 3680/6245] Loss: 0.2469\n",
            "  [Batch 3690/6245] Loss: 0.3276\n",
            "  [Batch 3700/6245] Loss: 0.2733\n",
            "  [Batch 3710/6245] Loss: 0.3100\n",
            "  [Batch 3720/6245] Loss: 0.1991\n",
            "  [Batch 3730/6245] Loss: 0.2322\n",
            "  [Batch 3740/6245] Loss: 0.3162\n",
            "  [Batch 3750/6245] Loss: 0.1969\n",
            "  [Batch 3760/6245] Loss: 0.5373\n",
            "  [Batch 3770/6245] Loss: 0.3790\n",
            "  [Batch 3780/6245] Loss: 0.3218\n",
            "  [Batch 3790/6245] Loss: 0.4611\n",
            "  [Batch 3800/6245] Loss: 0.2602\n",
            "  [Batch 3810/6245] Loss: 0.2752\n",
            "  [Batch 3820/6245] Loss: 0.1752\n",
            "  [Batch 3830/6245] Loss: 0.1991\n",
            "  [Batch 3840/6245] Loss: 0.1748\n",
            "  [Batch 3850/6245] Loss: 0.3221\n",
            "  [Batch 3860/6245] Loss: 0.4740\n",
            "  [Batch 3870/6245] Loss: 0.5145\n",
            "  [Batch 3880/6245] Loss: 0.3290\n",
            "  [Batch 3890/6245] Loss: 0.3597\n",
            "  [Batch 3900/6245] Loss: 0.2187\n",
            "  [Batch 3910/6245] Loss: 0.4649\n",
            "  [Batch 3920/6245] Loss: 0.2061\n",
            "  [Batch 3930/6245] Loss: 0.3305\n",
            "  [Batch 3940/6245] Loss: 0.1460\n",
            "  [Batch 3950/6245] Loss: 0.4020\n",
            "  [Batch 3960/6245] Loss: 0.2029\n",
            "  [Batch 3970/6245] Loss: 0.3807\n",
            "  [Batch 3980/6245] Loss: 0.4096\n",
            "  [Batch 3990/6245] Loss: 0.3317\n",
            "  [Batch 4000/6245] Loss: 0.1883\n",
            "  [Batch 4010/6245] Loss: 0.3671\n",
            "  [Batch 4020/6245] Loss: 0.3344\n",
            "  [Batch 4030/6245] Loss: 0.2693\n",
            "  [Batch 4040/6245] Loss: 0.2436\n",
            "  [Batch 4050/6245] Loss: 0.2126\n",
            "  [Batch 4060/6245] Loss: 0.4137\n",
            "  [Batch 4070/6245] Loss: 0.1694\n",
            "  [Batch 4080/6245] Loss: 0.4142\n",
            "  [Batch 4090/6245] Loss: 0.3759\n",
            "  [Batch 4100/6245] Loss: 0.1832\n",
            "  [Batch 4110/6245] Loss: 0.2064\n",
            "  [Batch 4120/6245] Loss: 0.2517\n",
            "  [Batch 4130/6245] Loss: 0.2320\n",
            "  [Batch 4140/6245] Loss: 0.4013\n",
            "  [Batch 4150/6245] Loss: 0.2350\n",
            "  [Batch 4160/6245] Loss: 0.3587\n",
            "  [Batch 4170/6245] Loss: 0.1647\n",
            "  [Batch 4180/6245] Loss: 0.3929\n",
            "  [Batch 4190/6245] Loss: 0.3366\n",
            "  [Batch 4200/6245] Loss: 0.3758\n",
            "  [Batch 4210/6245] Loss: 0.3362\n",
            "  [Batch 4220/6245] Loss: 0.2015\n",
            "  [Batch 4230/6245] Loss: 0.3168\n",
            "  [Batch 4240/6245] Loss: 0.4952\n",
            "  [Batch 4250/6245] Loss: 0.2308\n",
            "  [Batch 4260/6245] Loss: 0.2549\n",
            "  [Batch 4270/6245] Loss: 0.3889\n",
            "  [Batch 4280/6245] Loss: 0.2784\n",
            "  [Batch 4290/6245] Loss: 0.2706\n",
            "  [Batch 4300/6245] Loss: 0.1981\n",
            "  [Batch 4310/6245] Loss: 0.2894\n",
            "  [Batch 4320/6245] Loss: 0.4327\n",
            "  [Batch 4330/6245] Loss: 0.2500\n",
            "  [Batch 4340/6245] Loss: 0.2741\n",
            "  [Batch 4350/6245] Loss: 0.2271\n",
            "  [Batch 4360/6245] Loss: 0.2463\n",
            "  [Batch 4370/6245] Loss: 0.2812\n",
            "  [Batch 4380/6245] Loss: 0.2610\n",
            "  [Batch 4390/6245] Loss: 0.1609\n",
            "  [Batch 4400/6245] Loss: 0.0634\n",
            "  [Batch 4410/6245] Loss: 0.5191\n",
            "  [Batch 4420/6245] Loss: 0.2686\n",
            "  [Batch 4430/6245] Loss: 0.3535\n",
            "  [Batch 4440/6245] Loss: 0.2122\n",
            "  [Batch 4450/6245] Loss: 0.5213\n",
            "  [Batch 4460/6245] Loss: 0.2448\n",
            "  [Batch 4470/6245] Loss: 0.3335\n",
            "  [Batch 4480/6245] Loss: 0.3135\n",
            "  [Batch 4490/6245] Loss: 0.1642\n",
            "  [Batch 4500/6245] Loss: 0.1210\n",
            "  [Batch 4510/6245] Loss: 0.2509\n",
            "  [Batch 4520/6245] Loss: 0.2765\n",
            "  [Batch 4530/6245] Loss: 0.3901\n",
            "  [Batch 4540/6245] Loss: 0.4021\n",
            "  [Batch 4550/6245] Loss: 0.2461\n",
            "  [Batch 4560/6245] Loss: 0.2789\n",
            "  [Batch 4570/6245] Loss: 0.3339\n",
            "  [Batch 4580/6245] Loss: 0.1909\n",
            "  [Batch 4590/6245] Loss: 0.2801\n",
            "  [Batch 4600/6245] Loss: 0.1849\n",
            "  [Batch 4610/6245] Loss: 0.2380\n",
            "  [Batch 4620/6245] Loss: 0.4089\n",
            "  [Batch 4630/6245] Loss: 0.4441\n",
            "  [Batch 4640/6245] Loss: 0.3111\n",
            "  [Batch 4650/6245] Loss: 0.2809\n",
            "  [Batch 4660/6245] Loss: 0.1982\n",
            "  [Batch 4670/6245] Loss: 0.2396\n",
            "  [Batch 4680/6245] Loss: 0.2007\n",
            "  [Batch 4690/6245] Loss: 0.2746\n",
            "  [Batch 4700/6245] Loss: 0.3826\n",
            "  [Batch 4710/6245] Loss: 0.3203\n",
            "  [Batch 4720/6245] Loss: 0.4694\n",
            "  [Batch 4730/6245] Loss: 0.6115\n",
            "  [Batch 4740/6245] Loss: 0.3413\n",
            "  [Batch 4750/6245] Loss: 0.3392\n",
            "  [Batch 4760/6245] Loss: 0.3197\n",
            "  [Batch 4770/6245] Loss: 0.4346\n",
            "  [Batch 4780/6245] Loss: 0.3062\n",
            "  [Batch 4790/6245] Loss: 0.1356\n",
            "  [Batch 4800/6245] Loss: 0.5527\n",
            "  [Batch 4810/6245] Loss: 0.3039\n",
            "  [Batch 4820/6245] Loss: 0.2730\n",
            "  [Batch 4830/6245] Loss: 0.1712\n",
            "  [Batch 4840/6245] Loss: 0.1853\n",
            "  [Batch 4850/6245] Loss: 0.2297\n",
            "  [Batch 4860/6245] Loss: 0.4907\n",
            "  [Batch 4870/6245] Loss: 0.3109\n",
            "  [Batch 4880/6245] Loss: 0.2497\n",
            "  [Batch 4890/6245] Loss: 0.2160\n",
            "  [Batch 4900/6245] Loss: 0.2237\n",
            "  [Batch 4910/6245] Loss: 0.4812\n",
            "  [Batch 4920/6245] Loss: 0.4270\n",
            "  [Batch 4930/6245] Loss: 0.3290\n",
            "  [Batch 4940/6245] Loss: 0.5257\n",
            "  [Batch 4950/6245] Loss: 0.3187\n",
            "  [Batch 4960/6245] Loss: 0.2972\n",
            "  [Batch 4970/6245] Loss: 0.3592\n",
            "  [Batch 4980/6245] Loss: 0.2187\n",
            "  [Batch 4990/6245] Loss: 0.4319\n",
            "  [Batch 5000/6245] Loss: 0.2152\n",
            "  [Batch 5010/6245] Loss: 0.5239\n",
            "  [Batch 5020/6245] Loss: 0.3062\n",
            "  [Batch 5030/6245] Loss: 0.2370\n",
            "  [Batch 5040/6245] Loss: 0.2253\n",
            "  [Batch 5050/6245] Loss: 0.3416\n",
            "  [Batch 5060/6245] Loss: 0.4573\n",
            "  [Batch 5070/6245] Loss: 0.4691\n",
            "  [Batch 5080/6245] Loss: 0.3082\n",
            "  [Batch 5090/6245] Loss: 0.3713\n",
            "  [Batch 5100/6245] Loss: 0.3572\n",
            "  [Batch 5110/6245] Loss: 0.4505\n",
            "  [Batch 5120/6245] Loss: 0.1631\n",
            "  [Batch 5130/6245] Loss: 0.3172\n",
            "  [Batch 5140/6245] Loss: 0.2516\n",
            "  [Batch 5150/6245] Loss: 0.2451\n",
            "  [Batch 5160/6245] Loss: 0.2469\n",
            "  [Batch 5170/6245] Loss: 0.3847\n",
            "  [Batch 5180/6245] Loss: 0.2842\n",
            "  [Batch 5190/6245] Loss: 0.2539\n",
            "  [Batch 5200/6245] Loss: 0.3020\n",
            "  [Batch 5210/6245] Loss: 0.3010\n",
            "  [Batch 5220/6245] Loss: 0.3710\n",
            "  [Batch 5230/6245] Loss: 0.1495\n",
            "  [Batch 5240/6245] Loss: 0.2953\n",
            "  [Batch 5250/6245] Loss: 0.3160\n",
            "  [Batch 5260/6245] Loss: 0.4165\n",
            "  [Batch 5270/6245] Loss: 0.1913\n",
            "  [Batch 5280/6245] Loss: 0.3906\n",
            "  [Batch 5290/6245] Loss: 0.4065\n",
            "  [Batch 5300/6245] Loss: 0.2770\n",
            "  [Batch 5310/6245] Loss: 0.3379\n",
            "  [Batch 5320/6245] Loss: 0.2600\n",
            "  [Batch 5330/6245] Loss: 0.4571\n",
            "  [Batch 5340/6245] Loss: 0.2741\n",
            "  [Batch 5350/6245] Loss: 0.2247\n",
            "  [Batch 5360/6245] Loss: 0.5254\n",
            "  [Batch 5370/6245] Loss: 0.2592\n",
            "  [Batch 5380/6245] Loss: 0.2388\n",
            "  [Batch 5390/6245] Loss: 0.2216\n",
            "  [Batch 5400/6245] Loss: 0.2210\n",
            "  [Batch 5410/6245] Loss: 0.7030\n",
            "  [Batch 5420/6245] Loss: 0.3436\n",
            "  [Batch 5430/6245] Loss: 0.3176\n",
            "  [Batch 5440/6245] Loss: 0.3568\n",
            "  [Batch 5450/6245] Loss: 0.1205\n",
            "  [Batch 5460/6245] Loss: 0.2782\n",
            "  [Batch 5470/6245] Loss: 0.4688\n",
            "  [Batch 5480/6245] Loss: 0.1851\n",
            "  [Batch 5490/6245] Loss: 0.3078\n",
            "  [Batch 5500/6245] Loss: 0.2073\n",
            "  [Batch 5510/6245] Loss: 0.4167\n",
            "  [Batch 5520/6245] Loss: 0.1603\n",
            "  [Batch 5530/6245] Loss: 0.1929\n",
            "  [Batch 5540/6245] Loss: 0.2056\n",
            "  [Batch 5550/6245] Loss: 0.1637\n",
            "  [Batch 5560/6245] Loss: 0.4790\n",
            "  [Batch 5570/6245] Loss: 0.2390\n",
            "  [Batch 5580/6245] Loss: 0.2178\n",
            "  [Batch 5590/6245] Loss: 0.4018\n",
            "  [Batch 5600/6245] Loss: 0.3318\n",
            "  [Batch 5610/6245] Loss: 0.2535\n",
            "  [Batch 5620/6245] Loss: 0.2740\n",
            "  [Batch 5630/6245] Loss: 0.2783\n",
            "  [Batch 5640/6245] Loss: 0.4181\n",
            "  [Batch 5650/6245] Loss: 0.2848\n",
            "  [Batch 5660/6245] Loss: 0.4331\n",
            "  [Batch 5670/6245] Loss: 0.1678\n",
            "  [Batch 5680/6245] Loss: 0.4077\n",
            "  [Batch 5690/6245] Loss: 0.3091\n",
            "  [Batch 5700/6245] Loss: 0.2695\n",
            "  [Batch 5710/6245] Loss: 0.3014\n",
            "  [Batch 5720/6245] Loss: 0.4013\n",
            "  [Batch 5730/6245] Loss: 0.5877\n",
            "  [Batch 5740/6245] Loss: 0.4429\n",
            "  [Batch 5750/6245] Loss: 0.2923\n",
            "  [Batch 5760/6245] Loss: 0.2922\n",
            "  [Batch 5770/6245] Loss: 0.3607\n",
            "  [Batch 5780/6245] Loss: 0.2467\n",
            "  [Batch 5790/6245] Loss: 0.4019\n",
            "  [Batch 5800/6245] Loss: 0.3140\n",
            "  [Batch 5810/6245] Loss: 0.4804\n",
            "  [Batch 5820/6245] Loss: 0.2548\n",
            "  [Batch 5830/6245] Loss: 0.1993\n",
            "  [Batch 5840/6245] Loss: 0.2929\n",
            "  [Batch 5850/6245] Loss: 0.4630\n",
            "  [Batch 5860/6245] Loss: 0.2795\n",
            "  [Batch 5870/6245] Loss: 0.2727\n",
            "  [Batch 5880/6245] Loss: 0.3137\n",
            "  [Batch 5890/6245] Loss: 0.2229\n",
            "  [Batch 5900/6245] Loss: 0.4196\n",
            "  [Batch 5910/6245] Loss: 0.3007\n",
            "  [Batch 5920/6245] Loss: 0.2367\n",
            "  [Batch 5930/6245] Loss: 0.5333\n",
            "  [Batch 5940/6245] Loss: 0.2980\n",
            "  [Batch 5950/6245] Loss: 0.4379\n",
            "  [Batch 5960/6245] Loss: 0.2690\n",
            "  [Batch 5970/6245] Loss: 0.1271\n",
            "  [Batch 5980/6245] Loss: 0.1766\n",
            "  [Batch 5990/6245] Loss: 0.1837\n",
            "  [Batch 6000/6245] Loss: 0.3456\n",
            "  [Batch 6010/6245] Loss: 0.3396\n",
            "  [Batch 6020/6245] Loss: 0.3131\n",
            "  [Batch 6030/6245] Loss: 0.4181\n",
            "  [Batch 6040/6245] Loss: 0.1651\n",
            "  [Batch 6050/6245] Loss: 0.4220\n",
            "  [Batch 6060/6245] Loss: 0.2534\n",
            "  [Batch 6070/6245] Loss: 0.2928\n",
            "  [Batch 6080/6245] Loss: 0.3446\n",
            "  [Batch 6090/6245] Loss: 0.3602\n",
            "  [Batch 6100/6245] Loss: 0.1093\n",
            "  [Batch 6110/6245] Loss: 0.3549\n",
            "  [Batch 6120/6245] Loss: 0.3658\n",
            "  [Batch 6130/6245] Loss: 0.4167\n",
            "  [Batch 6140/6245] Loss: 0.3251\n",
            "  [Batch 6150/6245] Loss: 0.2137\n",
            "  [Batch 6160/6245] Loss: 0.2170\n",
            "  [Batch 6170/6245] Loss: 0.3128\n",
            "  [Batch 6180/6245] Loss: 0.2799\n",
            "  [Batch 6190/6245] Loss: 0.1899\n",
            "  [Batch 6200/6245] Loss: 0.2578\n",
            "  [Batch 6210/6245] Loss: 0.1530\n",
            "  [Batch 6220/6245] Loss: 0.3339\n",
            "  [Batch 6230/6245] Loss: 0.5586\n",
            "  [Batch 6240/6245] Loss: 0.5416\n",
            "  [Batch 6245/6245] Loss: 0.4339\n",
            "[Epoch 6] Train Loss: 0.3206, Acc: 86.18% | Val Loss: 0.3124, Acc: 86.57%\n",
            "[INFO] Model checkpoint saved at ./models\\breastnet_epoch6.pth\n",
            "\n",
            "[Epoch 7/10]\n",
            "  [Batch 10/6245] Loss: 0.4530\n",
            "  [Batch 20/6245] Loss: 0.3136\n",
            "  [Batch 30/6245] Loss: 0.2895\n",
            "  [Batch 40/6245] Loss: 0.2959\n",
            "  [Batch 50/6245] Loss: 0.5062\n",
            "  [Batch 60/6245] Loss: 0.3020\n",
            "  [Batch 70/6245] Loss: 0.2810\n",
            "  [Batch 80/6245] Loss: 0.3683\n",
            "  [Batch 90/6245] Loss: 0.3198\n",
            "  [Batch 100/6245] Loss: 0.2296\n",
            "  [Batch 110/6245] Loss: 0.4108\n",
            "  [Batch 120/6245] Loss: 0.3759\n",
            "  [Batch 130/6245] Loss: 0.2645\n",
            "  [Batch 140/6245] Loss: 0.3289\n",
            "  [Batch 150/6245] Loss: 0.6655\n",
            "  [Batch 160/6245] Loss: 0.2595\n",
            "  [Batch 170/6245] Loss: 0.2327\n",
            "  [Batch 180/6245] Loss: 0.4193\n",
            "  [Batch 190/6245] Loss: 0.1835\n",
            "  [Batch 200/6245] Loss: 0.3949\n",
            "  [Batch 210/6245] Loss: 0.2787\n",
            "  [Batch 220/6245] Loss: 0.2287\n",
            "  [Batch 230/6245] Loss: 0.3233\n",
            "  [Batch 240/6245] Loss: 0.3195\n",
            "  [Batch 250/6245] Loss: 0.3220\n",
            "  [Batch 260/6245] Loss: 0.2830\n",
            "  [Batch 270/6245] Loss: 0.2202\n",
            "  [Batch 280/6245] Loss: 0.1147\n",
            "  [Batch 290/6245] Loss: 0.2919\n",
            "  [Batch 300/6245] Loss: 0.4977\n",
            "  [Batch 310/6245] Loss: 0.2509\n",
            "  [Batch 320/6245] Loss: 0.1694\n",
            "  [Batch 330/6245] Loss: 0.2647\n",
            "  [Batch 340/6245] Loss: 0.3564\n",
            "  [Batch 350/6245] Loss: 0.3403\n",
            "  [Batch 360/6245] Loss: 0.2078\n",
            "  [Batch 370/6245] Loss: 0.3099\n",
            "  [Batch 380/6245] Loss: 0.2986\n",
            "  [Batch 390/6245] Loss: 0.3260\n",
            "  [Batch 400/6245] Loss: 0.3077\n",
            "  [Batch 410/6245] Loss: 0.2374\n",
            "  [Batch 420/6245] Loss: 0.1592\n",
            "  [Batch 430/6245] Loss: 0.2718\n",
            "  [Batch 440/6245] Loss: 0.5002\n",
            "  [Batch 450/6245] Loss: 0.4682\n",
            "  [Batch 460/6245] Loss: 0.4799\n",
            "  [Batch 470/6245] Loss: 0.3766\n",
            "  [Batch 480/6245] Loss: 0.4214\n",
            "  [Batch 490/6245] Loss: 0.2308\n",
            "  [Batch 500/6245] Loss: 0.2884\n",
            "  [Batch 510/6245] Loss: 0.4732\n",
            "  [Batch 520/6245] Loss: 0.3082\n",
            "  [Batch 530/6245] Loss: 0.2744\n",
            "  [Batch 540/6245] Loss: 0.3927\n",
            "  [Batch 550/6245] Loss: 0.2689\n",
            "  [Batch 560/6245] Loss: 0.2092\n",
            "  [Batch 570/6245] Loss: 0.6402\n",
            "  [Batch 580/6245] Loss: 0.2968\n",
            "  [Batch 590/6245] Loss: 0.2045\n",
            "  [Batch 600/6245] Loss: 0.2249\n",
            "  [Batch 610/6245] Loss: 0.2500\n",
            "  [Batch 620/6245] Loss: 0.3610\n",
            "  [Batch 630/6245] Loss: 0.2274\n",
            "  [Batch 640/6245] Loss: 0.4230\n",
            "  [Batch 650/6245] Loss: 0.2923\n",
            "  [Batch 660/6245] Loss: 0.2719\n",
            "  [Batch 670/6245] Loss: 0.3264\n",
            "  [Batch 680/6245] Loss: 0.2773\n",
            "  [Batch 690/6245] Loss: 0.3610\n",
            "  [Batch 700/6245] Loss: 0.2652\n",
            "  [Batch 710/6245] Loss: 0.3223\n",
            "  [Batch 720/6245] Loss: 0.3160\n",
            "  [Batch 730/6245] Loss: 0.3309\n",
            "  [Batch 740/6245] Loss: 0.1974\n",
            "  [Batch 750/6245] Loss: 0.4042\n",
            "  [Batch 760/6245] Loss: 0.1674\n",
            "  [Batch 770/6245] Loss: 0.3717\n",
            "  [Batch 780/6245] Loss: 0.3756\n",
            "  [Batch 790/6245] Loss: 0.3373\n",
            "  [Batch 800/6245] Loss: 0.1963\n",
            "  [Batch 810/6245] Loss: 0.3710\n",
            "  [Batch 820/6245] Loss: 0.3676\n",
            "  [Batch 830/6245] Loss: 0.2008\n",
            "  [Batch 840/6245] Loss: 0.3751\n",
            "  [Batch 850/6245] Loss: 0.2676\n",
            "  [Batch 860/6245] Loss: 0.4011\n",
            "  [Batch 870/6245] Loss: 0.2027\n",
            "  [Batch 880/6245] Loss: 0.4259\n",
            "  [Batch 890/6245] Loss: 0.5793\n",
            "  [Batch 900/6245] Loss: 0.3268\n",
            "  [Batch 910/6245] Loss: 0.1646\n",
            "  [Batch 920/6245] Loss: 0.3516\n",
            "  [Batch 930/6245] Loss: 0.4396\n",
            "  [Batch 940/6245] Loss: 0.3192\n",
            "  [Batch 950/6245] Loss: 0.4415\n",
            "  [Batch 960/6245] Loss: 0.4320\n",
            "  [Batch 970/6245] Loss: 0.2438\n",
            "  [Batch 980/6245] Loss: 0.3511\n",
            "  [Batch 990/6245] Loss: 0.4384\n",
            "  [Batch 1000/6245] Loss: 0.3434\n",
            "  [Batch 1010/6245] Loss: 0.2394\n",
            "  [Batch 1020/6245] Loss: 0.2244\n",
            "  [Batch 1030/6245] Loss: 0.2142\n",
            "  [Batch 1040/6245] Loss: 0.2755\n",
            "  [Batch 1050/6245] Loss: 0.3371\n",
            "  [Batch 1060/6245] Loss: 0.3287\n",
            "  [Batch 1070/6245] Loss: 0.3155\n",
            "  [Batch 1080/6245] Loss: 0.3377\n",
            "  [Batch 1090/6245] Loss: 0.3522\n",
            "  [Batch 1100/6245] Loss: 0.2580\n",
            "  [Batch 1110/6245] Loss: 0.4248\n",
            "  [Batch 1120/6245] Loss: 0.5166\n",
            "  [Batch 1130/6245] Loss: 0.3851\n",
            "  [Batch 1140/6245] Loss: 0.2411\n",
            "  [Batch 1150/6245] Loss: 0.3009\n",
            "  [Batch 1160/6245] Loss: 0.2273\n",
            "  [Batch 1170/6245] Loss: 0.3728\n",
            "  [Batch 1180/6245] Loss: 0.3722\n",
            "  [Batch 1190/6245] Loss: 0.1404\n",
            "  [Batch 1200/6245] Loss: 0.3093\n",
            "  [Batch 1210/6245] Loss: 0.2769\n",
            "  [Batch 1220/6245] Loss: 0.2653\n",
            "  [Batch 1230/6245] Loss: 0.3130\n",
            "  [Batch 1240/6245] Loss: 0.4659\n",
            "  [Batch 1250/6245] Loss: 0.2547\n",
            "  [Batch 1260/6245] Loss: 0.3964\n",
            "  [Batch 1270/6245] Loss: 0.2522\n",
            "  [Batch 1280/6245] Loss: 0.1698\n",
            "  [Batch 1290/6245] Loss: 0.2221\n",
            "  [Batch 1300/6245] Loss: 0.1651\n",
            "  [Batch 1310/6245] Loss: 0.3147\n",
            "  [Batch 1320/6245] Loss: 0.2802\n",
            "  [Batch 1330/6245] Loss: 0.1984\n",
            "  [Batch 1340/6245] Loss: 0.1849\n",
            "  [Batch 1350/6245] Loss: 0.2697\n",
            "  [Batch 1360/6245] Loss: 0.1488\n",
            "  [Batch 1370/6245] Loss: 0.3391\n",
            "  [Batch 1380/6245] Loss: 0.3795\n",
            "  [Batch 1390/6245] Loss: 0.2137\n",
            "  [Batch 1400/6245] Loss: 0.2761\n",
            "  [Batch 1410/6245] Loss: 0.2442\n",
            "  [Batch 1420/6245] Loss: 0.3329\n",
            "  [Batch 1430/6245] Loss: 0.2374\n",
            "  [Batch 1440/6245] Loss: 0.1958\n",
            "  [Batch 1450/6245] Loss: 0.3070\n",
            "  [Batch 1460/6245] Loss: 0.2667\n",
            "  [Batch 1470/6245] Loss: 0.2597\n",
            "  [Batch 1480/6245] Loss: 0.1633\n",
            "  [Batch 1490/6245] Loss: 0.4269\n",
            "  [Batch 1500/6245] Loss: 0.6215\n",
            "  [Batch 1510/6245] Loss: 0.2672\n",
            "  [Batch 1520/6245] Loss: 0.3600\n",
            "  [Batch 1530/6245] Loss: 0.3227\n",
            "  [Batch 1540/6245] Loss: 0.2136\n",
            "  [Batch 1550/6245] Loss: 0.3202\n",
            "  [Batch 1560/6245] Loss: 0.4807\n",
            "  [Batch 1570/6245] Loss: 0.2600\n",
            "  [Batch 1580/6245] Loss: 0.2684\n",
            "  [Batch 1590/6245] Loss: 0.3532\n",
            "  [Batch 1600/6245] Loss: 0.2366\n",
            "  [Batch 1610/6245] Loss: 0.3899\n",
            "  [Batch 1620/6245] Loss: 0.3480\n",
            "  [Batch 1630/6245] Loss: 0.3025\n",
            "  [Batch 1640/6245] Loss: 0.3917\n",
            "  [Batch 1650/6245] Loss: 0.2049\n",
            "  [Batch 1660/6245] Loss: 0.1935\n",
            "  [Batch 1670/6245] Loss: 0.4462\n",
            "  [Batch 1680/6245] Loss: 0.4226\n",
            "  [Batch 1690/6245] Loss: 0.2549\n",
            "  [Batch 1700/6245] Loss: 0.5061\n",
            "  [Batch 1710/6245] Loss: 0.3165\n",
            "  [Batch 1720/6245] Loss: 0.2248\n",
            "  [Batch 1730/6245] Loss: 0.2690\n",
            "  [Batch 1740/6245] Loss: 0.2894\n",
            "  [Batch 1750/6245] Loss: 0.3417\n",
            "  [Batch 1760/6245] Loss: 0.3925\n",
            "  [Batch 1770/6245] Loss: 0.4992\n",
            "  [Batch 1780/6245] Loss: 0.2730\n",
            "  [Batch 1790/6245] Loss: 0.4044\n",
            "  [Batch 1800/6245] Loss: 0.1633\n",
            "  [Batch 1810/6245] Loss: 0.4081\n",
            "  [Batch 1820/6245] Loss: 0.3059\n",
            "  [Batch 1830/6245] Loss: 0.2811\n",
            "  [Batch 1840/6245] Loss: 0.1766\n",
            "  [Batch 1850/6245] Loss: 0.2806\n",
            "  [Batch 1860/6245] Loss: 0.2316\n",
            "  [Batch 1870/6245] Loss: 0.3215\n",
            "  [Batch 1880/6245] Loss: 0.4263\n",
            "  [Batch 1890/6245] Loss: 0.4893\n",
            "  [Batch 1900/6245] Loss: 0.2464\n",
            "  [Batch 1910/6245] Loss: 0.3153\n",
            "  [Batch 1920/6245] Loss: 0.2372\n",
            "  [Batch 1930/6245] Loss: 0.3769\n",
            "  [Batch 1940/6245] Loss: 0.3449\n",
            "  [Batch 1950/6245] Loss: 0.4666\n",
            "  [Batch 1960/6245] Loss: 0.3018\n",
            "  [Batch 1970/6245] Loss: 0.3455\n",
            "  [Batch 1980/6245] Loss: 0.4250\n",
            "  [Batch 1990/6245] Loss: 0.4767\n",
            "  [Batch 2000/6245] Loss: 0.2500\n",
            "  [Batch 2010/6245] Loss: 0.2388\n",
            "  [Batch 2020/6245] Loss: 0.2532\n",
            "  [Batch 2030/6245] Loss: 0.3003\n",
            "  [Batch 2040/6245] Loss: 0.1778\n",
            "  [Batch 2050/6245] Loss: 0.5541\n",
            "  [Batch 2060/6245] Loss: 0.4080\n",
            "  [Batch 2070/6245] Loss: 0.1893\n",
            "  [Batch 2080/6245] Loss: 0.4523\n",
            "  [Batch 2090/6245] Loss: 0.3346\n",
            "  [Batch 2100/6245] Loss: 0.2171\n",
            "  [Batch 2110/6245] Loss: 0.2001\n",
            "  [Batch 2120/6245] Loss: 0.2964\n",
            "  [Batch 2130/6245] Loss: 0.3147\n",
            "  [Batch 2140/6245] Loss: 0.2722\n",
            "  [Batch 2150/6245] Loss: 0.4932\n",
            "  [Batch 2160/6245] Loss: 0.3380\n",
            "  [Batch 2170/6245] Loss: 0.2792\n",
            "  [Batch 2180/6245] Loss: 0.2192\n",
            "  [Batch 2190/6245] Loss: 0.1753\n",
            "  [Batch 2200/6245] Loss: 0.3672\n",
            "  [Batch 2210/6245] Loss: 0.1885\n",
            "  [Batch 2220/6245] Loss: 0.4003\n",
            "  [Batch 2230/6245] Loss: 0.5064\n",
            "  [Batch 2240/6245] Loss: 0.3560\n",
            "  [Batch 2250/6245] Loss: 0.3759\n",
            "  [Batch 2260/6245] Loss: 0.3063\n",
            "  [Batch 2270/6245] Loss: 0.5276\n",
            "  [Batch 2280/6245] Loss: 0.1997\n",
            "  [Batch 2290/6245] Loss: 0.2795\n",
            "  [Batch 2300/6245] Loss: 0.4152\n",
            "  [Batch 2310/6245] Loss: 0.2436\n",
            "  [Batch 2320/6245] Loss: 0.7068\n",
            "  [Batch 2330/6245] Loss: 0.4233\n",
            "  [Batch 2340/6245] Loss: 0.1981\n",
            "  [Batch 2350/6245] Loss: 0.5748\n",
            "  [Batch 2360/6245] Loss: 0.3051\n",
            "  [Batch 2370/6245] Loss: 0.2509\n",
            "  [Batch 2380/6245] Loss: 0.3509\n",
            "  [Batch 2390/6245] Loss: 0.3206\n",
            "  [Batch 2400/6245] Loss: 0.2142\n",
            "  [Batch 2410/6245] Loss: 0.2430\n",
            "  [Batch 2420/6245] Loss: 0.4247\n",
            "  [Batch 2430/6245] Loss: 0.1932\n",
            "  [Batch 2440/6245] Loss: 0.2903\n",
            "  [Batch 2450/6245] Loss: 0.5394\n",
            "  [Batch 2460/6245] Loss: 0.4319\n",
            "  [Batch 2470/6245] Loss: 0.1808\n",
            "  [Batch 2480/6245] Loss: 0.1262\n",
            "  [Batch 2490/6245] Loss: 0.1623\n",
            "  [Batch 2500/6245] Loss: 0.2474\n",
            "  [Batch 2510/6245] Loss: 0.2786\n",
            "  [Batch 2520/6245] Loss: 0.4630\n",
            "  [Batch 2530/6245] Loss: 0.2931\n",
            "  [Batch 2540/6245] Loss: 0.4853\n",
            "  [Batch 2550/6245] Loss: 0.4497\n",
            "  [Batch 2560/6245] Loss: 0.3039\n",
            "  [Batch 2570/6245] Loss: 0.2949\n",
            "  [Batch 2580/6245] Loss: 0.1925\n",
            "  [Batch 2590/6245] Loss: 0.2245\n",
            "  [Batch 2600/6245] Loss: 0.2253\n",
            "  [Batch 2610/6245] Loss: 0.3761\n",
            "  [Batch 2620/6245] Loss: 0.1610\n",
            "  [Batch 2630/6245] Loss: 0.2947\n",
            "  [Batch 2640/6245] Loss: 0.3275\n",
            "  [Batch 2650/6245] Loss: 0.2080\n",
            "  [Batch 2660/6245] Loss: 0.3179\n",
            "  [Batch 2670/6245] Loss: 0.3611\n",
            "  [Batch 2680/6245] Loss: 0.2059\n",
            "  [Batch 2690/6245] Loss: 0.1511\n",
            "  [Batch 2700/6245] Loss: 0.2400\n",
            "  [Batch 2710/6245] Loss: 0.4473\n",
            "  [Batch 2720/6245] Loss: 0.2499\n",
            "  [Batch 2730/6245] Loss: 0.1791\n",
            "  [Batch 2740/6245] Loss: 0.3320\n",
            "  [Batch 2750/6245] Loss: 0.4034\n",
            "  [Batch 2760/6245] Loss: 0.2294\n",
            "  [Batch 2770/6245] Loss: 0.5221\n",
            "  [Batch 2780/6245] Loss: 0.3996\n",
            "  [Batch 2790/6245] Loss: 0.2456\n",
            "  [Batch 2800/6245] Loss: 0.3388\n",
            "  [Batch 2810/6245] Loss: 0.3113\n",
            "  [Batch 2820/6245] Loss: 0.5027\n",
            "  [Batch 2830/6245] Loss: 0.2635\n",
            "  [Batch 2840/6245] Loss: 0.3872\n",
            "  [Batch 2850/6245] Loss: 0.2477\n",
            "  [Batch 2860/6245] Loss: 0.4368\n",
            "  [Batch 2870/6245] Loss: 0.3874\n",
            "  [Batch 2880/6245] Loss: 0.2801\n",
            "  [Batch 2890/6245] Loss: 0.1561\n",
            "  [Batch 2900/6245] Loss: 0.2763\n",
            "  [Batch 2910/6245] Loss: 0.3886\n",
            "  [Batch 2920/6245] Loss: 0.4336\n",
            "  [Batch 2930/6245] Loss: 0.4497\n",
            "  [Batch 2940/6245] Loss: 0.3828\n",
            "  [Batch 2950/6245] Loss: 0.3841\n",
            "  [Batch 2960/6245] Loss: 0.1235\n",
            "  [Batch 2970/6245] Loss: 0.3276\n",
            "  [Batch 2980/6245] Loss: 0.2774\n",
            "  [Batch 2990/6245] Loss: 0.4984\n",
            "  [Batch 3000/6245] Loss: 0.4388\n",
            "  [Batch 3010/6245] Loss: 0.3042\n",
            "  [Batch 3020/6245] Loss: 0.2035\n",
            "  [Batch 3030/6245] Loss: 0.1651\n",
            "  [Batch 3040/6245] Loss: 0.5268\n",
            "  [Batch 3050/6245] Loss: 0.3753\n",
            "  [Batch 3060/6245] Loss: 0.3468\n",
            "  [Batch 3070/6245] Loss: 0.3018\n",
            "  [Batch 3080/6245] Loss: 0.5053\n",
            "  [Batch 3090/6245] Loss: 0.2039\n",
            "  [Batch 3100/6245] Loss: 0.3057\n",
            "  [Batch 3110/6245] Loss: 0.2906\n",
            "  [Batch 3120/6245] Loss: 0.2663\n",
            "  [Batch 3130/6245] Loss: 0.3432\n",
            "  [Batch 3140/6245] Loss: 0.1835\n",
            "  [Batch 3150/6245] Loss: 0.5610\n",
            "  [Batch 3160/6245] Loss: 0.3642\n",
            "  [Batch 3170/6245] Loss: 0.4866\n",
            "  [Batch 3180/6245] Loss: 0.2544\n",
            "  [Batch 3190/6245] Loss: 0.3360\n",
            "  [Batch 3200/6245] Loss: 0.2509\n",
            "  [Batch 3210/6245] Loss: 0.2628\n",
            "  [Batch 3220/6245] Loss: 0.1846\n",
            "  [Batch 3230/6245] Loss: 0.3795\n",
            "  [Batch 3240/6245] Loss: 0.2709\n",
            "  [Batch 3250/6245] Loss: 0.3550\n",
            "  [Batch 3260/6245] Loss: 0.2162\n",
            "  [Batch 3270/6245] Loss: 0.2196\n",
            "  [Batch 3280/6245] Loss: 0.2650\n",
            "  [Batch 3290/6245] Loss: 0.2792\n",
            "  [Batch 3300/6245] Loss: 0.6634\n",
            "  [Batch 3310/6245] Loss: 0.2779\n",
            "  [Batch 3320/6245] Loss: 0.4369\n",
            "  [Batch 3330/6245] Loss: 0.4710\n",
            "  [Batch 3340/6245] Loss: 0.3364\n",
            "  [Batch 3350/6245] Loss: 0.2574\n",
            "  [Batch 3360/6245] Loss: 0.2273\n",
            "  [Batch 3370/6245] Loss: 0.4507\n",
            "  [Batch 3380/6245] Loss: 0.1475\n",
            "  [Batch 3390/6245] Loss: 0.4413\n",
            "  [Batch 3400/6245] Loss: 0.2446\n",
            "  [Batch 3410/6245] Loss: 0.3247\n",
            "  [Batch 3420/6245] Loss: 0.2201\n",
            "  [Batch 3430/6245] Loss: 0.1339\n",
            "  [Batch 3440/6245] Loss: 0.4400\n",
            "  [Batch 3450/6245] Loss: 0.3560\n",
            "  [Batch 3460/6245] Loss: 0.2179\n",
            "  [Batch 3470/6245] Loss: 0.5184\n",
            "  [Batch 3480/6245] Loss: 0.2000\n",
            "  [Batch 3490/6245] Loss: 0.2056\n",
            "  [Batch 3500/6245] Loss: 0.5064\n",
            "  [Batch 3510/6245] Loss: 0.2848\n",
            "  [Batch 3520/6245] Loss: 0.2205\n",
            "  [Batch 3530/6245] Loss: 0.2674\n",
            "  [Batch 3540/6245] Loss: 0.2492\n",
            "  [Batch 3550/6245] Loss: 0.2536\n",
            "  [Batch 3560/6245] Loss: 0.3075\n",
            "  [Batch 3570/6245] Loss: 0.3346\n",
            "  [Batch 3580/6245] Loss: 0.1985\n",
            "  [Batch 3590/6245] Loss: 0.5194\n",
            "  [Batch 3600/6245] Loss: 0.3019\n",
            "  [Batch 3610/6245] Loss: 0.2043\n",
            "  [Batch 3620/6245] Loss: 0.2339\n",
            "  [Batch 3630/6245] Loss: 0.2812\n",
            "  [Batch 3640/6245] Loss: 0.1670\n",
            "  [Batch 3650/6245] Loss: 0.3656\n",
            "  [Batch 3660/6245] Loss: 0.3064\n",
            "  [Batch 3670/6245] Loss: 0.3335\n",
            "  [Batch 3680/6245] Loss: 0.2938\n",
            "  [Batch 3690/6245] Loss: 0.4327\n",
            "  [Batch 3700/6245] Loss: 0.3086\n",
            "  [Batch 3710/6245] Loss: 0.2909\n",
            "  [Batch 3720/6245] Loss: 0.3313\n",
            "  [Batch 3730/6245] Loss: 0.3362\n",
            "  [Batch 3740/6245] Loss: 0.3139\n",
            "  [Batch 3750/6245] Loss: 0.4625\n",
            "  [Batch 3760/6245] Loss: 0.2518\n",
            "  [Batch 3770/6245] Loss: 0.2533\n",
            "  [Batch 3780/6245] Loss: 0.2746\n",
            "  [Batch 3790/6245] Loss: 0.4229\n",
            "  [Batch 3800/6245] Loss: 0.2398\n",
            "  [Batch 3810/6245] Loss: 0.1633\n",
            "  [Batch 3820/6245] Loss: 0.2718\n",
            "  [Batch 3830/6245] Loss: 0.5333\n",
            "  [Batch 3840/6245] Loss: 0.2631\n",
            "  [Batch 3850/6245] Loss: 0.1570\n",
            "  [Batch 3860/6245] Loss: 0.3427\n",
            "  [Batch 3870/6245] Loss: 0.2862\n",
            "  [Batch 3880/6245] Loss: 0.1772\n",
            "  [Batch 3890/6245] Loss: 0.2246\n",
            "  [Batch 3900/6245] Loss: 0.2160\n",
            "  [Batch 3910/6245] Loss: 0.1987\n",
            "  [Batch 3920/6245] Loss: 0.3507\n",
            "  [Batch 3930/6245] Loss: 0.3975\n",
            "  [Batch 3940/6245] Loss: 0.2277\n",
            "  [Batch 3950/6245] Loss: 0.2738\n",
            "  [Batch 3960/6245] Loss: 0.2583\n",
            "  [Batch 3970/6245] Loss: 0.2796\n",
            "  [Batch 3980/6245] Loss: 0.3053\n",
            "  [Batch 3990/6245] Loss: 0.3681\n",
            "  [Batch 4000/6245] Loss: 0.3350\n",
            "  [Batch 4010/6245] Loss: 0.2438\n",
            "  [Batch 4020/6245] Loss: 0.2391\n",
            "  [Batch 4030/6245] Loss: 0.2720\n",
            "  [Batch 4040/6245] Loss: 0.3932\n",
            "  [Batch 4050/6245] Loss: 0.2038\n",
            "  [Batch 4060/6245] Loss: 0.3769\n",
            "  [Batch 4070/6245] Loss: 0.2661\n",
            "  [Batch 4080/6245] Loss: 0.3314\n",
            "  [Batch 4090/6245] Loss: 0.3702\n",
            "  [Batch 4100/6245] Loss: 0.3006\n",
            "  [Batch 4110/6245] Loss: 0.2643\n",
            "  [Batch 4120/6245] Loss: 0.1807\n",
            "  [Batch 4130/6245] Loss: 0.4110\n",
            "  [Batch 4140/6245] Loss: 0.3194\n",
            "  [Batch 4150/6245] Loss: 0.3119\n",
            "  [Batch 4160/6245] Loss: 0.3015\n",
            "  [Batch 4170/6245] Loss: 0.4636\n",
            "  [Batch 4180/6245] Loss: 0.2726\n",
            "  [Batch 4190/6245] Loss: 0.3141\n",
            "  [Batch 4200/6245] Loss: 0.2172\n",
            "  [Batch 4210/6245] Loss: 0.2600\n",
            "  [Batch 4220/6245] Loss: 0.1990\n",
            "  [Batch 4230/6245] Loss: 0.2481\n",
            "  [Batch 4240/6245] Loss: 0.2763\n",
            "  [Batch 4250/6245] Loss: 0.4402\n",
            "  [Batch 4260/6245] Loss: 0.5544\n",
            "  [Batch 4270/6245] Loss: 0.3445\n",
            "  [Batch 4280/6245] Loss: 0.2455\n",
            "  [Batch 4290/6245] Loss: 0.3040\n",
            "  [Batch 4300/6245] Loss: 0.3269\n",
            "  [Batch 4310/6245] Loss: 0.2780\n",
            "  [Batch 4320/6245] Loss: 0.2194\n",
            "  [Batch 4330/6245] Loss: 0.5297\n",
            "  [Batch 4340/6245] Loss: 0.2981\n",
            "  [Batch 4350/6245] Loss: 0.3126\n",
            "  [Batch 4360/6245] Loss: 0.3990\n",
            "  [Batch 4370/6245] Loss: 0.2669\n",
            "  [Batch 4380/6245] Loss: 0.1507\n",
            "  [Batch 4390/6245] Loss: 0.1761\n",
            "  [Batch 4400/6245] Loss: 0.4019\n",
            "  [Batch 4410/6245] Loss: 0.2461\n",
            "  [Batch 4420/6245] Loss: 0.1341\n",
            "  [Batch 4430/6245] Loss: 0.3467\n",
            "  [Batch 4440/6245] Loss: 0.1160\n",
            "  [Batch 4450/6245] Loss: 0.3604\n",
            "  [Batch 4460/6245] Loss: 0.2112\n",
            "  [Batch 4470/6245] Loss: 0.3332\n",
            "  [Batch 4480/6245] Loss: 0.2572\n",
            "  [Batch 4490/6245] Loss: 0.3331\n",
            "  [Batch 4500/6245] Loss: 0.4058\n",
            "  [Batch 4510/6245] Loss: 0.3320\n",
            "  [Batch 4520/6245] Loss: 0.3178\n",
            "  [Batch 4530/6245] Loss: 0.4358\n",
            "  [Batch 4540/6245] Loss: 0.6407\n",
            "  [Batch 4550/6245] Loss: 0.3108\n",
            "  [Batch 4560/6245] Loss: 0.4350\n",
            "  [Batch 4570/6245] Loss: 0.4433\n",
            "  [Batch 4580/6245] Loss: 0.3085\n",
            "  [Batch 4590/6245] Loss: 0.3596\n",
            "  [Batch 4600/6245] Loss: 0.2224\n",
            "  [Batch 4610/6245] Loss: 0.1662\n",
            "  [Batch 4620/6245] Loss: 0.2378\n",
            "  [Batch 4630/6245] Loss: 0.2206\n",
            "  [Batch 4640/6245] Loss: 0.3627\n",
            "  [Batch 4650/6245] Loss: 0.2835\n",
            "  [Batch 4660/6245] Loss: 0.4051\n",
            "  [Batch 4670/6245] Loss: 0.4647\n",
            "  [Batch 4680/6245] Loss: 0.2628\n",
            "  [Batch 4690/6245] Loss: 0.2946\n",
            "  [Batch 4700/6245] Loss: 0.2389\n",
            "  [Batch 4710/6245] Loss: 0.2272\n",
            "  [Batch 4720/6245] Loss: 0.2354\n",
            "  [Batch 4730/6245] Loss: 0.3264\n",
            "  [Batch 4740/6245] Loss: 0.2113\n",
            "  [Batch 4750/6245] Loss: 0.1754\n",
            "  [Batch 4760/6245] Loss: 0.4394\n",
            "  [Batch 4770/6245] Loss: 0.4973\n",
            "  [Batch 4780/6245] Loss: 0.4591\n",
            "  [Batch 4790/6245] Loss: 0.5054\n",
            "  [Batch 4800/6245] Loss: 0.2290\n",
            "  [Batch 4810/6245] Loss: 0.2114\n",
            "  [Batch 4820/6245] Loss: 0.4595\n",
            "  [Batch 4830/6245] Loss: 0.4655\n",
            "  [Batch 4840/6245] Loss: 0.3055\n",
            "  [Batch 4850/6245] Loss: 0.2207\n",
            "  [Batch 4860/6245] Loss: 0.2923\n",
            "  [Batch 4870/6245] Loss: 0.2510\n",
            "  [Batch 4880/6245] Loss: 0.3056\n",
            "  [Batch 4890/6245] Loss: 0.2790\n",
            "  [Batch 4900/6245] Loss: 0.3039\n",
            "  [Batch 4910/6245] Loss: 0.3792\n",
            "  [Batch 4920/6245] Loss: 0.2469\n",
            "  [Batch 4930/6245] Loss: 0.2463\n",
            "  [Batch 4940/6245] Loss: 0.1792\n",
            "  [Batch 4950/6245] Loss: 0.1810\n",
            "  [Batch 4960/6245] Loss: 0.4189\n",
            "  [Batch 4970/6245] Loss: 0.4268\n",
            "  [Batch 4980/6245] Loss: 0.3307\n",
            "  [Batch 4990/6245] Loss: 0.3133\n",
            "  [Batch 5000/6245] Loss: 0.3950\n",
            "  [Batch 5010/6245] Loss: 0.3764\n",
            "  [Batch 5020/6245] Loss: 0.3755\n",
            "  [Batch 5030/6245] Loss: 0.3740\n",
            "  [Batch 5040/6245] Loss: 0.3172\n",
            "  [Batch 5050/6245] Loss: 0.1563\n",
            "  [Batch 5060/6245] Loss: 0.3558\n",
            "  [Batch 5070/6245] Loss: 0.4659\n",
            "  [Batch 5080/6245] Loss: 0.3791\n",
            "  [Batch 5090/6245] Loss: 0.2384\n",
            "  [Batch 5100/6245] Loss: 0.3327\n",
            "  [Batch 5110/6245] Loss: 0.0954\n",
            "  [Batch 5120/6245] Loss: 0.3407\n",
            "  [Batch 5130/6245] Loss: 0.1964\n",
            "  [Batch 5140/6245] Loss: 0.3344\n",
            "  [Batch 5150/6245] Loss: 0.2153\n",
            "  [Batch 5160/6245] Loss: 0.2470\n",
            "  [Batch 5170/6245] Loss: 0.2432\n",
            "  [Batch 5180/6245] Loss: 0.2018\n",
            "  [Batch 5190/6245] Loss: 0.1884\n",
            "  [Batch 5200/6245] Loss: 0.2548\n",
            "  [Batch 5210/6245] Loss: 0.1917\n",
            "  [Batch 5220/6245] Loss: 0.4309\n",
            "  [Batch 5230/6245] Loss: 0.2346\n",
            "  [Batch 5240/6245] Loss: 0.2759\n",
            "  [Batch 5250/6245] Loss: 0.3767\n",
            "  [Batch 5260/6245] Loss: 0.2990\n",
            "  [Batch 5270/6245] Loss: 0.3914\n",
            "  [Batch 5280/6245] Loss: 0.3415\n",
            "  [Batch 5290/6245] Loss: 0.3885\n",
            "  [Batch 5300/6245] Loss: 0.4631\n",
            "  [Batch 5310/6245] Loss: 0.2892\n",
            "  [Batch 5320/6245] Loss: 0.2350\n",
            "  [Batch 5330/6245] Loss: 0.2604\n",
            "  [Batch 5340/6245] Loss: 0.2892\n",
            "  [Batch 5350/6245] Loss: 0.3517\n",
            "  [Batch 5360/6245] Loss: 0.1715\n",
            "  [Batch 5370/6245] Loss: 0.3038\n",
            "  [Batch 5380/6245] Loss: 0.3671\n",
            "  [Batch 5390/6245] Loss: 0.1945\n",
            "  [Batch 5400/6245] Loss: 0.3246\n",
            "  [Batch 5410/6245] Loss: 0.4232\n",
            "  [Batch 5420/6245] Loss: 0.1635\n",
            "  [Batch 5430/6245] Loss: 0.3997\n",
            "  [Batch 5440/6245] Loss: 0.3697\n",
            "  [Batch 5450/6245] Loss: 0.3389\n",
            "  [Batch 5460/6245] Loss: 0.3346\n",
            "  [Batch 5470/6245] Loss: 0.5689\n",
            "  [Batch 5480/6245] Loss: 0.2544\n",
            "  [Batch 5490/6245] Loss: 0.3529\n",
            "  [Batch 5500/6245] Loss: 0.3372\n",
            "  [Batch 5510/6245] Loss: 0.1521\n",
            "  [Batch 5520/6245] Loss: 0.3154\n",
            "  [Batch 5530/6245] Loss: 0.4571\n",
            "  [Batch 5540/6245] Loss: 0.2249\n",
            "  [Batch 5550/6245] Loss: 0.2280\n",
            "  [Batch 5560/6245] Loss: 0.4326\n",
            "  [Batch 5570/6245] Loss: 0.4531\n",
            "  [Batch 5580/6245] Loss: 0.2317\n",
            "  [Batch 5590/6245] Loss: 0.3990\n",
            "  [Batch 5600/6245] Loss: 0.2659\n",
            "  [Batch 5610/6245] Loss: 0.3406\n",
            "  [Batch 5620/6245] Loss: 0.4256\n",
            "  [Batch 5630/6245] Loss: 0.1905\n",
            "  [Batch 5640/6245] Loss: 0.5313\n",
            "  [Batch 5650/6245] Loss: 0.2223\n",
            "  [Batch 5660/6245] Loss: 0.3565\n",
            "  [Batch 5670/6245] Loss: 0.3771\n",
            "  [Batch 5680/6245] Loss: 0.3302\n",
            "  [Batch 5690/6245] Loss: 0.4181\n",
            "  [Batch 5700/6245] Loss: 0.5263\n",
            "  [Batch 5710/6245] Loss: 0.3699\n",
            "  [Batch 5720/6245] Loss: 0.5221\n",
            "  [Batch 5730/6245] Loss: 0.1774\n",
            "  [Batch 5740/6245] Loss: 0.3016\n",
            "  [Batch 5750/6245] Loss: 0.3584\n",
            "  [Batch 5760/6245] Loss: 0.4651\n",
            "  [Batch 5770/6245] Loss: 0.1555\n",
            "  [Batch 5780/6245] Loss: 0.2688\n",
            "  [Batch 5790/6245] Loss: 0.1994\n",
            "  [Batch 5800/6245] Loss: 0.1851\n",
            "  [Batch 5810/6245] Loss: 0.2364\n",
            "  [Batch 5820/6245] Loss: 0.3689\n",
            "  [Batch 5830/6245] Loss: 0.4280\n",
            "  [Batch 5840/6245] Loss: 0.1875\n",
            "  [Batch 5850/6245] Loss: 0.2733\n",
            "  [Batch 5860/6245] Loss: 0.3263\n",
            "  [Batch 5870/6245] Loss: 0.2556\n",
            "  [Batch 5880/6245] Loss: 0.3364\n",
            "  [Batch 5890/6245] Loss: 0.4500\n",
            "  [Batch 5900/6245] Loss: 0.3417\n",
            "  [Batch 5910/6245] Loss: 0.4493\n",
            "  [Batch 5920/6245] Loss: 0.3523\n",
            "  [Batch 5930/6245] Loss: 0.3206\n",
            "  [Batch 5940/6245] Loss: 0.2786\n",
            "  [Batch 5950/6245] Loss: 0.3342\n",
            "  [Batch 5960/6245] Loss: 0.2508\n",
            "  [Batch 5970/6245] Loss: 0.3921\n",
            "  [Batch 5980/6245] Loss: 0.4479\n",
            "  [Batch 5990/6245] Loss: 0.2450\n",
            "  [Batch 6000/6245] Loss: 0.3499\n",
            "  [Batch 6010/6245] Loss: 0.3183\n",
            "  [Batch 6020/6245] Loss: 0.5513\n",
            "  [Batch 6030/6245] Loss: 0.2796\n",
            "  [Batch 6040/6245] Loss: 0.3192\n",
            "  [Batch 6050/6245] Loss: 0.2408\n",
            "  [Batch 6060/6245] Loss: 0.3529\n",
            "  [Batch 6070/6245] Loss: 0.3105\n",
            "  [Batch 6080/6245] Loss: 0.3335\n",
            "  [Batch 6090/6245] Loss: 0.3352\n",
            "  [Batch 6100/6245] Loss: 0.3573\n",
            "  [Batch 6110/6245] Loss: 0.1866\n",
            "  [Batch 6120/6245] Loss: 0.2867\n",
            "  [Batch 6130/6245] Loss: 0.3347\n",
            "  [Batch 6140/6245] Loss: 0.5924\n",
            "  [Batch 6150/6245] Loss: 0.2795\n",
            "  [Batch 6160/6245] Loss: 0.2639\n",
            "  [Batch 6170/6245] Loss: 0.2099\n",
            "  [Batch 6180/6245] Loss: 0.3986\n",
            "  [Batch 6190/6245] Loss: 0.3827\n",
            "  [Batch 6200/6245] Loss: 0.2044\n",
            "  [Batch 6210/6245] Loss: 0.2234\n",
            "  [Batch 6220/6245] Loss: 0.4084\n",
            "  [Batch 6230/6245] Loss: 0.2938\n",
            "  [Batch 6240/6245] Loss: 0.2320\n",
            "  [Batch 6245/6245] Loss: 0.1890\n",
            "[Epoch 7] Train Loss: 0.3180, Acc: 86.34% | Val Loss: 0.3177, Acc: 86.52%\n",
            "[INFO] Model checkpoint saved at ./models\\breastnet_epoch7.pth\n",
            "\n",
            "[Epoch 8/10]\n",
            "  [Batch 10/6245] Loss: 0.3790\n",
            "  [Batch 20/6245] Loss: 0.3840\n",
            "  [Batch 30/6245] Loss: 0.2992\n",
            "  [Batch 40/6245] Loss: 0.4605\n",
            "  [Batch 50/6245] Loss: 0.3098\n",
            "  [Batch 60/6245] Loss: 0.1916\n",
            "  [Batch 70/6245] Loss: 0.3355\n",
            "  [Batch 80/6245] Loss: 0.3606\n",
            "  [Batch 90/6245] Loss: 0.3473\n",
            "  [Batch 100/6245] Loss: 0.3918\n",
            "  [Batch 110/6245] Loss: 0.3453\n",
            "  [Batch 120/6245] Loss: 0.2296\n",
            "  [Batch 130/6245] Loss: 0.4692\n",
            "  [Batch 140/6245] Loss: 0.3313\n",
            "  [Batch 150/6245] Loss: 0.2645\n",
            "  [Batch 160/6245] Loss: 0.4430\n",
            "  [Batch 170/6245] Loss: 0.2571\n",
            "  [Batch 180/6245] Loss: 0.3837\n",
            "  [Batch 190/6245] Loss: 0.3168\n",
            "  [Batch 200/6245] Loss: 0.2719\n",
            "  [Batch 210/6245] Loss: 0.3132\n",
            "  [Batch 220/6245] Loss: 0.3531\n",
            "  [Batch 230/6245] Loss: 0.1976\n",
            "  [Batch 240/6245] Loss: 0.2726\n",
            "  [Batch 250/6245] Loss: 0.5768\n",
            "  [Batch 260/6245] Loss: 0.4259\n",
            "  [Batch 270/6245] Loss: 0.3040\n",
            "  [Batch 280/6245] Loss: 0.2947\n",
            "  [Batch 290/6245] Loss: 0.1312\n",
            "  [Batch 300/6245] Loss: 0.3165\n",
            "  [Batch 310/6245] Loss: 0.2638\n",
            "  [Batch 320/6245] Loss: 0.3896\n",
            "  [Batch 330/6245] Loss: 0.2425\n",
            "  [Batch 340/6245] Loss: 0.3175\n",
            "  [Batch 350/6245] Loss: 0.4581\n",
            "  [Batch 360/6245] Loss: 0.3826\n",
            "  [Batch 370/6245] Loss: 0.2945\n",
            "  [Batch 380/6245] Loss: 0.3167\n",
            "  [Batch 390/6245] Loss: 0.5514\n",
            "  [Batch 400/6245] Loss: 0.2321\n",
            "  [Batch 410/6245] Loss: 0.4118\n",
            "  [Batch 420/6245] Loss: 0.2920\n",
            "  [Batch 430/6245] Loss: 0.2178\n",
            "  [Batch 440/6245] Loss: 0.2635\n",
            "  [Batch 450/6245] Loss: 0.3047\n",
            "  [Batch 460/6245] Loss: 0.3014\n",
            "  [Batch 470/6245] Loss: 0.3671\n",
            "  [Batch 480/6245] Loss: 0.3454\n",
            "  [Batch 490/6245] Loss: 0.2601\n",
            "  [Batch 500/6245] Loss: 0.2771\n",
            "  [Batch 510/6245] Loss: 0.1776\n",
            "  [Batch 520/6245] Loss: 0.1790\n",
            "  [Batch 530/6245] Loss: 0.1557\n",
            "  [Batch 540/6245] Loss: 0.2375\n",
            "  [Batch 550/6245] Loss: 0.4177\n",
            "  [Batch 560/6245] Loss: 0.2622\n",
            "  [Batch 570/6245] Loss: 0.2671\n",
            "  [Batch 580/6245] Loss: 0.2462\n",
            "  [Batch 590/6245] Loss: 0.1820\n",
            "  [Batch 600/6245] Loss: 0.2953\n",
            "  [Batch 610/6245] Loss: 0.1786\n",
            "  [Batch 620/6245] Loss: 0.2116\n",
            "  [Batch 630/6245] Loss: 0.2306\n",
            "  [Batch 640/6245] Loss: 0.2749\n",
            "  [Batch 650/6245] Loss: 0.3537\n",
            "  [Batch 660/6245] Loss: 0.6591\n",
            "  [Batch 670/6245] Loss: 0.3158\n",
            "  [Batch 680/6245] Loss: 0.3125\n",
            "  [Batch 690/6245] Loss: 0.3949\n",
            "  [Batch 700/6245] Loss: 0.2549\n",
            "  [Batch 710/6245] Loss: 0.3519\n",
            "  [Batch 720/6245] Loss: 0.3196\n",
            "  [Batch 730/6245] Loss: 0.1155\n",
            "  [Batch 740/6245] Loss: 0.2499\n",
            "  [Batch 750/6245] Loss: 0.2109\n",
            "  [Batch 760/6245] Loss: 0.2179\n",
            "  [Batch 770/6245] Loss: 0.2002\n",
            "  [Batch 780/6245] Loss: 0.3237\n",
            "  [Batch 790/6245] Loss: 0.2050\n",
            "  [Batch 800/6245] Loss: 0.2905\n",
            "  [Batch 810/6245] Loss: 0.3819\n",
            "  [Batch 820/6245] Loss: 0.1901\n",
            "  [Batch 830/6245] Loss: 0.3539\n",
            "  [Batch 840/6245] Loss: 0.3755\n",
            "  [Batch 850/6245] Loss: 0.2973\n",
            "  [Batch 860/6245] Loss: 0.2880\n",
            "  [Batch 870/6245] Loss: 0.2535\n",
            "  [Batch 880/6245] Loss: 0.2257\n",
            "  [Batch 890/6245] Loss: 0.2735\n",
            "  [Batch 900/6245] Loss: 0.3289\n",
            "  [Batch 910/6245] Loss: 0.3571\n",
            "  [Batch 920/6245] Loss: 0.2280\n",
            "  [Batch 930/6245] Loss: 0.2271\n",
            "  [Batch 940/6245] Loss: 0.3518\n",
            "  [Batch 950/6245] Loss: 0.1881\n",
            "  [Batch 960/6245] Loss: 0.2993\n",
            "  [Batch 970/6245] Loss: 0.1460\n",
            "  [Batch 980/6245] Loss: 0.2561\n",
            "  [Batch 990/6245] Loss: 0.2414\n",
            "  [Batch 1000/6245] Loss: 0.2677\n",
            "  [Batch 1010/6245] Loss: 0.3223\n",
            "  [Batch 1020/6245] Loss: 0.2684\n",
            "  [Batch 1030/6245] Loss: 0.3335\n",
            "  [Batch 1040/6245] Loss: 0.3024\n",
            "  [Batch 1050/6245] Loss: 0.2940\n",
            "  [Batch 1060/6245] Loss: 0.4118\n",
            "  [Batch 1070/6245] Loss: 0.4896\n",
            "  [Batch 1080/6245] Loss: 0.3929\n",
            "  [Batch 1090/6245] Loss: 0.2828\n",
            "  [Batch 1100/6245] Loss: 0.1852\n",
            "  [Batch 1110/6245] Loss: 0.2577\n",
            "  [Batch 1120/6245] Loss: 0.2179\n",
            "  [Batch 1130/6245] Loss: 0.3053\n",
            "  [Batch 1140/6245] Loss: 0.2510\n",
            "  [Batch 1150/6245] Loss: 0.3658\n",
            "  [Batch 1160/6245] Loss: 0.2308\n",
            "  [Batch 1170/6245] Loss: 0.2219\n",
            "  [Batch 1180/6245] Loss: 0.3113\n",
            "  [Batch 1190/6245] Loss: 0.8539\n",
            "  [Batch 1200/6245] Loss: 0.3539\n",
            "  [Batch 1210/6245] Loss: 0.3802\n",
            "  [Batch 1220/6245] Loss: 0.3892\n",
            "  [Batch 1230/6245] Loss: 0.1813\n",
            "  [Batch 1240/6245] Loss: 0.2865\n",
            "  [Batch 1250/6245] Loss: 0.3264\n",
            "  [Batch 1260/6245] Loss: 0.2731\n",
            "  [Batch 1270/6245] Loss: 0.2404\n",
            "  [Batch 1280/6245] Loss: 0.3799\n",
            "  [Batch 1290/6245] Loss: 0.3898\n",
            "  [Batch 1300/6245] Loss: 0.3507\n",
            "  [Batch 1310/6245] Loss: 0.2994\n",
            "  [Batch 1320/6245] Loss: 0.3791\n",
            "  [Batch 1330/6245] Loss: 0.2459\n",
            "  [Batch 1340/6245] Loss: 0.3438\n",
            "  [Batch 1350/6245] Loss: 0.3261\n",
            "  [Batch 1360/6245] Loss: 0.3135\n",
            "  [Batch 1370/6245] Loss: 0.2356\n",
            "  [Batch 1380/6245] Loss: 0.4419\n",
            "  [Batch 1390/6245] Loss: 0.2923\n",
            "  [Batch 1400/6245] Loss: 0.2696\n",
            "  [Batch 1410/6245] Loss: 0.4266\n",
            "  [Batch 1420/6245] Loss: 0.1771\n",
            "  [Batch 1430/6245] Loss: 0.3374\n",
            "  [Batch 1440/6245] Loss: 0.6125\n",
            "  [Batch 1450/6245] Loss: 0.3050\n",
            "  [Batch 1460/6245] Loss: 0.3523\n",
            "  [Batch 1470/6245] Loss: 0.4030\n",
            "  [Batch 1480/6245] Loss: 0.1978\n",
            "  [Batch 1490/6245] Loss: 0.3884\n",
            "  [Batch 1500/6245] Loss: 0.4650\n",
            "  [Batch 1510/6245] Loss: 0.1955\n",
            "  [Batch 1520/6245] Loss: 0.2616\n",
            "  [Batch 1530/6245] Loss: 0.4020\n",
            "  [Batch 1540/6245] Loss: 0.2244\n",
            "  [Batch 1550/6245] Loss: 0.2391\n",
            "  [Batch 1560/6245] Loss: 0.2254\n",
            "  [Batch 1570/6245] Loss: 0.2586\n",
            "  [Batch 1580/6245] Loss: 0.2417\n",
            "  [Batch 1590/6245] Loss: 0.2434\n",
            "  [Batch 1600/6245] Loss: 0.3628\n",
            "  [Batch 1610/6245] Loss: 0.3506\n",
            "  [Batch 1620/6245] Loss: 0.1541\n",
            "  [Batch 1630/6245] Loss: 0.2955\n",
            "  [Batch 1640/6245] Loss: 0.3630\n",
            "  [Batch 1650/6245] Loss: 0.2979\n",
            "  [Batch 1660/6245] Loss: 0.2833\n",
            "  [Batch 1670/6245] Loss: 0.3964\n",
            "  [Batch 1680/6245] Loss: 0.2381\n",
            "  [Batch 1690/6245] Loss: 0.2231\n",
            "  [Batch 1700/6245] Loss: 0.1902\n",
            "  [Batch 1710/6245] Loss: 0.2098\n",
            "  [Batch 1720/6245] Loss: 0.3253\n",
            "  [Batch 1730/6245] Loss: 0.2292\n",
            "  [Batch 1740/6245] Loss: 0.2676\n",
            "  [Batch 1750/6245] Loss: 0.3179\n",
            "  [Batch 1760/6245] Loss: 0.5188\n",
            "  [Batch 1770/6245] Loss: 0.2126\n",
            "  [Batch 1780/6245] Loss: 0.4652\n",
            "  [Batch 1790/6245] Loss: 0.4115\n",
            "  [Batch 1800/6245] Loss: 0.4986\n",
            "  [Batch 1810/6245] Loss: 0.2714\n",
            "  [Batch 1820/6245] Loss: 0.3806\n",
            "  [Batch 1830/6245] Loss: 0.4231\n",
            "  [Batch 1840/6245] Loss: 0.2907\n",
            "  [Batch 1850/6245] Loss: 0.3341\n",
            "  [Batch 1860/6245] Loss: 0.5222\n",
            "  [Batch 1870/6245] Loss: 0.2741\n",
            "  [Batch 1880/6245] Loss: 0.4880\n",
            "  [Batch 1890/6245] Loss: 0.2124\n",
            "  [Batch 1900/6245] Loss: 0.2983\n",
            "  [Batch 1910/6245] Loss: 0.1552\n",
            "  [Batch 1920/6245] Loss: 0.1667\n",
            "  [Batch 1930/6245] Loss: 0.3199\n",
            "  [Batch 1940/6245] Loss: 0.2470\n",
            "  [Batch 1950/6245] Loss: 0.3177\n",
            "  [Batch 1960/6245] Loss: 0.1768\n",
            "  [Batch 1970/6245] Loss: 0.1915\n",
            "  [Batch 1980/6245] Loss: 0.3882\n",
            "  [Batch 1990/6245] Loss: 0.2236\n",
            "  [Batch 2000/6245] Loss: 0.2461\n",
            "  [Batch 2010/6245] Loss: 0.3199\n",
            "  [Batch 2020/6245] Loss: 0.3887\n",
            "  [Batch 2030/6245] Loss: 0.4184\n",
            "  [Batch 2040/6245] Loss: 0.4700\n",
            "  [Batch 2050/6245] Loss: 0.7137\n",
            "  [Batch 2060/6245] Loss: 0.4104\n",
            "  [Batch 2070/6245] Loss: 0.2426\n",
            "  [Batch 2080/6245] Loss: 0.4503\n",
            "  [Batch 2090/6245] Loss: 0.2500\n",
            "  [Batch 2100/6245] Loss: 0.3403\n",
            "  [Batch 2110/6245] Loss: 0.2636\n",
            "  [Batch 2120/6245] Loss: 0.5011\n",
            "  [Batch 2130/6245] Loss: 0.1640\n",
            "  [Batch 2140/6245] Loss: 0.3809\n",
            "  [Batch 2150/6245] Loss: 0.3050\n",
            "  [Batch 2160/6245] Loss: 0.4002\n",
            "  [Batch 2170/6245] Loss: 0.3408\n",
            "  [Batch 2180/6245] Loss: 0.2876\n",
            "  [Batch 2190/6245] Loss: 0.2584\n",
            "  [Batch 2200/6245] Loss: 0.4546\n",
            "  [Batch 2210/6245] Loss: 0.4136\n",
            "  [Batch 2220/6245] Loss: 0.3505\n",
            "  [Batch 2230/6245] Loss: 0.4935\n",
            "  [Batch 2240/6245] Loss: 0.3659\n",
            "  [Batch 2250/6245] Loss: 0.2411\n",
            "  [Batch 2260/6245] Loss: 0.4035\n",
            "  [Batch 2270/6245] Loss: 0.3596\n",
            "  [Batch 2280/6245] Loss: 0.2431\n",
            "  [Batch 2290/6245] Loss: 0.3570\n",
            "  [Batch 2300/6245] Loss: 0.3681\n",
            "  [Batch 2310/6245] Loss: 0.2004\n",
            "  [Batch 2320/6245] Loss: 0.4099\n",
            "  [Batch 2330/6245] Loss: 0.3517\n",
            "  [Batch 2340/6245] Loss: 0.2096\n",
            "  [Batch 2350/6245] Loss: 0.2862\n",
            "  [Batch 2360/6245] Loss: 0.3354\n",
            "  [Batch 2370/6245] Loss: 0.4546\n",
            "  [Batch 2380/6245] Loss: 0.1317\n",
            "  [Batch 2390/6245] Loss: 0.3375\n",
            "  [Batch 2400/6245] Loss: 0.3958\n",
            "  [Batch 2410/6245] Loss: 0.1836\n",
            "  [Batch 2420/6245] Loss: 0.0992\n",
            "  [Batch 2430/6245] Loss: 0.4522\n",
            "  [Batch 2440/6245] Loss: 0.2368\n",
            "  [Batch 2450/6245] Loss: 0.1447\n",
            "  [Batch 2460/6245] Loss: 0.4101\n",
            "  [Batch 2470/6245] Loss: 0.3368\n",
            "  [Batch 2480/6245] Loss: 0.3001\n",
            "  [Batch 2490/6245] Loss: 0.1708\n",
            "  [Batch 2500/6245] Loss: 0.3224\n",
            "  [Batch 2510/6245] Loss: 0.2200\n",
            "  [Batch 2520/6245] Loss: 0.2241\n",
            "  [Batch 2530/6245] Loss: 0.4286\n",
            "  [Batch 2540/6245] Loss: 0.3262\n",
            "  [Batch 2550/6245] Loss: 0.3939\n",
            "  [Batch 2560/6245] Loss: 0.3266\n",
            "  [Batch 2570/6245] Loss: 0.4566\n",
            "  [Batch 2580/6245] Loss: 0.2269\n",
            "  [Batch 2590/6245] Loss: 0.3070\n",
            "  [Batch 2600/6245] Loss: 0.4262\n",
            "  [Batch 2610/6245] Loss: 0.3334\n",
            "  [Batch 2620/6245] Loss: 0.2827\n",
            "  [Batch 2630/6245] Loss: 0.3048\n",
            "  [Batch 2640/6245] Loss: 0.2859\n",
            "  [Batch 2650/6245] Loss: 0.4681\n",
            "  [Batch 2660/6245] Loss: 0.2893\n",
            "  [Batch 2670/6245] Loss: 0.2777\n",
            "  [Batch 2680/6245] Loss: 0.2431\n",
            "  [Batch 2690/6245] Loss: 0.2745\n",
            "  [Batch 2700/6245] Loss: 0.5414\n",
            "  [Batch 2710/6245] Loss: 0.2353\n",
            "  [Batch 2720/6245] Loss: 0.2997\n",
            "  [Batch 2730/6245] Loss: 0.2449\n",
            "  [Batch 2740/6245] Loss: 0.3664\n",
            "  [Batch 2750/6245] Loss: 0.2711\n",
            "  [Batch 2760/6245] Loss: 0.5164\n",
            "  [Batch 2770/6245] Loss: 0.1939\n",
            "  [Batch 2780/6245] Loss: 0.3617\n",
            "  [Batch 2790/6245] Loss: 0.3941\n",
            "  [Batch 2800/6245] Loss: 0.2836\n",
            "  [Batch 2810/6245] Loss: 0.5238\n",
            "  [Batch 2820/6245] Loss: 0.5019\n",
            "  [Batch 2830/6245] Loss: 0.3169\n",
            "  [Batch 2840/6245] Loss: 0.2370\n",
            "  [Batch 2850/6245] Loss: 0.2404\n",
            "  [Batch 2860/6245] Loss: 0.3305\n",
            "  [Batch 2870/6245] Loss: 0.4839\n",
            "  [Batch 2880/6245] Loss: 0.4305\n",
            "  [Batch 2890/6245] Loss: 0.1975\n",
            "  [Batch 2900/6245] Loss: 0.1670\n",
            "  [Batch 2910/6245] Loss: 0.2889\n",
            "  [Batch 2920/6245] Loss: 0.2921\n",
            "  [Batch 2930/6245] Loss: 0.2187\n",
            "  [Batch 2940/6245] Loss: 0.2458\n",
            "  [Batch 2950/6245] Loss: 0.2635\n",
            "  [Batch 2960/6245] Loss: 0.1076\n",
            "  [Batch 2970/6245] Loss: 0.1756\n",
            "  [Batch 2980/6245] Loss: 0.3559\n",
            "  [Batch 2990/6245] Loss: 0.3202\n",
            "  [Batch 3000/6245] Loss: 0.3533\n",
            "  [Batch 3010/6245] Loss: 0.1519\n",
            "  [Batch 3020/6245] Loss: 0.3047\n",
            "  [Batch 3030/6245] Loss: 0.3194\n",
            "  [Batch 3040/6245] Loss: 0.2210\n",
            "  [Batch 3050/6245] Loss: 0.2078\n",
            "  [Batch 3060/6245] Loss: 0.4698\n",
            "  [Batch 3070/6245] Loss: 0.2103\n",
            "  [Batch 3080/6245] Loss: 0.3046\n",
            "  [Batch 3090/6245] Loss: 0.1408\n",
            "  [Batch 3100/6245] Loss: 0.3279\n",
            "  [Batch 3110/6245] Loss: 0.5210\n",
            "  [Batch 3120/6245] Loss: 0.1496\n",
            "  [Batch 3130/6245] Loss: 0.1657\n",
            "  [Batch 3140/6245] Loss: 0.3943\n",
            "  [Batch 3150/6245] Loss: 0.3223\n",
            "  [Batch 3160/6245] Loss: 0.4178\n",
            "  [Batch 3170/6245] Loss: 0.4991\n",
            "  [Batch 3180/6245] Loss: 0.2795\n",
            "  [Batch 3190/6245] Loss: 0.3601\n",
            "  [Batch 3200/6245] Loss: 0.5157\n",
            "  [Batch 3210/6245] Loss: 0.4241\n",
            "  [Batch 3220/6245] Loss: 0.3329\n",
            "  [Batch 3230/6245] Loss: 0.1746\n",
            "  [Batch 3240/6245] Loss: 0.2336\n",
            "  [Batch 3250/6245] Loss: 0.5946\n",
            "  [Batch 3260/6245] Loss: 0.2308\n",
            "  [Batch 3270/6245] Loss: 0.2765\n",
            "  [Batch 3280/6245] Loss: 0.4430\n",
            "  [Batch 3290/6245] Loss: 0.2908\n",
            "  [Batch 3300/6245] Loss: 0.4212\n",
            "  [Batch 3310/6245] Loss: 0.2871\n",
            "  [Batch 3320/6245] Loss: 0.3694\n",
            "  [Batch 3330/6245] Loss: 0.3492\n",
            "  [Batch 3340/6245] Loss: 0.1086\n",
            "  [Batch 3350/6245] Loss: 0.5090\n",
            "  [Batch 3360/6245] Loss: 0.5386\n",
            "  [Batch 3370/6245] Loss: 0.2069\n",
            "  [Batch 3380/6245] Loss: 0.3943\n",
            "  [Batch 3390/6245] Loss: 0.4754\n",
            "  [Batch 3400/6245] Loss: 0.2894\n",
            "  [Batch 3410/6245] Loss: 0.2431\n",
            "  [Batch 3420/6245] Loss: 0.3433\n",
            "  [Batch 3430/6245] Loss: 0.3055\n",
            "  [Batch 3440/6245] Loss: 0.2154\n",
            "  [Batch 3450/6245] Loss: 0.3214\n",
            "  [Batch 3460/6245] Loss: 0.2759\n",
            "  [Batch 3470/6245] Loss: 0.2700\n",
            "  [Batch 3480/6245] Loss: 0.5045\n",
            "  [Batch 3490/6245] Loss: 0.2665\n",
            "  [Batch 3500/6245] Loss: 0.4096\n",
            "  [Batch 3510/6245] Loss: 0.4772\n",
            "  [Batch 3520/6245] Loss: 0.3133\n",
            "  [Batch 3530/6245] Loss: 0.2283\n",
            "  [Batch 3540/6245] Loss: 0.3801\n",
            "  [Batch 3550/6245] Loss: 0.2914\n",
            "  [Batch 3560/6245] Loss: 0.3237\n",
            "  [Batch 3570/6245] Loss: 0.3283\n",
            "  [Batch 3580/6245] Loss: 0.2283\n",
            "  [Batch 3590/6245] Loss: 0.3052\n",
            "  [Batch 3600/6245] Loss: 0.2002\n",
            "  [Batch 3610/6245] Loss: 0.3080\n",
            "  [Batch 3620/6245] Loss: 0.2912\n",
            "  [Batch 3630/6245] Loss: 0.3846\n",
            "  [Batch 3640/6245] Loss: 0.2013\n",
            "  [Batch 3650/6245] Loss: 0.4139\n",
            "  [Batch 3660/6245] Loss: 0.3377\n",
            "  [Batch 3670/6245] Loss: 0.3554\n",
            "  [Batch 3680/6245] Loss: 0.4142\n",
            "  [Batch 3690/6245] Loss: 0.2586\n",
            "  [Batch 3700/6245] Loss: 0.2879\n",
            "  [Batch 3710/6245] Loss: 0.1920\n",
            "  [Batch 3720/6245] Loss: 0.2299\n",
            "  [Batch 3730/6245] Loss: 0.3133\n",
            "  [Batch 3740/6245] Loss: 0.2033\n",
            "  [Batch 3750/6245] Loss: 0.3632\n",
            "  [Batch 3760/6245] Loss: 0.3867\n",
            "  [Batch 3770/6245] Loss: 0.3586\n",
            "  [Batch 3780/6245] Loss: 0.3663\n",
            "  [Batch 3790/6245] Loss: 0.3738\n",
            "  [Batch 3800/6245] Loss: 0.4254\n",
            "  [Batch 3810/6245] Loss: 0.2620\n",
            "  [Batch 3820/6245] Loss: 0.3515\n",
            "  [Batch 3830/6245] Loss: 0.3990\n",
            "  [Batch 3840/6245] Loss: 0.2816\n",
            "  [Batch 3850/6245] Loss: 0.4928\n",
            "  [Batch 3860/6245] Loss: 0.3178\n",
            "  [Batch 3870/6245] Loss: 0.4842\n",
            "  [Batch 3880/6245] Loss: 0.3892\n",
            "  [Batch 3890/6245] Loss: 0.2843\n",
            "  [Batch 3900/6245] Loss: 0.2291\n",
            "  [Batch 3910/6245] Loss: 0.3237\n",
            "  [Batch 3920/6245] Loss: 0.2371\n",
            "  [Batch 3930/6245] Loss: 0.4400\n",
            "  [Batch 3940/6245] Loss: 0.1685\n",
            "  [Batch 3950/6245] Loss: 0.2244\n",
            "  [Batch 3960/6245] Loss: 0.4083\n",
            "  [Batch 3970/6245] Loss: 0.3751\n",
            "  [Batch 3980/6245] Loss: 0.2457\n",
            "  [Batch 3990/6245] Loss: 0.2699\n",
            "  [Batch 4000/6245] Loss: 0.1634\n",
            "  [Batch 4010/6245] Loss: 0.3300\n",
            "  [Batch 4020/6245] Loss: 0.3162\n",
            "  [Batch 4030/6245] Loss: 0.4192\n",
            "  [Batch 4040/6245] Loss: 0.3744\n",
            "  [Batch 4050/6245] Loss: 0.5481\n",
            "  [Batch 4060/6245] Loss: 0.3277\n",
            "  [Batch 4070/6245] Loss: 0.2442\n",
            "  [Batch 4080/6245] Loss: 0.4426\n",
            "  [Batch 4090/6245] Loss: 0.2131\n",
            "  [Batch 4100/6245] Loss: 0.3039\n",
            "  [Batch 4110/6245] Loss: 0.2476\n",
            "  [Batch 4120/6245] Loss: 0.1553\n",
            "  [Batch 4130/6245] Loss: 0.2140\n",
            "  [Batch 4140/6245] Loss: 0.3217\n",
            "  [Batch 4150/6245] Loss: 0.4869\n",
            "  [Batch 4160/6245] Loss: 0.4194\n",
            "  [Batch 4170/6245] Loss: 0.3857\n",
            "  [Batch 4180/6245] Loss: 0.2956\n",
            "  [Batch 4190/6245] Loss: 0.3179\n",
            "  [Batch 4200/6245] Loss: 0.2329\n",
            "  [Batch 4210/6245] Loss: 0.4985\n",
            "  [Batch 4220/6245] Loss: 0.2958\n",
            "  [Batch 4230/6245] Loss: 0.3139\n",
            "  [Batch 4240/6245] Loss: 0.2978\n",
            "  [Batch 4250/6245] Loss: 0.1976\n",
            "  [Batch 4260/6245] Loss: 0.2603\n",
            "  [Batch 4270/6245] Loss: 0.2500\n",
            "  [Batch 4280/6245] Loss: 0.2536\n",
            "  [Batch 4290/6245] Loss: 0.2459\n",
            "  [Batch 4300/6245] Loss: 0.1928\n",
            "  [Batch 4310/6245] Loss: 0.2717\n",
            "  [Batch 4320/6245] Loss: 0.3274\n",
            "  [Batch 4330/6245] Loss: 0.3337\n",
            "  [Batch 4340/6245] Loss: 0.2589\n",
            "  [Batch 4350/6245] Loss: 0.3453\n",
            "  [Batch 4360/6245] Loss: 0.2516\n",
            "  [Batch 4370/6245] Loss: 0.2777\n",
            "  [Batch 4380/6245] Loss: 0.1865\n",
            "  [Batch 4390/6245] Loss: 0.3607\n",
            "  [Batch 4400/6245] Loss: 0.2241\n",
            "  [Batch 4410/6245] Loss: 0.2761\n",
            "  [Batch 4420/6245] Loss: 0.1996\n",
            "  [Batch 4430/6245] Loss: 0.4371\n",
            "  [Batch 4440/6245] Loss: 0.1937\n",
            "  [Batch 4450/6245] Loss: 0.2726\n",
            "  [Batch 4460/6245] Loss: 0.3293\n",
            "  [Batch 4470/6245] Loss: 0.3984\n",
            "  [Batch 4480/6245] Loss: 0.2735\n",
            "  [Batch 4490/6245] Loss: 0.3145\n",
            "  [Batch 4500/6245] Loss: 0.4752\n",
            "  [Batch 4510/6245] Loss: 0.1808\n",
            "  [Batch 4520/6245] Loss: 0.2500\n",
            "  [Batch 4530/6245] Loss: 0.5071\n",
            "  [Batch 4540/6245] Loss: 0.3451\n",
            "  [Batch 4550/6245] Loss: 0.2661\n",
            "  [Batch 4560/6245] Loss: 0.2681\n",
            "  [Batch 4570/6245] Loss: 0.2414\n",
            "  [Batch 4580/6245] Loss: 0.3071\n",
            "  [Batch 4590/6245] Loss: 0.3578\n",
            "  [Batch 4600/6245] Loss: 0.2643\n",
            "  [Batch 4610/6245] Loss: 0.2009\n",
            "  [Batch 4620/6245] Loss: 0.3254\n",
            "  [Batch 4630/6245] Loss: 0.3660\n",
            "  [Batch 4640/6245] Loss: 0.3608\n",
            "  [Batch 4650/6245] Loss: 0.2822\n",
            "  [Batch 4660/6245] Loss: 0.3345\n",
            "  [Batch 4670/6245] Loss: 0.4901\n",
            "  [Batch 4680/6245] Loss: 0.1602\n",
            "  [Batch 4690/6245] Loss: 0.1564\n",
            "  [Batch 4700/6245] Loss: 0.2749\n",
            "  [Batch 4710/6245] Loss: 0.3600\n",
            "  [Batch 4720/6245] Loss: 0.3888\n",
            "  [Batch 4730/6245] Loss: 0.2610\n",
            "  [Batch 4740/6245] Loss: 0.5209\n",
            "  [Batch 4750/6245] Loss: 0.4541\n",
            "  [Batch 4760/6245] Loss: 0.1880\n",
            "  [Batch 4770/6245] Loss: 0.6084\n",
            "  [Batch 4780/6245] Loss: 0.1285\n",
            "  [Batch 4790/6245] Loss: 0.4731\n",
            "  [Batch 4800/6245] Loss: 0.1313\n",
            "  [Batch 4810/6245] Loss: 0.2687\n",
            "  [Batch 4820/6245] Loss: 0.3816\n",
            "  [Batch 4830/6245] Loss: 0.3267\n",
            "  [Batch 4840/6245] Loss: 0.3242\n",
            "  [Batch 4850/6245] Loss: 0.3261\n",
            "  [Batch 4860/6245] Loss: 0.3248\n",
            "  [Batch 4870/6245] Loss: 0.4500\n",
            "  [Batch 4880/6245] Loss: 0.2905\n",
            "  [Batch 4890/6245] Loss: 0.3854\n",
            "  [Batch 4900/6245] Loss: 0.3302\n",
            "  [Batch 4910/6245] Loss: 0.5926\n",
            "  [Batch 4920/6245] Loss: 0.2696\n",
            "  [Batch 4930/6245] Loss: 0.2446\n",
            "  [Batch 4940/6245] Loss: 0.1647\n",
            "  [Batch 4950/6245] Loss: 0.2051\n",
            "  [Batch 4960/6245] Loss: 0.3621\n",
            "  [Batch 4970/6245] Loss: 0.2441\n",
            "  [Batch 4980/6245] Loss: 0.3427\n",
            "  [Batch 4990/6245] Loss: 0.4631\n",
            "  [Batch 5000/6245] Loss: 0.3492\n",
            "  [Batch 5010/6245] Loss: 0.2202\n",
            "  [Batch 5020/6245] Loss: 0.3117\n",
            "  [Batch 5030/6245] Loss: 0.2399\n",
            "  [Batch 5040/6245] Loss: 0.2809\n",
            "  [Batch 5050/6245] Loss: 0.2280\n",
            "  [Batch 5060/6245] Loss: 0.3403\n",
            "  [Batch 5070/6245] Loss: 0.4078\n",
            "  [Batch 5080/6245] Loss: 0.2721\n",
            "  [Batch 5090/6245] Loss: 0.4541\n",
            "  [Batch 5100/6245] Loss: 0.4642\n",
            "  [Batch 5110/6245] Loss: 0.2571\n",
            "  [Batch 5120/6245] Loss: 0.3233\n",
            "  [Batch 5130/6245] Loss: 0.2943\n",
            "  [Batch 5140/6245] Loss: 0.1921\n",
            "  [Batch 5150/6245] Loss: 0.4052\n",
            "  [Batch 5160/6245] Loss: 0.4200\n",
            "  [Batch 5170/6245] Loss: 0.2321\n",
            "  [Batch 5180/6245] Loss: 0.3581\n",
            "  [Batch 5190/6245] Loss: 0.6644\n",
            "  [Batch 5200/6245] Loss: 0.3365\n",
            "  [Batch 5210/6245] Loss: 0.2788\n",
            "  [Batch 5220/6245] Loss: 0.2553\n",
            "  [Batch 5230/6245] Loss: 0.1795\n",
            "  [Batch 5240/6245] Loss: 0.4924\n",
            "  [Batch 5250/6245] Loss: 0.3254\n",
            "  [Batch 5260/6245] Loss: 0.3239\n",
            "  [Batch 5270/6245] Loss: 0.3008\n",
            "  [Batch 5280/6245] Loss: 0.3009\n",
            "  [Batch 5290/6245] Loss: 0.1815\n",
            "  [Batch 5300/6245] Loss: 0.3349\n",
            "  [Batch 5310/6245] Loss: 0.2337\n",
            "  [Batch 5320/6245] Loss: 0.2349\n",
            "  [Batch 5330/6245] Loss: 0.3373\n",
            "  [Batch 5340/6245] Loss: 0.3997\n",
            "  [Batch 5350/6245] Loss: 0.3291\n",
            "  [Batch 5360/6245] Loss: 0.2514\n",
            "  [Batch 5370/6245] Loss: 0.2681\n",
            "  [Batch 5380/6245] Loss: 0.1847\n",
            "  [Batch 5390/6245] Loss: 0.1823\n",
            "  [Batch 5400/6245] Loss: 0.3164\n",
            "  [Batch 5410/6245] Loss: 0.3234\n",
            "  [Batch 5420/6245] Loss: 0.4917\n",
            "  [Batch 5430/6245] Loss: 0.3131\n",
            "  [Batch 5440/6245] Loss: 0.1761\n",
            "  [Batch 5450/6245] Loss: 0.3465\n",
            "  [Batch 5460/6245] Loss: 0.2661\n",
            "  [Batch 5470/6245] Loss: 0.3677\n",
            "  [Batch 5480/6245] Loss: 0.3376\n",
            "  [Batch 5490/6245] Loss: 0.4548\n",
            "  [Batch 5500/6245] Loss: 0.3866\n",
            "  [Batch 5510/6245] Loss: 0.3673\n",
            "  [Batch 5520/6245] Loss: 0.4690\n",
            "  [Batch 5530/6245] Loss: 0.2374\n",
            "  [Batch 5540/6245] Loss: 0.1987\n",
            "  [Batch 5550/6245] Loss: 0.2402\n",
            "  [Batch 5560/6245] Loss: 0.3201\n",
            "  [Batch 5570/6245] Loss: 0.2135\n",
            "  [Batch 5580/6245] Loss: 0.3712\n",
            "  [Batch 5590/6245] Loss: 0.2347\n",
            "  [Batch 5600/6245] Loss: 0.3518\n",
            "  [Batch 5610/6245] Loss: 0.2413\n",
            "  [Batch 5620/6245] Loss: 0.2393\n",
            "  [Batch 5630/6245] Loss: 0.2093\n",
            "  [Batch 5640/6245] Loss: 0.1630\n",
            "  [Batch 5650/6245] Loss: 0.2903\n",
            "  [Batch 5660/6245] Loss: 0.3173\n",
            "  [Batch 5670/6245] Loss: 0.3432\n",
            "  [Batch 5680/6245] Loss: 0.5794\n",
            "  [Batch 5690/6245] Loss: 0.3416\n",
            "  [Batch 5700/6245] Loss: 0.4834\n",
            "  [Batch 5710/6245] Loss: 0.2760\n",
            "  [Batch 5720/6245] Loss: 0.2349\n",
            "  [Batch 5730/6245] Loss: 0.2799\n",
            "  [Batch 5740/6245] Loss: 0.4650\n",
            "  [Batch 5750/6245] Loss: 0.3607\n",
            "  [Batch 5760/6245] Loss: 0.3132\n",
            "  [Batch 5770/6245] Loss: 0.2428\n",
            "  [Batch 5780/6245] Loss: 0.4658\n",
            "  [Batch 5790/6245] Loss: 0.1909\n",
            "  [Batch 5800/6245] Loss: 0.4278\n",
            "  [Batch 5810/6245] Loss: 0.3591\n",
            "  [Batch 5820/6245] Loss: 0.2885\n",
            "  [Batch 5830/6245] Loss: 0.4679\n",
            "  [Batch 5840/6245] Loss: 0.2566\n",
            "  [Batch 5850/6245] Loss: 0.2589\n",
            "  [Batch 5860/6245] Loss: 0.4778\n",
            "  [Batch 5870/6245] Loss: 0.6035\n",
            "  [Batch 5880/6245] Loss: 0.4111\n",
            "  [Batch 5890/6245] Loss: 0.3070\n",
            "  [Batch 5900/6245] Loss: 0.3923\n",
            "  [Batch 5910/6245] Loss: 0.3065\n",
            "  [Batch 5920/6245] Loss: 0.2921\n",
            "  [Batch 5930/6245] Loss: 0.3304\n",
            "  [Batch 5940/6245] Loss: 0.2867\n",
            "  [Batch 5950/6245] Loss: 0.6440\n",
            "  [Batch 5960/6245] Loss: 0.3324\n",
            "  [Batch 5970/6245] Loss: 0.3399\n",
            "  [Batch 5980/6245] Loss: 0.4017\n",
            "  [Batch 5990/6245] Loss: 0.1292\n",
            "  [Batch 6000/6245] Loss: 0.4550\n",
            "  [Batch 6010/6245] Loss: 0.3061\n",
            "  [Batch 6020/6245] Loss: 0.3771\n",
            "  [Batch 6030/6245] Loss: 0.4581\n",
            "  [Batch 6040/6245] Loss: 0.3195\n",
            "  [Batch 6050/6245] Loss: 0.3141\n",
            "  [Batch 6060/6245] Loss: 0.3760\n",
            "  [Batch 6070/6245] Loss: 0.1815\n",
            "  [Batch 6080/6245] Loss: 0.1948\n",
            "  [Batch 6090/6245] Loss: 0.3417\n",
            "  [Batch 6100/6245] Loss: 0.3485\n",
            "  [Batch 6110/6245] Loss: 0.2094\n",
            "  [Batch 6120/6245] Loss: 0.2285\n",
            "  [Batch 6130/6245] Loss: 0.3278\n",
            "  [Batch 6140/6245] Loss: 0.2685\n",
            "  [Batch 6150/6245] Loss: 0.2395\n",
            "  [Batch 6160/6245] Loss: 0.2123\n",
            "  [Batch 6170/6245] Loss: 0.3155\n",
            "  [Batch 6180/6245] Loss: 0.2557\n",
            "  [Batch 6190/6245] Loss: 0.4566\n",
            "  [Batch 6200/6245] Loss: 0.3915\n",
            "  [Batch 6210/6245] Loss: 0.2742\n",
            "  [Batch 6220/6245] Loss: 0.2490\n",
            "  [Batch 6230/6245] Loss: 0.3087\n",
            "  [Batch 6240/6245] Loss: 0.2267\n",
            "  [Batch 6245/6245] Loss: 0.4043\n",
            "[Epoch 8] Train Loss: 0.3170, Acc: 86.37% | Val Loss: 0.3075, Acc: 86.74%\n",
            "[INFO] Model checkpoint saved at ./models\\breastnet_epoch8.pth\n",
            "\n",
            "[Epoch 9/10]\n",
            "  [Batch 10/6245] Loss: 0.3853\n",
            "  [Batch 20/6245] Loss: 0.3406\n",
            "  [Batch 30/6245] Loss: 0.2354\n",
            "  [Batch 40/6245] Loss: 0.5620\n",
            "  [Batch 50/6245] Loss: 0.2661\n",
            "  [Batch 60/6245] Loss: 0.1930\n",
            "  [Batch 70/6245] Loss: 0.2194\n",
            "  [Batch 80/6245] Loss: 0.6321\n",
            "  [Batch 90/6245] Loss: 0.3557\n",
            "  [Batch 100/6245] Loss: 0.2779\n",
            "  [Batch 110/6245] Loss: 0.3892\n",
            "  [Batch 120/6245] Loss: 0.3129\n",
            "  [Batch 130/6245] Loss: 0.2267\n",
            "  [Batch 140/6245] Loss: 0.3572\n",
            "  [Batch 150/6245] Loss: 0.1727\n",
            "  [Batch 160/6245] Loss: 0.2644\n",
            "  [Batch 170/6245] Loss: 0.3296\n",
            "  [Batch 180/6245] Loss: 0.2595\n",
            "  [Batch 190/6245] Loss: 0.3954\n",
            "  [Batch 200/6245] Loss: 0.2301\n",
            "  [Batch 210/6245] Loss: 0.2470\n",
            "  [Batch 220/6245] Loss: 0.3114\n",
            "  [Batch 230/6245] Loss: 0.2731\n",
            "  [Batch 240/6245] Loss: 0.2010\n",
            "  [Batch 250/6245] Loss: 0.2985\n",
            "  [Batch 260/6245] Loss: 0.1403\n",
            "  [Batch 270/6245] Loss: 0.2514\n",
            "  [Batch 280/6245] Loss: 0.3855\n",
            "  [Batch 290/6245] Loss: 0.2816\n",
            "  [Batch 300/6245] Loss: 0.2849\n",
            "  [Batch 310/6245] Loss: 0.4652\n",
            "  [Batch 320/6245] Loss: 0.5941\n",
            "  [Batch 330/6245] Loss: 0.2501\n",
            "  [Batch 340/6245] Loss: 0.2952\n",
            "  [Batch 350/6245] Loss: 0.2744\n",
            "  [Batch 360/6245] Loss: 0.2152\n",
            "  [Batch 370/6245] Loss: 0.3056\n",
            "  [Batch 380/6245] Loss: 0.2491\n",
            "  [Batch 390/6245] Loss: 0.3739\n",
            "  [Batch 400/6245] Loss: 0.3721\n",
            "  [Batch 410/6245] Loss: 0.2025\n",
            "  [Batch 420/6245] Loss: 0.3819\n",
            "  [Batch 430/6245] Loss: 0.1509\n",
            "  [Batch 440/6245] Loss: 0.3614\n",
            "  [Batch 450/6245] Loss: 0.2986\n",
            "  [Batch 460/6245] Loss: 0.2941\n",
            "  [Batch 470/6245] Loss: 0.3163\n",
            "  [Batch 480/6245] Loss: 0.4224\n",
            "  [Batch 490/6245] Loss: 0.4072\n",
            "  [Batch 500/6245] Loss: 0.2530\n",
            "  [Batch 510/6245] Loss: 0.1611\n",
            "  [Batch 520/6245] Loss: 0.4249\n",
            "  [Batch 530/6245] Loss: 0.2839\n",
            "  [Batch 540/6245] Loss: 0.1989\n",
            "  [Batch 550/6245] Loss: 0.5276\n",
            "  [Batch 560/6245] Loss: 0.3406\n",
            "  [Batch 570/6245] Loss: 0.2858\n",
            "  [Batch 580/6245] Loss: 0.3369\n",
            "  [Batch 590/6245] Loss: 0.4104\n",
            "  [Batch 600/6245] Loss: 0.2921\n",
            "  [Batch 610/6245] Loss: 0.2093\n",
            "  [Batch 620/6245] Loss: 0.4319\n",
            "  [Batch 630/6245] Loss: 0.3530\n",
            "  [Batch 640/6245] Loss: 0.4096\n",
            "  [Batch 650/6245] Loss: 0.2461\n",
            "  [Batch 660/6245] Loss: 0.3657\n",
            "  [Batch 670/6245] Loss: 0.3822\n",
            "  [Batch 680/6245] Loss: 0.2897\n",
            "  [Batch 690/6245] Loss: 0.2374\n",
            "  [Batch 700/6245] Loss: 0.3534\n",
            "  [Batch 710/6245] Loss: 0.2920\n",
            "  [Batch 720/6245] Loss: 0.2439\n",
            "  [Batch 730/6245] Loss: 0.1474\n",
            "  [Batch 740/6245] Loss: 0.3706\n",
            "  [Batch 750/6245] Loss: 0.1708\n",
            "  [Batch 760/6245] Loss: 0.2241\n",
            "  [Batch 770/6245] Loss: 0.3043\n",
            "  [Batch 780/6245] Loss: 0.4298\n",
            "  [Batch 790/6245] Loss: 0.2583\n",
            "  [Batch 800/6245] Loss: 0.1101\n",
            "  [Batch 810/6245] Loss: 0.4327\n",
            "  [Batch 820/6245] Loss: 0.3061\n",
            "  [Batch 830/6245] Loss: 0.2943\n",
            "  [Batch 840/6245] Loss: 0.2428\n",
            "  [Batch 850/6245] Loss: 0.1965\n",
            "  [Batch 860/6245] Loss: 0.2054\n",
            "  [Batch 870/6245] Loss: 0.2982\n",
            "  [Batch 880/6245] Loss: 0.2836\n",
            "  [Batch 890/6245] Loss: 0.2377\n",
            "  [Batch 900/6245] Loss: 0.4207\n",
            "  [Batch 910/6245] Loss: 0.3031\n",
            "  [Batch 920/6245] Loss: 0.4221\n",
            "  [Batch 930/6245] Loss: 0.2812\n",
            "  [Batch 940/6245] Loss: 0.2401\n",
            "  [Batch 950/6245] Loss: 0.3171\n",
            "  [Batch 960/6245] Loss: 0.1341\n",
            "  [Batch 970/6245] Loss: 0.5945\n",
            "  [Batch 980/6245] Loss: 0.3546\n",
            "  [Batch 990/6245] Loss: 0.2564\n",
            "  [Batch 1000/6245] Loss: 0.4505\n",
            "  [Batch 1010/6245] Loss: 0.3238\n",
            "  [Batch 1020/6245] Loss: 0.3317\n",
            "  [Batch 1030/6245] Loss: 0.6329\n",
            "  [Batch 1040/6245] Loss: 0.2616\n",
            "  [Batch 1050/6245] Loss: 0.3682\n",
            "  [Batch 1060/6245] Loss: 0.4246\n",
            "  [Batch 1070/6245] Loss: 0.4111\n",
            "  [Batch 1080/6245] Loss: 0.2378\n",
            "  [Batch 1090/6245] Loss: 0.3422\n",
            "  [Batch 1100/6245] Loss: 0.3393\n",
            "  [Batch 1110/6245] Loss: 0.2836\n",
            "  [Batch 1120/6245] Loss: 0.1657\n",
            "  [Batch 1130/6245] Loss: 0.5137\n",
            "  [Batch 1140/6245] Loss: 0.1475\n",
            "  [Batch 1150/6245] Loss: 0.3450\n",
            "  [Batch 1160/6245] Loss: 0.3492\n",
            "  [Batch 1170/6245] Loss: 0.4284\n",
            "  [Batch 1180/6245] Loss: 0.2071\n",
            "  [Batch 1190/6245] Loss: 0.2204\n",
            "  [Batch 1200/6245] Loss: 0.3358\n",
            "  [Batch 1210/6245] Loss: 0.1994\n",
            "  [Batch 1220/6245] Loss: 0.2016\n",
            "  [Batch 1230/6245] Loss: 0.4599\n",
            "  [Batch 1240/6245] Loss: 0.1480\n",
            "  [Batch 1250/6245] Loss: 0.3756\n",
            "  [Batch 1260/6245] Loss: 0.3936\n",
            "  [Batch 1270/6245] Loss: 0.4077\n",
            "  [Batch 1280/6245] Loss: 0.2635\n",
            "  [Batch 1290/6245] Loss: 0.3267\n",
            "  [Batch 1300/6245] Loss: 0.3208\n",
            "  [Batch 1310/6245] Loss: 0.4501\n",
            "  [Batch 1320/6245] Loss: 0.1838\n",
            "  [Batch 1330/6245] Loss: 0.4132\n",
            "  [Batch 1340/6245] Loss: 0.1424\n",
            "  [Batch 1350/6245] Loss: 0.3919\n",
            "  [Batch 1360/6245] Loss: 0.3592\n",
            "  [Batch 1370/6245] Loss: 0.2485\n",
            "  [Batch 1380/6245] Loss: 0.3040\n",
            "  [Batch 1390/6245] Loss: 0.3443\n",
            "  [Batch 1400/6245] Loss: 0.3132\n",
            "  [Batch 1410/6245] Loss: 0.2953\n",
            "  [Batch 1420/6245] Loss: 0.4373\n",
            "  [Batch 1430/6245] Loss: 0.2019\n",
            "  [Batch 1440/6245] Loss: 0.2220\n",
            "  [Batch 1450/6245] Loss: 0.2671\n",
            "  [Batch 1460/6245] Loss: 0.3175\n",
            "  [Batch 1470/6245] Loss: 0.3726\n",
            "  [Batch 1480/6245] Loss: 0.2244\n",
            "  [Batch 1490/6245] Loss: 0.2831\n",
            "  [Batch 1500/6245] Loss: 0.3552\n",
            "  [Batch 1510/6245] Loss: 0.1857\n",
            "  [Batch 1520/6245] Loss: 0.2136\n",
            "  [Batch 1530/6245] Loss: 0.3954\n",
            "  [Batch 1540/6245] Loss: 0.3245\n",
            "  [Batch 1550/6245] Loss: 0.5809\n",
            "  [Batch 1560/6245] Loss: 0.3407\n",
            "  [Batch 1570/6245] Loss: 0.2608\n",
            "  [Batch 1580/6245] Loss: 0.3719\n",
            "  [Batch 1590/6245] Loss: 0.2152\n",
            "  [Batch 1600/6245] Loss: 0.3911\n",
            "  [Batch 1610/6245] Loss: 0.2617\n",
            "  [Batch 1620/6245] Loss: 0.2717\n",
            "  [Batch 1630/6245] Loss: 0.3295\n",
            "  [Batch 1640/6245] Loss: 0.4352\n",
            "  [Batch 1650/6245] Loss: 0.3666\n",
            "  [Batch 1660/6245] Loss: 0.5110\n",
            "  [Batch 1670/6245] Loss: 0.2618\n",
            "  [Batch 1680/6245] Loss: 0.3284\n",
            "  [Batch 1690/6245] Loss: 0.2448\n",
            "  [Batch 1700/6245] Loss: 0.5269\n",
            "  [Batch 1710/6245] Loss: 0.3545\n",
            "  [Batch 1720/6245] Loss: 0.2464\n",
            "  [Batch 1730/6245] Loss: 0.3737\n",
            "  [Batch 1740/6245] Loss: 0.3106\n",
            "  [Batch 1750/6245] Loss: 0.2761\n",
            "  [Batch 1760/6245] Loss: 0.2411\n",
            "  [Batch 1770/6245] Loss: 0.3211\n",
            "  [Batch 1780/6245] Loss: 0.3672\n",
            "  [Batch 1790/6245] Loss: 0.4093\n",
            "  [Batch 1800/6245] Loss: 0.2134\n",
            "  [Batch 1810/6245] Loss: 0.2087\n",
            "  [Batch 1820/6245] Loss: 0.3421\n",
            "  [Batch 1830/6245] Loss: 0.2337\n",
            "  [Batch 1840/6245] Loss: 0.5205\n",
            "  [Batch 1850/6245] Loss: 0.2913\n",
            "  [Batch 1860/6245] Loss: 0.2586\n",
            "  [Batch 1870/6245] Loss: 0.5350\n",
            "  [Batch 1880/6245] Loss: 0.5425\n",
            "  [Batch 1890/6245] Loss: 0.4650\n",
            "  [Batch 1900/6245] Loss: 0.2828\n",
            "  [Batch 1910/6245] Loss: 0.4047\n",
            "  [Batch 1920/6245] Loss: 0.4247\n",
            "  [Batch 1930/6245] Loss: 0.3506\n",
            "  [Batch 1940/6245] Loss: 0.3556\n",
            "  [Batch 1950/6245] Loss: 0.1840\n",
            "  [Batch 1960/6245] Loss: 0.4486\n",
            "  [Batch 1970/6245] Loss: 0.4654\n",
            "  [Batch 1980/6245] Loss: 0.3143\n",
            "  [Batch 1990/6245] Loss: 0.3086\n",
            "  [Batch 2000/6245] Loss: 0.1780\n",
            "  [Batch 2010/6245] Loss: 0.4031\n",
            "  [Batch 2020/6245] Loss: 0.2804\n",
            "  [Batch 2030/6245] Loss: 0.3307\n",
            "  [Batch 2040/6245] Loss: 0.2333\n",
            "  [Batch 2050/6245] Loss: 0.3926\n",
            "  [Batch 2060/6245] Loss: 0.3834\n",
            "  [Batch 2070/6245] Loss: 0.3334\n",
            "  [Batch 2080/6245] Loss: 0.2490\n",
            "  [Batch 2090/6245] Loss: 0.2451\n",
            "  [Batch 2100/6245] Loss: 0.4604\n",
            "  [Batch 2110/6245] Loss: 0.3604\n",
            "  [Batch 2120/6245] Loss: 0.2299\n",
            "  [Batch 2130/6245] Loss: 0.2991\n",
            "  [Batch 2140/6245] Loss: 0.4317\n",
            "  [Batch 2150/6245] Loss: 0.2185\n",
            "  [Batch 2160/6245] Loss: 0.4878\n",
            "  [Batch 2170/6245] Loss: 0.4174\n",
            "  [Batch 2180/6245] Loss: 0.4084\n",
            "  [Batch 2190/6245] Loss: 0.2079\n",
            "  [Batch 2200/6245] Loss: 0.1770\n",
            "  [Batch 2210/6245] Loss: 0.4791\n",
            "  [Batch 2220/6245] Loss: 0.2539\n",
            "  [Batch 2230/6245] Loss: 0.2341\n",
            "  [Batch 2240/6245] Loss: 0.3737\n",
            "  [Batch 2250/6245] Loss: 0.2191\n",
            "  [Batch 2260/6245] Loss: 0.2448\n",
            "  [Batch 2270/6245] Loss: 0.2291\n",
            "  [Batch 2280/6245] Loss: 0.1841\n",
            "  [Batch 2290/6245] Loss: 0.2060\n",
            "  [Batch 2300/6245] Loss: 0.1434\n",
            "  [Batch 2310/6245] Loss: 0.2450\n",
            "  [Batch 2320/6245] Loss: 0.3733\n",
            "  [Batch 2330/6245] Loss: 0.3395\n",
            "  [Batch 2340/6245] Loss: 0.2962\n",
            "  [Batch 2350/6245] Loss: 0.3816\n",
            "  [Batch 2360/6245] Loss: 0.2629\n",
            "  [Batch 2370/6245] Loss: 0.2853\n",
            "  [Batch 2380/6245] Loss: 0.3531\n",
            "  [Batch 2390/6245] Loss: 0.4278\n",
            "  [Batch 2400/6245] Loss: 0.4147\n",
            "  [Batch 2410/6245] Loss: 0.2829\n",
            "  [Batch 2420/6245] Loss: 0.4860\n",
            "  [Batch 2430/6245] Loss: 0.1869\n",
            "  [Batch 2440/6245] Loss: 0.2035\n",
            "  [Batch 2450/6245] Loss: 0.7449\n",
            "  [Batch 2460/6245] Loss: 0.3380\n",
            "  [Batch 2470/6245] Loss: 0.2965\n",
            "  [Batch 2480/6245] Loss: 0.4225\n",
            "  [Batch 2490/6245] Loss: 0.3201\n",
            "  [Batch 2500/6245] Loss: 0.2889\n",
            "  [Batch 2510/6245] Loss: 0.2624\n",
            "  [Batch 2520/6245] Loss: 0.2942\n",
            "  [Batch 2530/6245] Loss: 0.2606\n",
            "  [Batch 2540/6245] Loss: 0.2367\n",
            "  [Batch 2550/6245] Loss: 0.2617\n",
            "  [Batch 2560/6245] Loss: 0.2365\n",
            "  [Batch 2570/6245] Loss: 0.2612\n",
            "  [Batch 2580/6245] Loss: 0.5155\n",
            "  [Batch 2590/6245] Loss: 0.3148\n",
            "  [Batch 2600/6245] Loss: 0.5586\n",
            "  [Batch 2610/6245] Loss: 0.3232\n",
            "  [Batch 2620/6245] Loss: 0.2514\n",
            "  [Batch 2630/6245] Loss: 0.2172\n",
            "  [Batch 2640/6245] Loss: 0.1345\n",
            "  [Batch 2650/6245] Loss: 0.2062\n",
            "  [Batch 2660/6245] Loss: 0.4680\n",
            "  [Batch 2670/6245] Loss: 0.3873\n",
            "  [Batch 2680/6245] Loss: 0.2011\n",
            "  [Batch 2690/6245] Loss: 0.3158\n",
            "  [Batch 2700/6245] Loss: 0.2140\n",
            "  [Batch 2710/6245] Loss: 0.2284\n",
            "  [Batch 2720/6245] Loss: 0.3390\n",
            "  [Batch 2730/6245] Loss: 0.2947\n",
            "  [Batch 2740/6245] Loss: 0.2136\n",
            "  [Batch 2750/6245] Loss: 0.4097\n",
            "  [Batch 2760/6245] Loss: 0.4039\n",
            "  [Batch 2770/6245] Loss: 0.1202\n",
            "  [Batch 2780/6245] Loss: 0.2559\n",
            "  [Batch 2790/6245] Loss: 0.2258\n",
            "  [Batch 2800/6245] Loss: 0.3201\n",
            "  [Batch 2810/6245] Loss: 0.3005\n",
            "  [Batch 2820/6245] Loss: 0.5223\n",
            "  [Batch 2830/6245] Loss: 0.3453\n",
            "  [Batch 2840/6245] Loss: 0.2140\n",
            "  [Batch 2850/6245] Loss: 0.4492\n",
            "  [Batch 2860/6245] Loss: 0.3108\n",
            "  [Batch 2870/6245] Loss: 0.2190\n",
            "  [Batch 2880/6245] Loss: 0.1790\n",
            "  [Batch 2890/6245] Loss: 0.4789\n",
            "  [Batch 2900/6245] Loss: 0.4329\n",
            "  [Batch 2910/6245] Loss: 0.3607\n",
            "  [Batch 2920/6245] Loss: 0.6569\n",
            "  [Batch 2930/6245] Loss: 0.3421\n",
            "  [Batch 2940/6245] Loss: 0.3487\n",
            "  [Batch 2950/6245] Loss: 0.1961\n",
            "  [Batch 2960/6245] Loss: 0.3395\n",
            "  [Batch 2970/6245] Loss: 0.2189\n",
            "  [Batch 2980/6245] Loss: 0.2363\n",
            "  [Batch 2990/6245] Loss: 0.3612\n",
            "  [Batch 3000/6245] Loss: 0.2491\n",
            "  [Batch 3010/6245] Loss: 0.2908\n",
            "  [Batch 3020/6245] Loss: 0.3880\n",
            "  [Batch 3030/6245] Loss: 0.2325\n",
            "  [Batch 3040/6245] Loss: 0.2515\n",
            "  [Batch 3050/6245] Loss: 0.4723\n",
            "  [Batch 3060/6245] Loss: 0.2750\n",
            "  [Batch 3070/6245] Loss: 0.4532\n",
            "  [Batch 3080/6245] Loss: 0.3564\n",
            "  [Batch 3090/6245] Loss: 0.4590\n",
            "  [Batch 3100/6245] Loss: 0.2680\n",
            "  [Batch 3110/6245] Loss: 0.3562\n",
            "  [Batch 3120/6245] Loss: 0.2912\n",
            "  [Batch 3130/6245] Loss: 0.4609\n",
            "  [Batch 3140/6245] Loss: 0.1761\n",
            "  [Batch 3150/6245] Loss: 0.3393\n",
            "  [Batch 3160/6245] Loss: 0.2628\n",
            "  [Batch 3170/6245] Loss: 0.2041\n",
            "  [Batch 3180/6245] Loss: 0.3876\n",
            "  [Batch 3190/6245] Loss: 0.2697\n",
            "  [Batch 3200/6245] Loss: 0.2247\n",
            "  [Batch 3210/6245] Loss: 0.4657\n",
            "  [Batch 3220/6245] Loss: 0.3757\n",
            "  [Batch 3230/6245] Loss: 0.3843\n",
            "  [Batch 3240/6245] Loss: 0.3819\n",
            "  [Batch 3250/6245] Loss: 0.2958\n",
            "  [Batch 3260/6245] Loss: 0.4435\n",
            "  [Batch 3270/6245] Loss: 0.1745\n",
            "  [Batch 3280/6245] Loss: 0.3974\n",
            "  [Batch 3290/6245] Loss: 0.3120\n",
            "  [Batch 3300/6245] Loss: 0.3219\n",
            "  [Batch 3310/6245] Loss: 0.3641\n",
            "  [Batch 3320/6245] Loss: 0.2424\n",
            "  [Batch 3330/6245] Loss: 0.3220\n",
            "  [Batch 3340/6245] Loss: 0.2538\n",
            "  [Batch 3350/6245] Loss: 0.4805\n",
            "  [Batch 3360/6245] Loss: 0.3529\n",
            "  [Batch 3370/6245] Loss: 0.2578\n",
            "  [Batch 3380/6245] Loss: 0.3906\n",
            "  [Batch 3390/6245] Loss: 0.3597\n",
            "  [Batch 3400/6245] Loss: 0.4174\n",
            "  [Batch 3410/6245] Loss: 0.2366\n",
            "  [Batch 3420/6245] Loss: 0.3002\n",
            "  [Batch 3430/6245] Loss: 0.2527\n",
            "  [Batch 3440/6245] Loss: 0.3402\n",
            "  [Batch 3450/6245] Loss: 0.2313\n",
            "  [Batch 3460/6245] Loss: 0.3785\n",
            "  [Batch 3470/6245] Loss: 0.2522\n",
            "  [Batch 3480/6245] Loss: 0.2756\n",
            "  [Batch 3490/6245] Loss: 0.5328\n",
            "  [Batch 3500/6245] Loss: 0.1952\n",
            "  [Batch 3510/6245] Loss: 0.2512\n",
            "  [Batch 3520/6245] Loss: 0.2400\n",
            "  [Batch 3530/6245] Loss: 0.4860\n",
            "  [Batch 3540/6245] Loss: 0.2283\n",
            "  [Batch 3550/6245] Loss: 0.2640\n",
            "  [Batch 3560/6245] Loss: 0.2131\n",
            "  [Batch 3570/6245] Loss: 0.2601\n",
            "  [Batch 3580/6245] Loss: 0.2428\n",
            "  [Batch 3590/6245] Loss: 0.2482\n",
            "  [Batch 3600/6245] Loss: 0.3800\n",
            "  [Batch 3610/6245] Loss: 0.2649\n",
            "  [Batch 3620/6245] Loss: 0.1497\n",
            "  [Batch 3630/6245] Loss: 0.5840\n",
            "  [Batch 3640/6245] Loss: 0.2994\n",
            "  [Batch 3650/6245] Loss: 0.2045\n",
            "  [Batch 3660/6245] Loss: 0.3407\n",
            "  [Batch 3670/6245] Loss: 0.3668\n",
            "  [Batch 3680/6245] Loss: 0.1902\n",
            "  [Batch 3690/6245] Loss: 0.3560\n",
            "  [Batch 3700/6245] Loss: 0.3364\n",
            "  [Batch 3710/6245] Loss: 0.1694\n",
            "  [Batch 3720/6245] Loss: 0.3365\n",
            "  [Batch 3730/6245] Loss: 0.3618\n",
            "  [Batch 3740/6245] Loss: 0.2835\n",
            "  [Batch 3750/6245] Loss: 0.3751\n",
            "  [Batch 3760/6245] Loss: 0.1885\n",
            "  [Batch 3770/6245] Loss: 0.2218\n",
            "  [Batch 3780/6245] Loss: 0.1767\n",
            "  [Batch 3790/6245] Loss: 0.3515\n",
            "  [Batch 3800/6245] Loss: 0.2288\n",
            "  [Batch 3810/6245] Loss: 0.3256\n",
            "  [Batch 3820/6245] Loss: 0.2649\n",
            "  [Batch 3830/6245] Loss: 0.4350\n",
            "  [Batch 3840/6245] Loss: 0.3057\n",
            "  [Batch 3850/6245] Loss: 0.2139\n",
            "  [Batch 3860/6245] Loss: 0.3586\n",
            "  [Batch 3870/6245] Loss: 0.2484\n",
            "  [Batch 3880/6245] Loss: 0.2525\n",
            "  [Batch 3890/6245] Loss: 0.3569\n",
            "  [Batch 3900/6245] Loss: 0.2547\n",
            "  [Batch 3910/6245] Loss: 0.2411\n",
            "  [Batch 3920/6245] Loss: 0.6363\n",
            "  [Batch 3930/6245] Loss: 0.2088\n",
            "  [Batch 3940/6245] Loss: 0.4147\n",
            "  [Batch 3950/6245] Loss: 0.2670\n",
            "  [Batch 3960/6245] Loss: 0.4809\n",
            "  [Batch 3970/6245] Loss: 0.4634\n",
            "  [Batch 3980/6245] Loss: 0.5222\n",
            "  [Batch 3990/6245] Loss: 0.1912\n",
            "  [Batch 4000/6245] Loss: 0.3369\n",
            "  [Batch 4010/6245] Loss: 0.1829\n",
            "  [Batch 4020/6245] Loss: 0.2823\n",
            "  [Batch 4030/6245] Loss: 0.3184\n",
            "  [Batch 4040/6245] Loss: 0.2269\n",
            "  [Batch 4050/6245] Loss: 0.2414\n",
            "  [Batch 4060/6245] Loss: 0.2168\n",
            "  [Batch 4070/6245] Loss: 0.1810\n",
            "  [Batch 4080/6245] Loss: 0.2111\n",
            "  [Batch 4090/6245] Loss: 0.2547\n",
            "  [Batch 4100/6245] Loss: 0.3522\n",
            "  [Batch 4110/6245] Loss: 0.3050\n",
            "  [Batch 4120/6245] Loss: 0.3414\n",
            "  [Batch 4130/6245] Loss: 0.3523\n",
            "  [Batch 4140/6245] Loss: 0.2313\n",
            "  [Batch 4150/6245] Loss: 0.2609\n",
            "  [Batch 4160/6245] Loss: 0.4162\n",
            "  [Batch 4170/6245] Loss: 0.1934\n",
            "  [Batch 4180/6245] Loss: 0.3249\n",
            "  [Batch 4190/6245] Loss: 0.2632\n",
            "  [Batch 4200/6245] Loss: 0.3748\n",
            "  [Batch 4210/6245] Loss: 0.2514\n",
            "  [Batch 4220/6245] Loss: 0.3363\n",
            "  [Batch 4230/6245] Loss: 0.1684\n",
            "  [Batch 4240/6245] Loss: 0.2600\n",
            "  [Batch 4250/6245] Loss: 0.3689\n",
            "  [Batch 4260/6245] Loss: 0.3622\n",
            "  [Batch 4270/6245] Loss: 0.2484\n",
            "  [Batch 4280/6245] Loss: 0.3476\n",
            "  [Batch 4290/6245] Loss: 0.2745\n",
            "  [Batch 4300/6245] Loss: 0.1665\n",
            "  [Batch 4310/6245] Loss: 0.4511\n",
            "  [Batch 4320/6245] Loss: 0.4460\n",
            "  [Batch 4330/6245] Loss: 0.2005\n",
            "  [Batch 4340/6245] Loss: 0.2689\n",
            "  [Batch 4350/6245] Loss: 0.3956\n",
            "  [Batch 4360/6245] Loss: 0.2518\n",
            "  [Batch 4370/6245] Loss: 0.2318\n",
            "  [Batch 4380/6245] Loss: 0.6309\n",
            "  [Batch 4390/6245] Loss: 0.4436\n",
            "  [Batch 4400/6245] Loss: 0.5253\n",
            "  [Batch 4410/6245] Loss: 0.2882\n",
            "  [Batch 4420/6245] Loss: 0.2753\n",
            "  [Batch 4430/6245] Loss: 0.4328\n",
            "  [Batch 4440/6245] Loss: 0.3605\n",
            "  [Batch 4450/6245] Loss: 0.3744\n",
            "  [Batch 4460/6245] Loss: 0.3990\n",
            "  [Batch 4470/6245] Loss: 0.2012\n",
            "  [Batch 4480/6245] Loss: 0.2844\n",
            "  [Batch 4490/6245] Loss: 0.3920\n",
            "  [Batch 4500/6245] Loss: 0.3924\n",
            "  [Batch 4510/6245] Loss: 0.1915\n",
            "  [Batch 4520/6245] Loss: 0.2933\n",
            "  [Batch 4530/6245] Loss: 0.1949\n",
            "  [Batch 4540/6245] Loss: 0.2801\n",
            "  [Batch 4550/6245] Loss: 0.2095\n",
            "  [Batch 4560/6245] Loss: 0.3021\n",
            "  [Batch 4570/6245] Loss: 0.2777\n",
            "  [Batch 4580/6245] Loss: 0.3581\n",
            "  [Batch 4590/6245] Loss: 0.3560\n",
            "  [Batch 4600/6245] Loss: 0.2664\n",
            "  [Batch 4610/6245] Loss: 0.2054\n",
            "  [Batch 4620/6245] Loss: 0.3356\n",
            "  [Batch 4630/6245] Loss: 0.4132\n",
            "  [Batch 4640/6245] Loss: 0.2676\n",
            "  [Batch 4650/6245] Loss: 0.3712\n",
            "  [Batch 4660/6245] Loss: 0.4297\n",
            "  [Batch 4670/6245] Loss: 0.2532\n",
            "  [Batch 4680/6245] Loss: 0.4346\n",
            "  [Batch 4690/6245] Loss: 0.6675\n",
            "  [Batch 4700/6245] Loss: 0.1888\n",
            "  [Batch 4710/6245] Loss: 0.2914\n",
            "  [Batch 4720/6245] Loss: 0.3513\n",
            "  [Batch 4730/6245] Loss: 0.2504\n",
            "  [Batch 4740/6245] Loss: 0.3624\n",
            "  [Batch 4750/6245] Loss: 0.1989\n",
            "  [Batch 4760/6245] Loss: 0.3289\n",
            "  [Batch 4770/6245] Loss: 0.2677\n",
            "  [Batch 4780/6245] Loss: 0.2634\n",
            "  [Batch 4790/6245] Loss: 0.2978\n",
            "  [Batch 4800/6245] Loss: 0.2196\n",
            "  [Batch 4810/6245] Loss: 0.2355\n",
            "  [Batch 4820/6245] Loss: 0.1919\n",
            "  [Batch 4830/6245] Loss: 0.3000\n",
            "  [Batch 4840/6245] Loss: 0.3575\n",
            "  [Batch 4850/6245] Loss: 0.2973\n",
            "  [Batch 4860/6245] Loss: 0.4408\n",
            "  [Batch 4870/6245] Loss: 0.3061\n",
            "  [Batch 4880/6245] Loss: 0.3631\n",
            "  [Batch 4890/6245] Loss: 0.2530\n",
            "  [Batch 4900/6245] Loss: 0.1554\n",
            "  [Batch 4910/6245] Loss: 0.2497\n",
            "  [Batch 4920/6245] Loss: 0.3057\n",
            "  [Batch 4930/6245] Loss: 0.2996\n",
            "  [Batch 4940/6245] Loss: 0.3838\n",
            "  [Batch 4950/6245] Loss: 0.2110\n",
            "  [Batch 4960/6245] Loss: 0.2347\n",
            "  [Batch 4970/6245] Loss: 0.4556\n",
            "  [Batch 4980/6245] Loss: 0.4129\n",
            "  [Batch 4990/6245] Loss: 0.2680\n",
            "  [Batch 5000/6245] Loss: 0.4130\n",
            "  [Batch 5010/6245] Loss: 0.3733\n",
            "  [Batch 5020/6245] Loss: 0.2301\n",
            "  [Batch 5030/6245] Loss: 0.3882\n",
            "  [Batch 5040/6245] Loss: 0.3532\n",
            "  [Batch 5050/6245] Loss: 0.1744\n",
            "  [Batch 5060/6245] Loss: 0.1857\n",
            "  [Batch 5070/6245] Loss: 0.1810\n",
            "  [Batch 5080/6245] Loss: 0.2898\n",
            "  [Batch 5090/6245] Loss: 0.1550\n",
            "  [Batch 5100/6245] Loss: 0.2831\n",
            "  [Batch 5110/6245] Loss: 0.3600\n",
            "  [Batch 5120/6245] Loss: 0.1983\n",
            "  [Batch 5130/6245] Loss: 0.3937\n",
            "  [Batch 5140/6245] Loss: 0.5259\n",
            "  [Batch 5150/6245] Loss: 0.2379\n",
            "  [Batch 5160/6245] Loss: 0.3818\n",
            "  [Batch 5170/6245] Loss: 0.1868\n",
            "  [Batch 5180/6245] Loss: 0.2420\n",
            "  [Batch 5190/6245] Loss: 0.2514\n",
            "  [Batch 5200/6245] Loss: 0.3357\n",
            "  [Batch 5210/6245] Loss: 0.2356\n",
            "  [Batch 5220/6245] Loss: 0.3696\n",
            "  [Batch 5230/6245] Loss: 0.2410\n",
            "  [Batch 5240/6245] Loss: 0.2805\n",
            "  [Batch 5250/6245] Loss: 0.3552\n",
            "  [Batch 5260/6245] Loss: 0.2308\n",
            "  [Batch 5270/6245] Loss: 0.2853\n",
            "  [Batch 5280/6245] Loss: 0.2944\n",
            "  [Batch 5290/6245] Loss: 0.3646\n",
            "  [Batch 5300/6245] Loss: 0.4896\n",
            "  [Batch 5310/6245] Loss: 0.3562\n",
            "  [Batch 5320/6245] Loss: 0.3113\n",
            "  [Batch 5330/6245] Loss: 0.3426\n",
            "  [Batch 5340/6245] Loss: 0.2828\n",
            "  [Batch 5350/6245] Loss: 0.4220\n",
            "  [Batch 5360/6245] Loss: 0.3947\n",
            "  [Batch 5370/6245] Loss: 0.3355\n",
            "  [Batch 5380/6245] Loss: 0.4238\n",
            "  [Batch 5390/6245] Loss: 0.4023\n",
            "  [Batch 5400/6245] Loss: 0.2872\n",
            "  [Batch 5410/6245] Loss: 0.3085\n",
            "  [Batch 5420/6245] Loss: 0.2349\n",
            "  [Batch 5430/6245] Loss: 0.2975\n",
            "  [Batch 5440/6245] Loss: 0.2136\n",
            "  [Batch 5450/6245] Loss: 0.3194\n",
            "  [Batch 5460/6245] Loss: 0.2780\n",
            "  [Batch 5470/6245] Loss: 0.2289\n",
            "  [Batch 5480/6245] Loss: 0.4218\n",
            "  [Batch 5490/6245] Loss: 0.2094\n",
            "  [Batch 5500/6245] Loss: 0.4798\n",
            "  [Batch 5510/6245] Loss: 0.1782\n",
            "  [Batch 5520/6245] Loss: 0.2918\n",
            "  [Batch 5530/6245] Loss: 0.5544\n",
            "  [Batch 5540/6245] Loss: 0.2851\n",
            "  [Batch 5550/6245] Loss: 0.1707\n",
            "  [Batch 5560/6245] Loss: 0.2323\n",
            "  [Batch 5570/6245] Loss: 0.2422\n",
            "  [Batch 5580/6245] Loss: 0.2403\n",
            "  [Batch 5590/6245] Loss: 0.5207\n",
            "  [Batch 5600/6245] Loss: 0.3572\n",
            "  [Batch 5610/6245] Loss: 0.2987\n",
            "  [Batch 5620/6245] Loss: 0.3020\n",
            "  [Batch 5630/6245] Loss: 0.5270\n",
            "  [Batch 5640/6245] Loss: 0.4209\n",
            "  [Batch 5650/6245] Loss: 0.2747\n",
            "  [Batch 5660/6245] Loss: 0.2786\n",
            "  [Batch 5670/6245] Loss: 0.2746\n",
            "  [Batch 5680/6245] Loss: 0.2308\n",
            "  [Batch 5690/6245] Loss: 0.3048\n",
            "  [Batch 5700/6245] Loss: 0.3628\n",
            "  [Batch 5710/6245] Loss: 0.1847\n",
            "  [Batch 5720/6245] Loss: 0.4998\n",
            "  [Batch 5730/6245] Loss: 0.1811\n",
            "  [Batch 5740/6245] Loss: 0.2626\n",
            "  [Batch 5750/6245] Loss: 0.3998\n",
            "  [Batch 5760/6245] Loss: 0.2529\n",
            "  [Batch 5770/6245] Loss: 0.1682\n",
            "  [Batch 5780/6245] Loss: 0.2812\n",
            "  [Batch 5790/6245] Loss: 0.1912\n",
            "  [Batch 5800/6245] Loss: 0.2083\n",
            "  [Batch 5810/6245] Loss: 0.5722\n",
            "  [Batch 5820/6245] Loss: 0.2882\n",
            "  [Batch 5830/6245] Loss: 0.5351\n",
            "  [Batch 5840/6245] Loss: 0.2698\n",
            "  [Batch 5850/6245] Loss: 0.3539\n",
            "  [Batch 5860/6245] Loss: 0.3232\n",
            "  [Batch 5870/6245] Loss: 0.3210\n",
            "  [Batch 5880/6245] Loss: 0.3938\n",
            "  [Batch 5890/6245] Loss: 0.5449\n",
            "  [Batch 5900/6245] Loss: 0.3803\n",
            "  [Batch 5910/6245] Loss: 0.2421\n",
            "  [Batch 5920/6245] Loss: 0.2888\n",
            "  [Batch 5930/6245] Loss: 0.3109\n",
            "  [Batch 5940/6245] Loss: 0.2856\n",
            "  [Batch 5950/6245] Loss: 0.2319\n",
            "  [Batch 5960/6245] Loss: 0.2592\n",
            "  [Batch 5970/6245] Loss: 0.2970\n",
            "  [Batch 5980/6245] Loss: 0.2994\n",
            "  [Batch 5990/6245] Loss: 0.2249\n",
            "  [Batch 6000/6245] Loss: 0.3059\n",
            "  [Batch 6010/6245] Loss: 0.2314\n",
            "  [Batch 6020/6245] Loss: 0.2497\n",
            "  [Batch 6030/6245] Loss: 0.1524\n",
            "  [Batch 6040/6245] Loss: 0.3703\n",
            "  [Batch 6050/6245] Loss: 0.2194\n",
            "  [Batch 6060/6245] Loss: 0.2170\n",
            "  [Batch 6070/6245] Loss: 0.3870\n",
            "  [Batch 6080/6245] Loss: 0.4204\n",
            "  [Batch 6090/6245] Loss: 0.2080\n",
            "  [Batch 6100/6245] Loss: 0.2313\n",
            "  [Batch 6110/6245] Loss: 0.2263\n",
            "  [Batch 6120/6245] Loss: 0.2266\n",
            "  [Batch 6130/6245] Loss: 0.3223\n",
            "  [Batch 6140/6245] Loss: 0.4097\n",
            "  [Batch 6150/6245] Loss: 0.2610\n",
            "  [Batch 6160/6245] Loss: 0.2908\n",
            "  [Batch 6170/6245] Loss: 0.1683\n",
            "  [Batch 6180/6245] Loss: 0.3399\n",
            "  [Batch 6190/6245] Loss: 0.2985\n",
            "  [Batch 6200/6245] Loss: 0.3827\n",
            "  [Batch 6210/6245] Loss: 0.2685\n",
            "  [Batch 6220/6245] Loss: 0.4955\n",
            "  [Batch 6230/6245] Loss: 0.1135\n",
            "  [Batch 6240/6245] Loss: 0.2305\n",
            "  [Batch 6245/6245] Loss: 0.1213\n",
            "[Epoch 9] Train Loss: 0.3161, Acc: 86.41% | Val Loss: 0.3068, Acc: 86.81%\n",
            "[INFO] Model checkpoint saved at ./models\\breastnet_epoch9.pth\n",
            "\n",
            "[Epoch 10/10]\n",
            "  [Batch 10/6245] Loss: 0.2402\n",
            "  [Batch 20/6245] Loss: 0.1749\n",
            "  [Batch 30/6245] Loss: 0.2129\n",
            "  [Batch 40/6245] Loss: 0.2463\n",
            "  [Batch 50/6245] Loss: 0.1382\n",
            "  [Batch 60/6245] Loss: 0.2896\n",
            "  [Batch 70/6245] Loss: 0.2369\n",
            "  [Batch 80/6245] Loss: 0.2649\n",
            "  [Batch 90/6245] Loss: 0.1895\n",
            "  [Batch 100/6245] Loss: 0.3098\n",
            "  [Batch 110/6245] Loss: 0.3581\n",
            "  [Batch 120/6245] Loss: 0.2814\n",
            "  [Batch 130/6245] Loss: 0.5660\n",
            "  [Batch 140/6245] Loss: 0.1900\n",
            "  [Batch 150/6245] Loss: 0.3363\n",
            "  [Batch 160/6245] Loss: 0.3665\n",
            "  [Batch 170/6245] Loss: 0.3766\n",
            "  [Batch 180/6245] Loss: 0.3089\n",
            "  [Batch 190/6245] Loss: 0.2721\n",
            "  [Batch 200/6245] Loss: 0.2849\n",
            "  [Batch 210/6245] Loss: 0.3476\n",
            "  [Batch 220/6245] Loss: 0.3219\n",
            "  [Batch 230/6245] Loss: 0.4685\n",
            "  [Batch 240/6245] Loss: 0.3733\n",
            "  [Batch 250/6245] Loss: 0.3152\n",
            "  [Batch 260/6245] Loss: 0.1053\n",
            "  [Batch 270/6245] Loss: 0.3009\n",
            "  [Batch 280/6245] Loss: 0.2798\n",
            "  [Batch 290/6245] Loss: 0.1988\n",
            "  [Batch 300/6245] Loss: 0.2874\n",
            "  [Batch 310/6245] Loss: 0.3943\n",
            "  [Batch 320/6245] Loss: 0.3731\n",
            "  [Batch 330/6245] Loss: 0.1874\n",
            "  [Batch 340/6245] Loss: 0.2393\n",
            "  [Batch 350/6245] Loss: 0.3735\n",
            "  [Batch 360/6245] Loss: 0.4765\n",
            "  [Batch 370/6245] Loss: 0.3535\n",
            "  [Batch 380/6245] Loss: 0.3666\n",
            "  [Batch 390/6245] Loss: 0.3041\n",
            "  [Batch 400/6245] Loss: 0.3072\n",
            "  [Batch 410/6245] Loss: 0.3532\n",
            "  [Batch 420/6245] Loss: 0.4249\n",
            "  [Batch 430/6245] Loss: 0.2163\n",
            "  [Batch 440/6245] Loss: 0.3675\n",
            "  [Batch 450/6245] Loss: 0.3521\n",
            "  [Batch 460/6245] Loss: 0.3369\n",
            "  [Batch 470/6245] Loss: 0.1987\n",
            "  [Batch 480/6245] Loss: 0.2413\n",
            "  [Batch 490/6245] Loss: 0.5045\n",
            "  [Batch 500/6245] Loss: 0.2607\n",
            "  [Batch 510/6245] Loss: 0.4177\n",
            "  [Batch 520/6245] Loss: 0.4083\n",
            "  [Batch 530/6245] Loss: 0.2200\n",
            "  [Batch 540/6245] Loss: 0.1978\n",
            "  [Batch 550/6245] Loss: 0.2014\n",
            "  [Batch 560/6245] Loss: 0.2640\n",
            "  [Batch 570/6245] Loss: 0.3122\n",
            "  [Batch 580/6245] Loss: 0.2849\n",
            "  [Batch 590/6245] Loss: 0.2126\n",
            "  [Batch 600/6245] Loss: 0.4743\n",
            "  [Batch 610/6245] Loss: 0.3042\n",
            "  [Batch 620/6245] Loss: 0.3819\n",
            "  [Batch 630/6245] Loss: 0.2117\n",
            "  [Batch 640/6245] Loss: 0.2744\n",
            "  [Batch 650/6245] Loss: 0.2118\n",
            "  [Batch 660/6245] Loss: 0.4659\n",
            "  [Batch 670/6245] Loss: 0.2989\n",
            "  [Batch 680/6245] Loss: 0.2761\n",
            "  [Batch 690/6245] Loss: 0.3029\n",
            "  [Batch 700/6245] Loss: 0.3302\n",
            "  [Batch 710/6245] Loss: 0.2757\n",
            "  [Batch 720/6245] Loss: 0.5067\n",
            "  [Batch 730/6245] Loss: 0.2128\n",
            "  [Batch 740/6245] Loss: 0.1903\n",
            "  [Batch 750/6245] Loss: 0.3772\n",
            "  [Batch 760/6245] Loss: 0.2330\n",
            "  [Batch 770/6245] Loss: 0.3830\n",
            "  [Batch 780/6245] Loss: 0.3581\n",
            "  [Batch 790/6245] Loss: 0.1799\n",
            "  [Batch 800/6245] Loss: 0.2637\n",
            "  [Batch 810/6245] Loss: 0.1892\n",
            "  [Batch 820/6245] Loss: 0.2813\n",
            "  [Batch 830/6245] Loss: 0.1365\n",
            "  [Batch 840/6245] Loss: 0.3028\n",
            "  [Batch 850/6245] Loss: 0.1965\n",
            "  [Batch 860/6245] Loss: 0.3152\n",
            "  [Batch 870/6245] Loss: 0.4246\n",
            "  [Batch 880/6245] Loss: 0.1703\n",
            "  [Batch 890/6245] Loss: 0.3486\n",
            "  [Batch 900/6245] Loss: 0.5017\n",
            "  [Batch 910/6245] Loss: 0.1711\n",
            "  [Batch 920/6245] Loss: 0.4404\n",
            "  [Batch 930/6245] Loss: 0.2547\n",
            "  [Batch 940/6245] Loss: 0.2446\n",
            "  [Batch 950/6245] Loss: 0.2682\n",
            "  [Batch 960/6245] Loss: 0.3794\n",
            "  [Batch 970/6245] Loss: 0.4371\n",
            "  [Batch 980/6245] Loss: 0.2514\n",
            "  [Batch 990/6245] Loss: 0.3750\n",
            "  [Batch 1000/6245] Loss: 0.3024\n",
            "  [Batch 1010/6245] Loss: 0.3254\n",
            "  [Batch 1020/6245] Loss: 0.2590\n",
            "  [Batch 1030/6245] Loss: 0.3613\n",
            "  [Batch 1040/6245] Loss: 0.1098\n",
            "  [Batch 1050/6245] Loss: 0.4022\n",
            "  [Batch 1060/6245] Loss: 0.2920\n",
            "  [Batch 1070/6245] Loss: 0.3235\n",
            "  [Batch 1080/6245] Loss: 0.2304\n",
            "  [Batch 1090/6245] Loss: 0.3264\n",
            "  [Batch 1100/6245] Loss: 0.1982\n",
            "  [Batch 1110/6245] Loss: 0.1103\n",
            "  [Batch 1120/6245] Loss: 0.2644\n",
            "  [Batch 1130/6245] Loss: 0.4560\n",
            "  [Batch 1140/6245] Loss: 0.2962\n",
            "  [Batch 1150/6245] Loss: 0.3371\n",
            "  [Batch 1160/6245] Loss: 0.2722\n",
            "  [Batch 1170/6245] Loss: 0.2113\n",
            "  [Batch 1180/6245] Loss: 0.4237\n",
            "  [Batch 1190/6245] Loss: 0.3256\n",
            "  [Batch 1200/6245] Loss: 0.1907\n",
            "  [Batch 1210/6245] Loss: 0.2853\n",
            "  [Batch 1220/6245] Loss: 0.4616\n",
            "  [Batch 1230/6245] Loss: 0.3324\n",
            "  [Batch 1240/6245] Loss: 0.3957\n",
            "  [Batch 1250/6245] Loss: 0.3478\n",
            "  [Batch 1260/6245] Loss: 0.4328\n",
            "  [Batch 1270/6245] Loss: 0.1235\n",
            "  [Batch 1280/6245] Loss: 0.0985\n",
            "  [Batch 1290/6245] Loss: 0.2136\n",
            "  [Batch 1300/6245] Loss: 0.2550\n",
            "  [Batch 1310/6245] Loss: 0.2644\n",
            "  [Batch 1320/6245] Loss: 0.4280\n",
            "  [Batch 1330/6245] Loss: 0.3673\n",
            "  [Batch 1340/6245] Loss: 0.2665\n",
            "  [Batch 1350/6245] Loss: 0.3109\n",
            "  [Batch 1360/6245] Loss: 0.2930\n",
            "  [Batch 1370/6245] Loss: 0.2477\n",
            "  [Batch 1380/6245] Loss: 0.2638\n",
            "  [Batch 1390/6245] Loss: 0.3254\n",
            "  [Batch 1400/6245] Loss: 0.3745\n",
            "  [Batch 1410/6245] Loss: 0.3960\n",
            "  [Batch 1420/6245] Loss: 0.3388\n",
            "  [Batch 1430/6245] Loss: 0.2354\n",
            "  [Batch 1440/6245] Loss: 0.2639\n",
            "  [Batch 1450/6245] Loss: 0.2795\n",
            "  [Batch 1460/6245] Loss: 0.3727\n",
            "  [Batch 1470/6245] Loss: 0.3253\n",
            "  [Batch 1480/6245] Loss: 0.3039\n",
            "  [Batch 1490/6245] Loss: 0.3524\n",
            "  [Batch 1500/6245] Loss: 0.2250\n",
            "  [Batch 1510/6245] Loss: 0.4788\n",
            "  [Batch 1520/6245] Loss: 0.3731\n",
            "  [Batch 1530/6245] Loss: 0.4474\n",
            "  [Batch 1540/6245] Loss: 0.3009\n",
            "  [Batch 1550/6245] Loss: 0.2300\n",
            "  [Batch 1560/6245] Loss: 0.1858\n",
            "  [Batch 1570/6245] Loss: 0.2363\n",
            "  [Batch 1580/6245] Loss: 0.2267\n",
            "  [Batch 1590/6245] Loss: 0.2611\n",
            "  [Batch 1600/6245] Loss: 0.2812\n",
            "  [Batch 1610/6245] Loss: 0.2678\n",
            "  [Batch 1620/6245] Loss: 0.3882\n",
            "  [Batch 1630/6245] Loss: 0.2310\n",
            "  [Batch 1640/6245] Loss: 0.4766\n",
            "  [Batch 1650/6245] Loss: 0.2002\n",
            "  [Batch 1660/6245] Loss: 0.2535\n",
            "  [Batch 1670/6245] Loss: 0.2293\n",
            "  [Batch 1680/6245] Loss: 0.1892\n",
            "  [Batch 1690/6245] Loss: 0.2407\n",
            "  [Batch 1700/6245] Loss: 0.3981\n",
            "  [Batch 1710/6245] Loss: 0.2943\n",
            "  [Batch 1720/6245] Loss: 0.3736\n",
            "  [Batch 1730/6245] Loss: 0.3203\n",
            "  [Batch 1740/6245] Loss: 0.2594\n",
            "  [Batch 1750/6245] Loss: 0.2785\n",
            "  [Batch 1760/6245] Loss: 0.3784\n",
            "  [Batch 1770/6245] Loss: 0.2651\n",
            "  [Batch 1780/6245] Loss: 0.3015\n",
            "  [Batch 1790/6245] Loss: 0.2983\n",
            "  [Batch 1800/6245] Loss: 0.3444\n",
            "  [Batch 1810/6245] Loss: 0.3829\n",
            "  [Batch 1820/6245] Loss: 0.4025\n",
            "  [Batch 1830/6245] Loss: 0.3180\n",
            "  [Batch 1840/6245] Loss: 0.2526\n",
            "  [Batch 1850/6245] Loss: 0.4605\n",
            "  [Batch 1860/6245] Loss: 0.3389\n",
            "  [Batch 1870/6245] Loss: 0.2520\n",
            "  [Batch 1880/6245] Loss: 0.2412\n",
            "  [Batch 1890/6245] Loss: 0.4225\n",
            "  [Batch 1900/6245] Loss: 0.2930\n",
            "  [Batch 1910/6245] Loss: 0.5204\n",
            "  [Batch 1920/6245] Loss: 0.3109\n",
            "  [Batch 1930/6245] Loss: 0.3493\n",
            "  [Batch 1940/6245] Loss: 0.2595\n",
            "  [Batch 1950/6245] Loss: 0.1771\n",
            "  [Batch 1960/6245] Loss: 0.2855\n",
            "  [Batch 1970/6245] Loss: 0.4326\n",
            "  [Batch 1980/6245] Loss: 0.6975\n",
            "  [Batch 1990/6245] Loss: 0.1762\n",
            "  [Batch 2000/6245] Loss: 0.1846\n",
            "  [Batch 2010/6245] Loss: 0.1013\n",
            "  [Batch 2020/6245] Loss: 0.2170\n",
            "  [Batch 2030/6245] Loss: 0.2609\n",
            "  [Batch 2040/6245] Loss: 0.4905\n",
            "  [Batch 2050/6245] Loss: 0.5230\n",
            "  [Batch 2060/6245] Loss: 0.2846\n",
            "  [Batch 2070/6245] Loss: 0.3294\n",
            "  [Batch 2080/6245] Loss: 0.2329\n",
            "  [Batch 2090/6245] Loss: 0.4164\n",
            "  [Batch 2100/6245] Loss: 0.3687\n",
            "  [Batch 2110/6245] Loss: 0.2823\n",
            "  [Batch 2120/6245] Loss: 0.4931\n",
            "  [Batch 2130/6245] Loss: 0.2778\n",
            "  [Batch 2140/6245] Loss: 0.2619\n",
            "  [Batch 2150/6245] Loss: 0.2453\n",
            "  [Batch 2160/6245] Loss: 0.3004\n",
            "  [Batch 2170/6245] Loss: 0.4326\n",
            "  [Batch 2180/6245] Loss: 0.4301\n",
            "  [Batch 2190/6245] Loss: 0.2286\n",
            "  [Batch 2200/6245] Loss: 0.3860\n",
            "  [Batch 2210/6245] Loss: 0.2198\n",
            "  [Batch 2220/6245] Loss: 0.4469\n",
            "  [Batch 2230/6245] Loss: 0.4902\n",
            "  [Batch 2240/6245] Loss: 0.2682\n",
            "  [Batch 2250/6245] Loss: 0.3585\n",
            "  [Batch 2260/6245] Loss: 0.1703\n",
            "  [Batch 2270/6245] Loss: 0.4425\n",
            "  [Batch 2280/6245] Loss: 0.4087\n",
            "  [Batch 2290/6245] Loss: 0.3420\n",
            "  [Batch 2300/6245] Loss: 0.2433\n",
            "  [Batch 2310/6245] Loss: 0.1618\n",
            "  [Batch 2320/6245] Loss: 0.2556\n",
            "  [Batch 2330/6245] Loss: 0.2563\n",
            "  [Batch 2340/6245] Loss: 0.3013\n",
            "  [Batch 2350/6245] Loss: 0.3447\n",
            "  [Batch 2360/6245] Loss: 0.2604\n",
            "  [Batch 2370/6245] Loss: 0.3097\n",
            "  [Batch 2380/6245] Loss: 0.3055\n",
            "  [Batch 2390/6245] Loss: 0.2495\n",
            "  [Batch 2400/6245] Loss: 0.2074\n",
            "  [Batch 2410/6245] Loss: 0.3804\n",
            "  [Batch 2420/6245] Loss: 0.3998\n",
            "  [Batch 2430/6245] Loss: 0.4937\n",
            "  [Batch 2440/6245] Loss: 0.2760\n",
            "  [Batch 2450/6245] Loss: 0.3398\n",
            "  [Batch 2460/6245] Loss: 0.4658\n",
            "  [Batch 2470/6245] Loss: 0.4901\n",
            "  [Batch 2480/6245] Loss: 0.2371\n",
            "  [Batch 2490/6245] Loss: 0.2314\n",
            "  [Batch 2500/6245] Loss: 0.2732\n",
            "  [Batch 2510/6245] Loss: 0.2629\n",
            "  [Batch 2520/6245] Loss: 0.3520\n",
            "  [Batch 2530/6245] Loss: 0.3618\n",
            "  [Batch 2540/6245] Loss: 0.4181\n",
            "  [Batch 2550/6245] Loss: 0.2949\n",
            "  [Batch 2560/6245] Loss: 0.4668\n",
            "  [Batch 2570/6245] Loss: 0.3361\n",
            "  [Batch 2580/6245] Loss: 0.5529\n",
            "  [Batch 2590/6245] Loss: 0.2845\n",
            "  [Batch 2600/6245] Loss: 0.0967\n",
            "  [Batch 2610/6245] Loss: 0.3148\n",
            "  [Batch 2620/6245] Loss: 0.3851\n",
            "  [Batch 2630/6245] Loss: 0.1852\n",
            "  [Batch 2640/6245] Loss: 0.3547\n",
            "  [Batch 2650/6245] Loss: 0.1678\n",
            "  [Batch 2660/6245] Loss: 0.1994\n",
            "  [Batch 2670/6245] Loss: 0.2354\n",
            "  [Batch 2680/6245] Loss: 0.5965\n",
            "  [Batch 2690/6245] Loss: 0.1820\n",
            "  [Batch 2700/6245] Loss: 0.3056\n",
            "  [Batch 2710/6245] Loss: 0.3997\n",
            "  [Batch 2720/6245] Loss: 0.4498\n",
            "  [Batch 2730/6245] Loss: 0.3318\n",
            "  [Batch 2740/6245] Loss: 0.3718\n",
            "  [Batch 2750/6245] Loss: 0.3215\n",
            "  [Batch 2760/6245] Loss: 0.1891\n",
            "  [Batch 2770/6245] Loss: 0.3726\n",
            "  [Batch 2780/6245] Loss: 0.3225\n",
            "  [Batch 2790/6245] Loss: 0.4160\n",
            "  [Batch 2800/6245] Loss: 0.2730\n",
            "  [Batch 2810/6245] Loss: 0.2292\n",
            "  [Batch 2820/6245] Loss: 0.5059\n",
            "  [Batch 2830/6245] Loss: 0.2601\n",
            "  [Batch 2840/6245] Loss: 0.2385\n",
            "  [Batch 2850/6245] Loss: 0.3686\n",
            "  [Batch 2860/6245] Loss: 0.5829\n",
            "  [Batch 2870/6245] Loss: 0.2293\n",
            "  [Batch 2880/6245] Loss: 0.2270\n",
            "  [Batch 2890/6245] Loss: 0.3776\n",
            "  [Batch 2900/6245] Loss: 0.4412\n",
            "  [Batch 2910/6245] Loss: 0.4539\n",
            "  [Batch 2920/6245] Loss: 0.2711\n",
            "  [Batch 2930/6245] Loss: 0.2859\n",
            "  [Batch 2940/6245] Loss: 0.2995\n",
            "  [Batch 2950/6245] Loss: 0.1785\n",
            "  [Batch 2960/6245] Loss: 0.2192\n",
            "  [Batch 2970/6245] Loss: 0.2449\n",
            "  [Batch 2980/6245] Loss: 0.3053\n",
            "  [Batch 2990/6245] Loss: 0.3003\n",
            "  [Batch 3000/6245] Loss: 0.4615\n",
            "  [Batch 3010/6245] Loss: 0.3708\n",
            "  [Batch 3020/6245] Loss: 0.3232\n",
            "  [Batch 3030/6245] Loss: 0.2434\n",
            "  [Batch 3040/6245] Loss: 0.2212\n",
            "  [Batch 3050/6245] Loss: 0.2954\n",
            "  [Batch 3060/6245] Loss: 0.2587\n",
            "  [Batch 3070/6245] Loss: 0.1456\n",
            "  [Batch 3080/6245] Loss: 0.1300\n",
            "  [Batch 3090/6245] Loss: 0.5095\n",
            "  [Batch 3100/6245] Loss: 0.3071\n",
            "  [Batch 3110/6245] Loss: 0.4943\n",
            "  [Batch 3120/6245] Loss: 0.2729\n",
            "  [Batch 3130/6245] Loss: 0.2743\n",
            "  [Batch 3140/6245] Loss: 0.3909\n",
            "  [Batch 3150/6245] Loss: 0.2582\n",
            "  [Batch 3160/6245] Loss: 0.3269\n",
            "  [Batch 3170/6245] Loss: 0.1610\n",
            "  [Batch 3180/6245] Loss: 0.2814\n",
            "  [Batch 3190/6245] Loss: 0.3455\n",
            "  [Batch 3200/6245] Loss: 0.3806\n",
            "  [Batch 3210/6245] Loss: 0.2862\n",
            "  [Batch 3220/6245] Loss: 0.2773\n",
            "  [Batch 3230/6245] Loss: 0.3532\n",
            "  [Batch 3240/6245] Loss: 0.3153\n",
            "  [Batch 3250/6245] Loss: 0.2113\n",
            "  [Batch 3260/6245] Loss: 0.4248\n",
            "  [Batch 3270/6245] Loss: 0.4124\n",
            "  [Batch 3280/6245] Loss: 0.2416\n",
            "  [Batch 3290/6245] Loss: 0.2386\n",
            "  [Batch 3300/6245] Loss: 0.3376\n",
            "  [Batch 3310/6245] Loss: 0.2332\n",
            "  [Batch 3320/6245] Loss: 0.1972\n",
            "  [Batch 3330/6245] Loss: 0.4751\n",
            "  [Batch 3340/6245] Loss: 0.3532\n",
            "  [Batch 3350/6245] Loss: 0.4044\n",
            "  [Batch 3360/6245] Loss: 0.2100\n",
            "  [Batch 3370/6245] Loss: 0.3355\n",
            "  [Batch 3380/6245] Loss: 0.2762\n",
            "  [Batch 3390/6245] Loss: 0.1655\n",
            "  [Batch 3400/6245] Loss: 0.3444\n",
            "  [Batch 3410/6245] Loss: 0.5039\n",
            "  [Batch 3420/6245] Loss: 0.2157\n",
            "  [Batch 3430/6245] Loss: 0.2096\n",
            "  [Batch 3440/6245] Loss: 0.2410\n",
            "  [Batch 3450/6245] Loss: 0.2648\n",
            "  [Batch 3460/6245] Loss: 0.3008\n",
            "  [Batch 3470/6245] Loss: 0.3000\n",
            "  [Batch 3480/6245] Loss: 0.1492\n",
            "  [Batch 3490/6245] Loss: 0.4099\n",
            "  [Batch 3500/6245] Loss: 0.4641\n",
            "  [Batch 3510/6245] Loss: 0.3366\n",
            "  [Batch 3520/6245] Loss: 0.2710\n",
            "  [Batch 3530/6245] Loss: 0.2786\n",
            "  [Batch 3540/6245] Loss: 0.3527\n",
            "  [Batch 3550/6245] Loss: 0.2868\n",
            "  [Batch 3560/6245] Loss: 0.4104\n",
            "  [Batch 3570/6245] Loss: 0.2546\n",
            "  [Batch 3580/6245] Loss: 0.2338\n",
            "  [Batch 3590/6245] Loss: 0.1956\n",
            "  [Batch 3600/6245] Loss: 0.2820\n",
            "  [Batch 3610/6245] Loss: 0.3210\n",
            "  [Batch 3620/6245] Loss: 0.2991\n",
            "  [Batch 3630/6245] Loss: 0.2396\n",
            "  [Batch 3640/6245] Loss: 0.4766\n",
            "  [Batch 3650/6245] Loss: 0.3018\n",
            "  [Batch 3660/6245] Loss: 0.2185\n",
            "  [Batch 3670/6245] Loss: 0.2824\n",
            "  [Batch 3680/6245] Loss: 0.4322\n",
            "  [Batch 3690/6245] Loss: 0.2500\n",
            "  [Batch 3700/6245] Loss: 0.3558\n",
            "  [Batch 3710/6245] Loss: 0.3270\n",
            "  [Batch 3720/6245] Loss: 0.2138\n",
            "  [Batch 3730/6245] Loss: 0.4024\n",
            "  [Batch 3740/6245] Loss: 0.4058\n",
            "  [Batch 3750/6245] Loss: 0.2835\n",
            "  [Batch 3760/6245] Loss: 0.2838\n",
            "  [Batch 3770/6245] Loss: 0.1993\n",
            "  [Batch 3780/6245] Loss: 0.2185\n",
            "  [Batch 3790/6245] Loss: 0.3909\n",
            "  [Batch 3800/6245] Loss: 0.2524\n",
            "  [Batch 3810/6245] Loss: 0.2893\n",
            "  [Batch 3820/6245] Loss: 0.3839\n",
            "  [Batch 3830/6245] Loss: 0.3416\n",
            "  [Batch 3840/6245] Loss: 0.2494\n",
            "  [Batch 3850/6245] Loss: 0.3398\n",
            "  [Batch 3860/6245] Loss: 0.1851\n",
            "  [Batch 3870/6245] Loss: 0.2116\n",
            "  [Batch 3880/6245] Loss: 0.4616\n",
            "  [Batch 3890/6245] Loss: 0.2882\n",
            "  [Batch 3900/6245] Loss: 0.2907\n",
            "  [Batch 3910/6245] Loss: 0.2236\n",
            "  [Batch 3920/6245] Loss: 0.2414\n",
            "  [Batch 3930/6245] Loss: 0.3059\n",
            "  [Batch 3940/6245] Loss: 0.4496\n",
            "  [Batch 3950/6245] Loss: 0.1801\n",
            "  [Batch 3960/6245] Loss: 0.3088\n",
            "  [Batch 3970/6245] Loss: 0.2623\n",
            "  [Batch 3980/6245] Loss: 0.3195\n",
            "  [Batch 3990/6245] Loss: 0.1516\n",
            "  [Batch 4000/6245] Loss: 0.4854\n",
            "  [Batch 4010/6245] Loss: 0.2845\n",
            "  [Batch 4020/6245] Loss: 0.2142\n",
            "  [Batch 4030/6245] Loss: 0.3634\n",
            "  [Batch 4040/6245] Loss: 0.2802\n",
            "  [Batch 4050/6245] Loss: 0.3420\n",
            "  [Batch 4060/6245] Loss: 0.3866\n",
            "  [Batch 4070/6245] Loss: 0.3521\n",
            "  [Batch 4080/6245] Loss: 0.5373\n",
            "  [Batch 4090/6245] Loss: 0.4697\n",
            "  [Batch 4100/6245] Loss: 0.4770\n",
            "  [Batch 4110/6245] Loss: 0.2970\n",
            "  [Batch 4120/6245] Loss: 0.4213\n",
            "  [Batch 4130/6245] Loss: 0.3530\n",
            "  [Batch 4140/6245] Loss: 0.1964\n",
            "  [Batch 4150/6245] Loss: 0.2360\n",
            "  [Batch 4160/6245] Loss: 0.3631\n",
            "  [Batch 4170/6245] Loss: 0.2325\n",
            "  [Batch 4180/6245] Loss: 0.1895\n",
            "  [Batch 4190/6245] Loss: 0.2254\n",
            "  [Batch 4200/6245] Loss: 0.4879\n",
            "  [Batch 4210/6245] Loss: 0.2691\n",
            "  [Batch 4220/6245] Loss: 0.3531\n",
            "  [Batch 4230/6245] Loss: 0.2555\n",
            "  [Batch 4240/6245] Loss: 0.2279\n",
            "  [Batch 4250/6245] Loss: 0.3800\n",
            "  [Batch 4260/6245] Loss: 0.3670\n",
            "  [Batch 4270/6245] Loss: 0.2877\n",
            "  [Batch 4280/6245] Loss: 0.5419\n",
            "  [Batch 4290/6245] Loss: 0.2512\n",
            "  [Batch 4300/6245] Loss: 0.5460\n",
            "  [Batch 4310/6245] Loss: 0.2561\n",
            "  [Batch 4320/6245] Loss: 0.3911\n",
            "  [Batch 4330/6245] Loss: 0.5255\n",
            "  [Batch 4340/6245] Loss: 0.3892\n",
            "  [Batch 4350/6245] Loss: 0.2205\n",
            "  [Batch 4360/6245] Loss: 0.5490\n",
            "  [Batch 4370/6245] Loss: 0.2115\n",
            "  [Batch 4380/6245] Loss: 0.3000\n",
            "  [Batch 4390/6245] Loss: 0.6085\n",
            "  [Batch 4400/6245] Loss: 0.3096\n",
            "  [Batch 4410/6245] Loss: 0.4313\n",
            "  [Batch 4420/6245] Loss: 0.2623\n",
            "  [Batch 4430/6245] Loss: 0.2496\n",
            "  [Batch 4440/6245] Loss: 0.2554\n",
            "  [Batch 4450/6245] Loss: 0.1805\n",
            "  [Batch 4460/6245] Loss: 0.5705\n",
            "  [Batch 4470/6245] Loss: 0.4031\n",
            "  [Batch 4480/6245] Loss: 0.2872\n",
            "  [Batch 4490/6245] Loss: 0.3729\n",
            "  [Batch 4500/6245] Loss: 0.3100\n",
            "  [Batch 4510/6245] Loss: 0.1894\n",
            "  [Batch 4520/6245] Loss: 0.3668\n",
            "  [Batch 4530/6245] Loss: 0.2913\n",
            "  [Batch 4540/6245] Loss: 0.3291\n",
            "  [Batch 4550/6245] Loss: 0.3780\n",
            "  [Batch 4560/6245] Loss: 0.2602\n",
            "  [Batch 4570/6245] Loss: 0.3579\n",
            "  [Batch 4580/6245] Loss: 0.4325\n",
            "  [Batch 4590/6245] Loss: 0.2988\n",
            "  [Batch 4600/6245] Loss: 0.2581\n",
            "  [Batch 4610/6245] Loss: 0.2206\n",
            "  [Batch 4620/6245] Loss: 0.2486\n",
            "  [Batch 4630/6245] Loss: 0.2442\n",
            "  [Batch 4640/6245] Loss: 0.3217\n",
            "  [Batch 4650/6245] Loss: 0.3389\n",
            "  [Batch 4660/6245] Loss: 0.3095\n",
            "  [Batch 4670/6245] Loss: 0.2719\n",
            "  [Batch 4680/6245] Loss: 0.4128\n",
            "  [Batch 4690/6245] Loss: 0.1508\n",
            "  [Batch 4700/6245] Loss: 0.3075\n",
            "  [Batch 4710/6245] Loss: 0.3351\n",
            "  [Batch 4720/6245] Loss: 0.4861\n",
            "  [Batch 4730/6245] Loss: 0.2226\n",
            "  [Batch 4740/6245] Loss: 0.3921\n",
            "  [Batch 4750/6245] Loss: 0.3697\n",
            "  [Batch 4760/6245] Loss: 0.4100\n",
            "  [Batch 4770/6245] Loss: 0.2935\n",
            "  [Batch 4780/6245] Loss: 0.2231\n",
            "  [Batch 4790/6245] Loss: 0.4120\n",
            "  [Batch 4800/6245] Loss: 0.4242\n",
            "  [Batch 4810/6245] Loss: 0.3864\n",
            "  [Batch 4820/6245] Loss: 0.2830\n",
            "  [Batch 4830/6245] Loss: 0.1461\n",
            "  [Batch 4840/6245] Loss: 0.1731\n",
            "  [Batch 4850/6245] Loss: 0.5495\n",
            "  [Batch 4860/6245] Loss: 0.3029\n",
            "  [Batch 4870/6245] Loss: 0.3017\n",
            "  [Batch 4880/6245] Loss: 0.2813\n",
            "  [Batch 4890/6245] Loss: 0.3053\n",
            "  [Batch 4900/6245] Loss: 0.2871\n",
            "  [Batch 4910/6245] Loss: 0.3898\n",
            "  [Batch 4920/6245] Loss: 0.2881\n",
            "  [Batch 4930/6245] Loss: 0.6604\n",
            "  [Batch 4940/6245] Loss: 0.3791\n",
            "  [Batch 4950/6245] Loss: 0.2824\n",
            "  [Batch 4960/6245] Loss: 0.2950\n",
            "  [Batch 4970/6245] Loss: 0.2900\n",
            "  [Batch 4980/6245] Loss: 0.1708\n",
            "  [Batch 4990/6245] Loss: 0.4259\n",
            "  [Batch 5000/6245] Loss: 0.4710\n",
            "  [Batch 5010/6245] Loss: 0.2029\n",
            "  [Batch 5020/6245] Loss: 0.3871\n",
            "  [Batch 5030/6245] Loss: 0.4011\n",
            "  [Batch 5040/6245] Loss: 0.4127\n",
            "  [Batch 5050/6245] Loss: 0.2641\n",
            "  [Batch 5060/6245] Loss: 0.2856\n",
            "  [Batch 5070/6245] Loss: 0.1963\n",
            "  [Batch 5080/6245] Loss: 0.2298\n",
            "  [Batch 5090/6245] Loss: 0.3736\n",
            "  [Batch 5100/6245] Loss: 0.4881\n",
            "  [Batch 5110/6245] Loss: 0.2247\n",
            "  [Batch 5120/6245] Loss: 0.2275\n",
            "  [Batch 5130/6245] Loss: 0.1651\n",
            "  [Batch 5140/6245] Loss: 0.2600\n",
            "  [Batch 5150/6245] Loss: 0.2475\n",
            "  [Batch 5160/6245] Loss: 0.2561\n",
            "  [Batch 5170/6245] Loss: 0.3052\n",
            "  [Batch 5180/6245] Loss: 0.2253\n",
            "  [Batch 5190/6245] Loss: 0.1985\n",
            "  [Batch 5200/6245] Loss: 0.1205\n",
            "  [Batch 5210/6245] Loss: 0.1797\n",
            "  [Batch 5220/6245] Loss: 0.3384\n",
            "  [Batch 5230/6245] Loss: 0.3350\n",
            "  [Batch 5240/6245] Loss: 0.3529\n",
            "  [Batch 5250/6245] Loss: 0.3679\n",
            "  [Batch 5260/6245] Loss: 0.2819\n",
            "  [Batch 5270/6245] Loss: 0.2205\n",
            "  [Batch 5280/6245] Loss: 0.2313\n",
            "  [Batch 5290/6245] Loss: 0.2578\n",
            "  [Batch 5300/6245] Loss: 0.4208\n",
            "  [Batch 5310/6245] Loss: 0.3421\n",
            "  [Batch 5320/6245] Loss: 0.2224\n",
            "  [Batch 5330/6245] Loss: 0.1525\n",
            "  [Batch 5340/6245] Loss: 0.3573\n",
            "  [Batch 5350/6245] Loss: 0.2645\n",
            "  [Batch 5360/6245] Loss: 0.4336\n",
            "  [Batch 5370/6245] Loss: 0.3851\n",
            "  [Batch 5380/6245] Loss: 0.5546\n",
            "  [Batch 5390/6245] Loss: 0.2416\n",
            "  [Batch 5400/6245] Loss: 0.4026\n",
            "  [Batch 5410/6245] Loss: 0.3950\n",
            "  [Batch 5420/6245] Loss: 0.3171\n",
            "  [Batch 5430/6245] Loss: 0.3696\n",
            "  [Batch 5440/6245] Loss: 0.5621\n",
            "  [Batch 5450/6245] Loss: 0.2391\n",
            "  [Batch 5460/6245] Loss: 0.3176\n",
            "  [Batch 5470/6245] Loss: 0.1650\n",
            "  [Batch 5480/6245] Loss: 0.5055\n",
            "  [Batch 5490/6245] Loss: 0.4339\n",
            "  [Batch 5500/6245] Loss: 0.3714\n",
            "  [Batch 5510/6245] Loss: 0.4272\n",
            "  [Batch 5520/6245] Loss: 0.2404\n",
            "  [Batch 5530/6245] Loss: 0.3040\n",
            "  [Batch 5540/6245] Loss: 0.4480\n",
            "  [Batch 5550/6245] Loss: 0.4891\n",
            "  [Batch 5560/6245] Loss: 0.1466\n",
            "  [Batch 5570/6245] Loss: 0.2120\n",
            "  [Batch 5580/6245] Loss: 0.2107\n",
            "  [Batch 5590/6245] Loss: 0.3914\n",
            "  [Batch 5600/6245] Loss: 0.4101\n",
            "  [Batch 5610/6245] Loss: 0.1320\n",
            "  [Batch 5620/6245] Loss: 0.2659\n",
            "  [Batch 5630/6245] Loss: 0.3316\n",
            "  [Batch 5640/6245] Loss: 0.3173\n",
            "  [Batch 5650/6245] Loss: 0.3327\n",
            "  [Batch 5660/6245] Loss: 0.2741\n",
            "  [Batch 5670/6245] Loss: 0.1987\n",
            "  [Batch 5680/6245] Loss: 0.1830\n",
            "  [Batch 5690/6245] Loss: 0.3541\n",
            "  [Batch 5700/6245] Loss: 0.1536\n",
            "  [Batch 5710/6245] Loss: 0.2367\n",
            "  [Batch 5720/6245] Loss: 0.4026\n",
            "  [Batch 5730/6245] Loss: 0.3161\n",
            "  [Batch 5740/6245] Loss: 0.2282\n",
            "  [Batch 5750/6245] Loss: 0.5117\n",
            "  [Batch 5760/6245] Loss: 0.2567\n",
            "  [Batch 5770/6245] Loss: 0.3400\n",
            "  [Batch 5780/6245] Loss: 0.3161\n",
            "  [Batch 5790/6245] Loss: 0.3815\n",
            "  [Batch 5800/6245] Loss: 0.4462\n",
            "  [Batch 5810/6245] Loss: 0.1868\n",
            "  [Batch 5820/6245] Loss: 0.5328\n",
            "  [Batch 5830/6245] Loss: 0.3766\n",
            "  [Batch 5840/6245] Loss: 0.3494\n",
            "  [Batch 5850/6245] Loss: 0.4696\n",
            "  [Batch 5860/6245] Loss: 0.3386\n",
            "  [Batch 5870/6245] Loss: 0.1873\n",
            "  [Batch 5880/6245] Loss: 0.2624\n",
            "  [Batch 5890/6245] Loss: 0.2370\n",
            "  [Batch 5900/6245] Loss: 0.2295\n",
            "  [Batch 5910/6245] Loss: 0.4184\n",
            "  [Batch 5920/6245] Loss: 0.2147\n",
            "  [Batch 5930/6245] Loss: 0.1925\n",
            "  [Batch 5940/6245] Loss: 0.2043\n",
            "  [Batch 5950/6245] Loss: 0.2433\n",
            "  [Batch 5960/6245] Loss: 0.2987\n",
            "  [Batch 5970/6245] Loss: 0.2375\n",
            "  [Batch 5980/6245] Loss: 0.3320\n",
            "  [Batch 5990/6245] Loss: 0.3400\n",
            "  [Batch 6000/6245] Loss: 0.2240\n",
            "  [Batch 6010/6245] Loss: 0.2644\n",
            "  [Batch 6020/6245] Loss: 0.4146\n",
            "  [Batch 6030/6245] Loss: 0.2341\n",
            "  [Batch 6040/6245] Loss: 0.4257\n",
            "  [Batch 6050/6245] Loss: 0.2948\n",
            "  [Batch 6060/6245] Loss: 0.3175\n",
            "  [Batch 6070/6245] Loss: 0.2860\n",
            "  [Batch 6080/6245] Loss: 0.1705\n",
            "  [Batch 6090/6245] Loss: 0.5393\n",
            "  [Batch 6100/6245] Loss: 0.3744\n",
            "  [Batch 6110/6245] Loss: 0.2786\n",
            "  [Batch 6120/6245] Loss: 0.4140\n",
            "  [Batch 6130/6245] Loss: 0.4594\n",
            "  [Batch 6140/6245] Loss: 0.4975\n",
            "  [Batch 6150/6245] Loss: 0.3569\n",
            "  [Batch 6160/6245] Loss: 0.2569\n",
            "  [Batch 6170/6245] Loss: 0.3311\n",
            "  [Batch 6180/6245] Loss: 0.2332\n",
            "  [Batch 6190/6245] Loss: 0.2510\n",
            "  [Batch 6200/6245] Loss: 0.3752\n",
            "  [Batch 6210/6245] Loss: 0.2559\n",
            "  [Batch 6220/6245] Loss: 0.2401\n",
            "  [Batch 6230/6245] Loss: 0.3284\n",
            "  [Batch 6240/6245] Loss: 0.3737\n",
            "  [Batch 6245/6245] Loss: 0.3789\n",
            "[Epoch 10] Train Loss: 0.3145, Acc: 86.48% | Val Loss: 0.3070, Acc: 86.72%\n",
            "[INFO] Model checkpoint saved at ./models\\breastnet_epoch10.pth\n",
            "✅ Training Complete.\n"
          ]
        }
      ],
      "source": [
        "# #import os\n",
        "# from torchvision import datasets, transforms\n",
        "# from torch.utils.data import DataLoader\n",
        "# import torch.nn.functional as F\n",
        "# import torch.nn as nn\n",
        "# import torch\n",
        "\n",
        "# # Set paths and device\n",
        "# data_root = r\"D:\\extra\\BC\\Preprocessed\"\n",
        "# model_dir = \"./models\"\n",
        "# os.makedirs(model_dir, exist_ok=True)\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# print(f\"[INFO] Using device: {device}\")\n",
        "\n",
        "# # Transforms\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.Resize((224, 224)),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize([0.5], [0.5])\n",
        "# ])\n",
        "\n",
        "# # Loaders\n",
        "# batch_size = 32\n",
        "# print(\"[INFO] Loading datasets...\")\n",
        "# train_loader = DataLoader(datasets.ImageFolder(f\"{data_root}/train\", transform=transform), batch_size=batch_size, shuffle=True)\n",
        "# val_loader   = DataLoader(datasets.ImageFolder(f\"{data_root}/val\", transform=transform), batch_size=batch_size, shuffle=False)\n",
        "# test_loader  = DataLoader(datasets.ImageFolder(f\"{data_root}/test\", transform=transform), batch_size=batch_size, shuffle=False)\n",
        "# print(\"[INFO] Datasets loaded successfully.\")\n",
        "\n",
        "# # Define BreastNet\n",
        "# class ChannelAttention(nn.Module):\n",
        "#     def __init__(self, in_planes, ratio=8):\n",
        "#         super(ChannelAttention, self).__init__()\n",
        "#         self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "#         self.fc1 = nn.Conv2d(in_planes, in_planes // ratio, kernel_size=1)\n",
        "#         self.relu = nn.ReLU()\n",
        "#         self.fc2 = nn.Conv2d(in_planes // ratio, in_planes, kernel_size=1)\n",
        "#         self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         out = self.pool(x)\n",
        "#         out = self.relu(self.fc1(out))\n",
        "#         out = self.sigmoid(self.fc2(out))\n",
        "#         return x * out\n",
        "\n",
        "# class BreastNet(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(BreastNet, self).__init__()\n",
        "#         self.conv = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "#         self.bn = nn.BatchNorm2d(32)\n",
        "#         self.attn = ChannelAttention(32)\n",
        "#         self.pool = nn.AdaptiveAvgPool2d((8, 8))\n",
        "#         self.fc = nn.Linear(32 * 8 * 8, 2)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = F.relu(self.bn(self.conv(x)))\n",
        "#         x = self.attn(x)\n",
        "#         x = self.pool(x)\n",
        "#         x = x.view(x.size(0), -1)\n",
        "#         return self.fc(x)\n",
        "\n",
        "# # Initialize model\n",
        "# model = BreastNet().to(device)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# # Training Loop\n",
        "# num_epochs = 10\n",
        "# print(\"[INFO] Starting training...\")\n",
        "# for epoch in range(num_epochs):\n",
        "#     print(f\"\\n[Epoch {epoch+1}/{num_epochs}]\")\n",
        "#     model.train()\n",
        "#     total, correct, running_loss = 0, 0, 0\n",
        "\n",
        "#     for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "#         images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs = model(images)\n",
        "#         loss = criterion(outputs, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         running_loss += loss.item()\n",
        "#         _, predicted = torch.max(outputs, 1)\n",
        "#         total += labels.size(0)\n",
        "#         correct += (predicted == labels).sum().item()\n",
        "\n",
        "#         if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == len(train_loader):\n",
        "#             print(f\"  [Batch {batch_idx+1}/{len(train_loader)}] Loss: {loss.item():.4f}\")\n",
        "\n",
        "#     train_acc = 100 * correct / total\n",
        "#     train_loss = running_loss / len(train_loader)\n",
        "\n",
        "#     # Validation\n",
        "#     model.eval()\n",
        "#     val_total, val_correct, val_loss = 0, 0, 0\n",
        "#     with torch.no_grad():\n",
        "#         for images, labels in val_loader:\n",
        "#             images, labels = images.to(device), labels.to(device)\n",
        "#             outputs = model(images)\n",
        "#             loss = criterion(outputs, labels)\n",
        "#             val_loss += loss.item()\n",
        "#             _, predicted = torch.max(outputs, 1)\n",
        "#             val_total += labels.size(0)\n",
        "#             val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "#     val_acc = 100 * val_correct / val_total\n",
        "#     val_loss = val_loss / len(val_loader)\n",
        "\n",
        "#     print(f\"[Epoch {epoch+1}] Train Loss: {train_loss:.4f}, Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%\")\n",
        "\n",
        "#     # Save checkpoint\n",
        "#     checkpoint_path = os.path.join(model_dir, f\"breastnet_epoch{epoch+1}.pth\")\n",
        "#     torch.save({\n",
        "#         'epoch': epoch + 1,\n",
        "#         'model_state_dict': model.state_dict(),\n",
        "#         'optimizer_state_dict': optimizer.state_dict(),\n",
        "#         'train_loss': train_loss,\n",
        "#         'val_loss': val_loss,\n",
        "#         'val_accuracy': val_acc\n",
        "#     }, checkpoint_path)\n",
        "#     print(f\"[INFO] Model checkpoint saved at {checkpoint_path}\")\n",
        "\n",
        "# print(\"✅ Training Complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ONE MORE EXPER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/5: 100%|██████████| 5138/5138 [2:27:13<00:00,  1.72s/it]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy after epoch 1: 0.8469\n",
            "[INFO] Best model saved.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/5: 100%|██████████| 5138/5138 [2:52:18<00:00,  2.01s/it]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy after epoch 2: 0.8569\n",
            "[INFO] Best model saved.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/5: 100%|██████████| 5138/5138 [2:13:28<00:00,  1.56s/it]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy after epoch 3: 0.8604\n",
            "[INFO] Best model saved.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/5: 100%|██████████| 5138/5138 [2:18:49<00:00,  1.62s/it]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy after epoch 4: 0.8603\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/5: 100%|██████████| 5138/5138 [2:14:13<00:00,  1.57s/it]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy after epoch 5: 0.8648\n",
            "[INFO] Best model saved.\n",
            "Final Best Accuracy: 0.8647749167980382\n"
          ]
        }
      ],
      "source": [
        "# breastnet_train_fast.py\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import json\n",
        "\n",
        "# === Config ===\n",
        "data_dir = r\"D:\\\\thisisimp\\\\working-bc\\\\Preprocessed\"\n",
        "breastnet_output_dir = r\"D:\\\\thisisimp\\\\working-bc\\\\breastnet\"\n",
        "os.makedirs(breastnet_output_dir, exist_ok=True)\n",
        "train_path = os.path.join(data_dir, \"train\")\n",
        "val_path = os.path.join(data_dir, \"val\")\n",
        "log_file = os.path.join(breastnet_output_dir, \"breastnet_accuracy_log_fast_5epoch.json\")\n",
        "model_save_path = os.path.join(breastnet_output_dir, \"breastnet_best_model_fast_5epoch.pth\")\n",
        "\n",
        "batch_size = 32\n",
        "lr = 1e-3\n",
        "epochs = 5\n",
        "sample_fraction = 0.8  # use 80% of train data each epoch\n",
        "resize_dim = 192\n",
        "\n",
        "# === Device ===\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# === Transforms (light augmentations for speed) ===\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((resize_dim, resize_dim)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])\n",
        "\n",
        "# === Dataset ===\n",
        "train_full = ImageFolder(train_path, transform=transform)\n",
        "val_ds = ImageFolder(val_path, transform=transform)\n",
        "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# === Channel Attention Block ===\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, in_planes, ratio=8):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc1 = nn.Conv2d(in_planes, in_planes // ratio, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Conv2d(in_planes // ratio, in_planes, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.pool(x)\n",
        "        out = self.relu(self.fc1(out))\n",
        "        out = self.sigmoid(self.fc2(out))\n",
        "        return x * out\n",
        "\n",
        "# === BreastNet ===\n",
        "class BreastNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BreastNet, self).__init__()\n",
        "        self.conv = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.bn = nn.BatchNorm2d(32)\n",
        "        self.attn = ChannelAttention(32)\n",
        "        self.pool = nn.AdaptiveAvgPool2d((8, 8))\n",
        "        self.fc = nn.Linear(32 * 8 * 8, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn(self.conv(x)))\n",
        "        x = self.attn(x)\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.fc(x)\n",
        "\n",
        "# === Training ===\n",
        "model = BreastNet().to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "best_acc = 0\n",
        "log_data = {}\n",
        "\n",
        "if os.path.exists(log_file):\n",
        "    with open(log_file, 'r') as f:\n",
        "        log_data = json.load(f)\n",
        "    print(\"[INFO] Resuming training from previous state...\")\n",
        "\n",
        "start_epoch = len(log_data)\n",
        "\n",
        "for epoch in range(start_epoch, epochs):\n",
        "    # === Use 80% random subset of training data each epoch ===\n",
        "    indices = random.sample(range(len(train_full)), int(sample_fraction * len(train_full)))\n",
        "    train_subset = Subset(train_full, indices)\n",
        "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    for imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # === Validation ===\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in val_loader:\n",
        "            imgs = imgs.to(device)\n",
        "            outputs = model(imgs)\n",
        "            preds = outputs.argmax(dim=1).cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels.numpy())\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    print(f\"Validation Accuracy after epoch {epoch+1}: {acc:.4f}\")\n",
        "\n",
        "    if acc > best_acc:\n",
        "        best_acc = acc\n",
        "        torch.save(model.state_dict(), model_save_path)\n",
        "        print(\"[INFO] Best model saved.\")\n",
        "\n",
        "    log_data[str(epoch+1)] = {\"val_accuracy\": acc}\n",
        "    with open(log_file, 'w') as f:\n",
        "        json.dump(log_data, f, indent=2)\n",
        "\n",
        "print(\"Final Best Accuracy:\", best_acc)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Evaluating BreastNet\n",
            "[INFO] Evaluating ResNet18\n",
            "[INFO] Evaluating VGG16\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x400 with 3 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model        Accuracy   Precision  Recall     F1-Score   AUC       \n",
            "-----------------------------------------------------------------\n",
            "BreastNet    0.8650     0.7738     0.7665     0.7701     0.9236    \n",
            "ResNet18     0.8683     0.7789     0.7731     0.7760     0.9261    \n",
            "VGG16        0.7733     0.5831     0.8131     0.6792     0.8698    \n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# === Paths ===\n",
        "val_path = r\"D:\\thisisimp\\working-bc\\Preprocessed\\val\"\n",
        "model_paths = {\n",
        "    \"BreastNet\": r\"D:\\thisisimp\\working-bc\\breastnet\\breastnet_best_model_fast_5epoch.pth\",\n",
        "    \"ResNet18\": r\"D:\\thisisimp\\working-bc\\resnet18 models\\resnet18_epoch5.pth\",\n",
        "    \"VGG16\": r\"D:\\thisisimp\\working-bc\\vgg16 models\\vgg16_best.pth\"\n",
        "}\n",
        "\n",
        "# === Device ===\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# === Transforms ===\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((192, 192)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])\n",
        "\n",
        "# === Dataset ===\n",
        "val_dataset = ImageFolder(val_path, transform=transform)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "class_names = val_dataset.classes  # ['benign', 'malignant']\n",
        "\n",
        "# === Model Definitions ===\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, in_planes, ratio=8):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc1 = nn.Conv2d(in_planes, in_planes // ratio, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Conv2d(in_planes // ratio, in_planes, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.pool(x)\n",
        "        out = self.relu(self.fc1(out))\n",
        "        out = self.sigmoid(self.fc2(out))\n",
        "        return x * out\n",
        "\n",
        "class BreastNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BreastNet, self).__init__()\n",
        "        self.conv = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.bn = nn.BatchNorm2d(32)\n",
        "        self.attn = ChannelAttention(32)\n",
        "        self.pool = nn.AdaptiveAvgPool2d((8, 8))\n",
        "        self.fc = nn.Linear(32 * 8 * 8, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn(self.conv(x)))\n",
        "        x = self.attn(x)\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.fc(x)\n",
        "\n",
        "def get_model(name):\n",
        "    if name == \"BreastNet\":\n",
        "        model = BreastNet()\n",
        "    elif name == \"ResNet18\":\n",
        "        model = models.resnet18(weights=None)\n",
        "        model.fc = nn.Linear(model.fc.in_features, 2)\n",
        "    elif name == \"VGG16\":\n",
        "        model = models.vgg16(weights=None)\n",
        "        model.classifier[6] = nn.Linear(model.classifier[6].in_features, 2)\n",
        "    return model.to(device)\n",
        "\n",
        "# === Evaluation + Storage ===\n",
        "results = {}\n",
        "for name, path in model_paths.items():\n",
        "    print(f\"[INFO] Evaluating {name}\")\n",
        "    model = get_model(name)\n",
        "    \n",
        "    checkpoint = torch.load(path, map_location=device)\n",
        "    if \"model_state_dict\" in checkpoint:\n",
        "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    else:\n",
        "        model.load_state_dict(checkpoint)\n",
        "        \n",
        "    model.eval()\n",
        "\n",
        "    all_probs, all_preds, all_labels = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in val_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            outputs = model(imgs)\n",
        "            probs = torch.softmax(outputs, dim=1)[:, 1]  # Prob of class 1\n",
        "            preds = outputs.argmax(dim=1)\n",
        "\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "\n",
        "    results[name] = {\n",
        "        \"labels\": np.array(all_labels),\n",
        "        \"preds\": np.array(all_preds),\n",
        "        \"probs\": np.array(all_probs)\n",
        "    }\n",
        "\n",
        "# === Confusion Matrices ===\n",
        "plt.figure(figsize=(12, 4))\n",
        "for i, (name, res) in enumerate(results.items()):\n",
        "    cm = confusion_matrix(res[\"labels\"], res[\"preds\"])\n",
        "    plt.subplot(1, 3, i + 1)\n",
        "    plt.imshow(cm, cmap=\"Blues\")\n",
        "    plt.title(f\"{name} Confusion Matrix\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.xticks([0, 1], class_names)\n",
        "    plt.yticks([0, 1], class_names)\n",
        "    for r in range(2):\n",
        "        for c in range(2):\n",
        "            plt.text(c, r, cm[r, c], ha=\"center\", va=\"center\", color=\"red\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"confusion_matrix_compare.png\")\n",
        "plt.show()\n",
        "\n",
        "# === ROC Curves ===\n",
        "plt.figure(figsize=(8, 6))\n",
        "for name, res in results.items():\n",
        "    fpr, tpr, _ = roc_curve(res[\"labels\"], res[\"probs\"])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f\"{name} (AUC = {roc_auc:.3f})\")\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curves\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig(\"roc_compare.png\")\n",
        "plt.show()\n",
        "\n",
        "# === Metrics Table ===\n",
        "print(f\"{'Model':<12} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'AUC':<10}\")\n",
        "print(\"-\" * 65)\n",
        "for name, res in results.items():\n",
        "    acc = accuracy_score(res[\"labels\"], res[\"preds\"])\n",
        "    prec = precision_score(res[\"labels\"], res[\"preds\"])\n",
        "    rec = recall_score(res[\"labels\"], res[\"preds\"])\n",
        "    f1 = f1_score(res[\"labels\"], res[\"preds\"])\n",
        "    roc_auc = auc(*roc_curve(res[\"labels\"], res[\"probs\"])[:2])\n",
        "    print(f\"{name:<12} {acc:<10.4f} {prec:<10.4f} {rec:<10.4f} {f1:<10.4f} {roc_auc:<10.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
